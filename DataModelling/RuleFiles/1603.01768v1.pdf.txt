Thus, we increase the quality of images generated by avoiding common glitches, make the results look signiﬁcantly more plausible, and extend the functional range of these algorithms—whether for portraits or landscapes, etc.
0.45 (we; increase; the quality of images)
0.88 (the results; extend whether; the functional range of these algorithms)
0.89 (images; generated; by avoiding common glitches)
0.47 (we; make; the results look signiﬁcantly more plausible, and extend the functional range of these algorithms-whether for portraits or landscapes, etc.)
0.72 Context(we make):(the results; look; signiﬁcantly more plausible)

Through our social media bot that ﬁrst provided these algorithms as a service (Champandard 2015), we observe that users have clear expectations how style transfer should ∗This research was funded out of the marketing budget.
0.64 (our social media bot; ﬁrst provided; these algorithms; as a service)
0.19 (we; observe; that users have clear expectations)
0.91 Context(we observe):(users; have; clear expectations how style transfer should *This research was funded out of the marketing budget)

We attribute these problems to two underlying causes: 1.
0.45 (We; attribute; these problems; to two underlying causes)

To remedy this, we introduce an architecture that bridges the gap between generative algorithms and pixel labeling neural networks.
0.93 (an architecture; bridges; the gap between generative algorithms and pixel labeling neural networks)
0.27 (we; introduce; an architecture that bridges the gap between generative algorithms and pixel labeling neural networks)

Then we explain how existing algorithms can be adapted to include such annotations, and ﬁnally we showcase some applications in style transfer as well as image synthesis by analogy (e.g.
0.89 (existing algorithms; to include; such annotations)
0.22 (we; showcase e.g.; T:ﬁnally)
0.50 (we; explain; how existing algorithms can be adapted to include such annotations, and ﬁnally we showcase some applications in style transfer as well as image synthesis by analogy (e.g.; T:Then)
0.88 Context(we explain):(existing algorithms; can be adapted; to include such annotations)

Figure 3: Our augmented CNN that uses regular ﬁlters of N channels (top), concatenated with a semantic map of M=1 channel (bottom) either output from another network capable of labeling pixels or as manual annotations.
0.73 (Our augmented CNN; uses; regular ﬁlters of N channels (top)
0.94 (N channels (top; concatenated; with a semantic map of M=1 channel)

Before concatenation, the semantic channels are weighted by parameter γ to provide an additional user control point: Our approach (Li and Wand 2016) to style transfer, using optimization to minimize content reconstruction error Ec (weighted by α) and style remapping error Es (weight β).
0.95 (the semantic channels; are weighted; by parameter γ; to provide an additional user control point; T:Before concatenation)
0.90 (the semantic channels; to provide; an additional user control point)
0.94 (the semantic channels; using; optimization; to minimize content reconstruction error Ec (weighted by α) and style remapping error Es (weight β)
0.90 (content reconstruction error; weighted; by α)

E = αEc + βEs First we introduce an augmented CNN (Figure 6) that incorporates semantic information, then we deﬁne the input semantic map and its representation, and ﬁnally show how the algorithm is able to exploit this additional information.
0.50 (we; introduce; an augmented CNN)
0.89 (the algorithm; to exploit; this additional information)
0.92 (an augmented CNN; incorporates; semantic information)
0.44 (we; deﬁne; the input semantic map and its representation; T:then)
0.46 (we; ﬁnally show; how the algorithm is able to exploit this additional information)
0.90 Context(we ﬁnally show):(the algorithm; is; able to exploit this additional information)

Our augmented network concatenates additional semantic channels ml of size M at the same resolution, computed by down-sampling a static semantic map speciﬁed as input.
0.66 (Our augmented network; concatenates; additional semantic channels ml of size M)
0.92 (a static semantic map; speciﬁed; as input)

Typical results from our solution are shown in portraits from Figure 2, which contains both success cases (top row) and sub-optimal results (bottom row).
0.70 (Typical results from our solution; are shown; L:in portraits from Figure 2)
0.93 (Figure 2; contains; both success cases (top row) and sub-optimal results (bottom row)

Figure 5 shows a grid with visualizations of results as β and γ vary; we note the following: • The quality and variety of the style degenerates as γ increases too far, without noticeably improving the precision wrt.
0.61 (β and γ; vary; )
0.96 (The quality and variety of the style; degenerates; as γ increases too far, without noticeably improving the precision wrt)
0.51 (we; note; the following: • The quality and variety of the style degenerates as γ increases too far, without noticeably improving the precision wrt)
0.91 Context(we note):(Figure 5; shows; a grid)

Here we report observations from working with the algorithm, and provide our interpretations.
0.27 (we; provide; our interpretations)
0.53 (we; report; observations; from working with the algorithm; L:Here)
0.39 Context(we report):(we; report observations from working; with the algorithm)

Authored Representations We noticed that when users are asked to annotate images, after a bit of experience with the system, they implicitly create “semantic embeddings” that compactly describe pixel classes.
0.89 (semantic embeddings; compactly describe; pixel classes)
0.17 (We; noticed; that when users are asked to annotate images, after a bit of experience with the system, they implicitly create "semantic embeddings)
0.41 Context(We noticed):(they; implicitly create; semantic embeddings" that compactly describe pixel classes; T:when users are asked to annotate images; T:after a bit of experience with the system)
0.89 (users; are asked; to annotate images)
0.88 (users; to annotate; images)

Weight Sensitivity The algorithm is less fragile to adjustments in style weight; typically as the weight increases, the image degenerates and becomes a patchwork of the style content.
0.93 (the image; becomes; a patchwork of the style content; T:typically as the weight increases)
0.69 (the image; degenerates; )
0.91 Context(the image degenerates):(The algorithm; is; less fragile to adjustments in style weight)

Performance Due to the additional channels in the model, our algorithm requires more memory as well as extra computation compared to its predecessor.
0.61 (our algorithm; requires; more memory as well as extra computation compared to its predecessor)

In this paper, we resolved these issues by annotating input images with a semantic map, either manually authored or from pixel labeling algorithms.
0.72 (a semantic map; manually authored; )
0.53 (we; resolved; these issues; L:In this paper)
0.50 Context(we resolved):(we; resolved these issues by annotating; input images with a semantic map, either manually authored or from pixel labeling algorithms)

We introduced an augmented CNN architecture to leverage this information at runtime, while further tying advances in image segmentation to image synthesis.
0.55 (We; introduced; an augmented CNN architecture; to leverage this information at runtime; T:while further tying advances in image segmentation to image synthesis)
0.29 Context(We introduced):(We; introduced an augmented CNN architecture further tying; advances in image segmentation)

We showed that existing patch-based algorithms require minor adjustments and perform very well using this additional information.
0.28 (We; showed; that existing patch-based algorithms require minor adjustments and perform very well)
0.91 Context(We showed):(existing patch-based algorithms; require; minor adjustments)

The examples shown for style transfer show how this technique helps deal with completely opposite patterns/colors in corresponding image regions, and we analyzed how it helps users control the output of these algorithms better
0.94 (The examples; shown; for style transfer show; how  this technique helps deal with completely opposite patterns/colors in corresponding image regions)
0.94 Context(The examples shown):(this technique; helps; deal with completely opposite patterns/colors in corresponding image regions)
0.92 Context(The examples shown this technique helps):(this technique; helps deal; with completely opposite patterns/colors in corresponding image regions)
0.36 (we; analyzed; how it helps users control the output of these algorithms better)
0.46 Context(we analyzed):(it; helps; users control the output of these algorithms better)
0.86 Context(we analyzed it helps):(users; control better; the output of these algorithms)

