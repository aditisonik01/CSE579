We present a method for detecting objects in images using a single deep neural network.
0.45 (We; present; a method for detecting objects in images)

Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location.
0.64 (Our approach; named; SSD)
0.73 (Our approach, named SSD; discretizes; the output space of bounding boxes; into a set of default boxes)

1 We achieved even better results using an improved data augmentation scheme in follow-on experiments: 77.2% mAP for 300×300 input and 79.8% mAP for 512×512 input on VOC2007.
0.39 (We; achieved; even better results)
0.39 Context(We achieved):(We; achieved even better results using; an improved data augmentation scheme; L:in follow-on experiments)

We are not the ﬁrst to do this (cf [4,5]), but by adding a series of improvements, we manage to increase the accuracy signiﬁcantly over previous attempts.
0.57 (We; are not; the ﬁrst to do this (cf [4,5]))
0.39 (we; manage; to increase the accuracy signiﬁcantly over previous attempts)
0.39 Context(we manage):(we; manage to increase; the accuracy)

Our improvements include using a small convolutional ﬁlter to predict object categories and offsets in bounding box locations, using separate predictors (ﬁlters) for different aspect ratio detections, and applying these ﬁlters to multiple feature maps from the later stages of a network in order to perform detection at multiple scales.
0.74 (Our improvements; include; using a small convolutional ﬁlter to predict object categories and offsets in bounding box locations)

While these contributions may seem small independently, we note that the resulting system improves accuracy on real-time detection for PASCAL VOC from 63.4% mAP for YOLO to 74.3% mAP for our SSD.
0.20 (we; note; that the resulting system improves accuracy on real-time detection for PASCAL VOC from 63.4% mAP for YOLO to 74.3% mAP for our SSD)
0.90 Context(we note):(the resulting system; improves; accuracy on real-time detection for PASCAL VOC)

We summarize our contributions as follows: – We introduce SSD, a single-shot detector for multiple categories that is faster than the previous state-of-the-art for single shot detectors (YOLO), and signiﬁcantly more accurate, in fact as accurate as slower techniques that perform explicit region proposals and pooling (including Faster R-CNN).
0.44 (We; introduce; SSD)
0.26 Context(We introduce):(We; summarize; our contributions; T:as follows)
0.97 (a single-shot detector for multiple categories; is; faster than the previous state-of-the-art for single shot detectors)
0.89 (slower techniques; perform; explicit region proposals)

– To achieve high detection accuracy we produce predictions of different scales from feature maps of different scales, and explicitly separate predictions by aspect ratio.
0.45 (we; produce; predictions of different scales)

2 The Single Shot Detector (SSD) This section describes our proposed SSD framework for detection (Sec.
0.83 (This section; describes; our proposed SSD framework for detection)

In a convolutional fashion, we evaluate a small set (e.g.
0.60 (we; evaluate; a small set (e.g.; L:In a convolutional fashion)

For each default box, we predict both the shape offsets and the conﬁdences for all object categories ((c1, c2,··· , cp)).
0.57 (we; predict; both the shape offsets and the conﬁdences for all object categories)

At training time, we ﬁrst match these default boxes to the ground truth boxes.
0.66 (we; ﬁrst match; these default boxes; T:At training time)

For example, we have matched two default boxes with the cat and one with the dog, which are treated as positives and the rest as negatives.
0.57 (we; have matched; two default boxes with the cat and one with the dog)
0.89 (the dog; are treated; as positives and the rest as negatives)

The early network layers are based on a standard architecture used for high quality image classiﬁcation (truncated before any classiﬁcation layers), which we will call the base network2.
0.92 (The early network layers; are based; on a standard architecture)
0.91 (a standard architecture; used; for high quality image classiﬁcation)
0.94 (high quality image classiﬁcation (truncated before any classiﬁcation layers; will call; the base network2)

We then add auxiliary structure to the network to produce detections with the following key features: Multi-scale feature maps for detection We add convolutional feature layers to the end of the truncated base network.
0.52 (We; add; convolutional feature layers; to the end of the truncated base network)
0.56 (We; add; auxiliary structure; to the network; T:then)
0.26 Context(We add):(We; add auxiliary structure to produce; detections)

The bounding box offset output values are measured relative to a default 2 We use the VGG-16 network as a base, but other networks should also produce good results.
0.93 (The bounding box offset output values; are measured; relative to a default 2 We use the VGG-16 network as a base, but other networks should also produce good results)
0.90 (other networks; should also produce; good results)

Our SSD model adds several feature layers to the end of a base network, which predict the offsets to default boxes of different scales and aspect ratios and their associated conﬁdences.
0.78 (Our SSD model; adds; several feature layers; to the end of a base network)
0.88 (a base network; predict; the offsets to default boxes of different scales and aspect ratios and their associated conﬁdences)

Default boxes and aspect ratios We associate a set of default bounding boxes with each feature map cell, for multiple feature maps at the top of the network.
0.45 (We; associate; a set of default bounding boxes; with each feature map cell)

At each feature map cell, we predict the offsets relative to the default box shapes in the cell, as well as the per-class scores that indicate the presence of a class instance in each of those boxes.
0.70 (we; predict; the offsets relative to the default box shapes in the cell, as well as the per-class scores; L:At each feature map cell)
0.86 (the per-class scores; indicate; the presence of a class instance in each of those boxes)

Speciﬁcally, for each box out of k at a given location, we compute c class scores and the 4 offsets relative to the original default box shape.
0.57 (we; compute; c class scores and the 4 offsets relative to the original default box shape)

Our default boxes are similar to the anchor boxes used in Faster R-CNN [2], however we apply them to several feature maps of different resolutions.
0.93 (the anchor boxes; used; L:in Faster R-CNN)
0.26 (we; apply; them; to several feature maps of different resolutions)
0.60 Context(we apply):(Our default boxes; are; similar to the anchor boxes)

3003003VGG-16 through Conv5_3 layer1919Conv7(FC7)10241010Conv8_251255Conv9_22563Conv10_22562563838Conv4_331ImageConv: 1x1x1024Conv: 1x1x256Conv:  3x3x512-s2Conv: 1x1x128Conv: 3x3x256-s2Conv: 1x1x128Conv: 3x3x256-s1Detections:8732  per ClassClassifier : Conv: 3x3x(4x(Classes+4))5124484483Image7710247730Fully ConnectedYOLO Customized ArchitectureNon-Maximum Suppression Fully ConnectedNon-Maximum Suppression  Detections: 98 per classConv11_274.3mAP   59FPS63.4mAP   45FPSClassifier : Conv: 3x3x(6x(Classes+4))1919Conv6(FC6)1024Conv: 3x3x1024SSDYOLOExtra Feature LayersConv: 1x1x128Conv: 3x3x256-s1Conv: 3x3x(4x(Classes+4))SSD: Single Shot MultiBox Detector Matching strategy During training we need to determine which default boxes correspond to a ground truth detection and train the network accordingly.
0.89 (which default boxes; train accordingly; the network)
0.92 (Single Shot MultiBox Detector Matching strategy During training; need; to determine which default boxes correspond to a ground truth detection and train the network accordingly)
0.46 Context(Single Shot MultiBox Detector Matching strategy During training need):(we; to determine; which default boxes correspond to a ground truth detection and train the network accordingly)
0.89 Context(Single Shot MultiBox Detector Matching strategy During training we need to determine):(which default boxes; correspond; to a ground truth detection)

For each ground truth box we are selecting from default boxes that vary over location, aspect ratio, and scale.
0.91 (each ground truth box; are selecting; from default boxes)
0.89 (default boxes; vary; L:over location, aspect ratio, and scale)

We begin by matching each ground truth box to the default box with the best jaccard overlap (as in MultiBox [7]).
0.61 (We; begin; by matching each ground truth box to the default box with the best jaccard overlap (as in MultiBox [7)

Unlike MultiBox, we then match default boxes to any ground truth with jaccard overlap higher than a threshold (0.5).
0.22 (we; match higher; T:then)

In the matching strategy above, we can have(cid:80) Training objective The SSD training objective is derived from the MultiBox objective [7,8] but is extended to handle multiple object categories.
0.65 (we; can have; Training objective The SSD training objective is derived from the MultiBox objective; T:In the matching strategy above)
0.94 Context(we can have):(Training; objective; The SSD training objective is derived from the MultiBox objective)
0.93 Context(we can have Training objective):(The SSD training objective; is derived; from the MultiBox objective)

Similar to Faster R-CNN [2], we regress to offsets for the center (cx, cy) of the default bounding box (d) and for its width (w) and height (h).
0.50 (we; regress; to offsets for the center (cx, cy) of the default bounding box)
0.50 Context(we regress):(we; regress to offsets; for the center (cx, cy) of the default bounding box)

However, by utilizing feature maps from several different layers in a single network for prediction we can mimic the same effect, while also sharing parameters across all object scales.
0.39 (we; can mimic; the same effect)
0.29 Context(we can mimic):(we; can mimic the same effect also sharing; parameters across all object scales)

(cid:88) i∈P os (cid:17) (cid:16) gw j − dcx ˆgw j = log i − ˆgm j ) (cid:17) (cid:16) gh j − dcy i )/dh i Motivated by these methods, we use both the lower and upper feature maps for detection.
0.57 (i; −; ^gm j)
0.40 (we; use; both the lower and upper feature maps for detection)

In practice, we can use many more with small computational overhead.
0.23 (we; can use; many more)

We design the tiling of default boxes so that speciﬁc feature maps learn to be responsive to particular scales of the objects.
0.45 (We; design; the tiling of default boxes)
0.91 (speciﬁc feature maps; learn; to be responsive to particular scales of the objects)
0.90 (speciﬁc feature maps; to be; responsive to particular scales of the objects)

Suppose we want to use m feature maps for prediction.

We can compute the width (wa where smin is 0.2 and smax is 0.9, meaning the lowest layer has a scale of 0.2 and the highest layer has a scale of 0.9, and all layers in between are regularly spaced.
0.45 (We; can compute; the width)
0.91 (smin; is; 0.2 and smax; L:wa)
0.77 (all layers in between; are regularly spaced; )

We impose different aspect ratios for the default boxes, and denote them as ar ∈ k = sk/√ar) {1, 2, 3, 1 for each default box.
0.45 (We; impose; different aspect ratios for the default boxes)
0.27 (We; denote; them; as ar ∈ k = sk/√ar)

For the aspect ratio of 1, we also add a default box whose scale is s(cid:48) k = √sksk+1, resulting in 6 default boxes per feature map location.
0.60 (we; add; a default box whose scale is s(cid:48) k = √sksk+1,)

We set the center of each default box to ( i+0.5 |fk| ), where |fk| is the size of the k-th square feature map, i, j ∈ [0,|fk|).
0.18 (i; j; )
0.47 Context(i j):(We; set; the center of each default box; to ( i+0.5 |)

By combining predictions for all default boxes with different scales and aspect ratios from all locations of many feature maps, we have a diverse set of predictions, covering various input object sizes and shapes.
0.57 (we; have; a diverse set of predictions, covering various input object sizes and shapes)

Instead of using all the negative examples, we sort them using the highest conﬁdence loss for each default box and pick the top ones so that the ratio between the negatives and positives is at most 3:1.
0.48 (we; pick; the top ones)
0.88 (the ratio between the negatives and positives; is; at most 3:1)
0.31 (we; sort; them)
0.39 Context(we sort):(we; sort them using; the highest conﬁdence loss for each default box)

We found that this leads to faster optimization and a more stable training.
0.28 (We; found; that this leads to faster optimization and a more stable training)
0.33 Context(We found):(this; leads; to faster optimization and a more stable training)

We keep the overlapped part of the ground truth box if the center of is between 1 it is in the sampled patch.
0.45 (We; keep; the overlapped part of the ground truth box)
0.79 (the center of; is; between 1)
0.47 Context(the center of is):(it; is; in the sampled patch)

SSD: Single Shot MultiBox Detector Base network Our experiments are all based on VGG16 [15], which is pre-trained on the ILSVRC CLS-LOC dataset [16].
0.58 (Our experiments; are based; on VGG16 [15)
0.93 (VGG16 [15; is pre-trained; L:on the ILSVRC CLS-LOC dataset)

Similar to DeepLab-LargeFOV [17], we convert fc6 and fc7 to convolutional layers, subsample parameters from fc6 and fc7, change pool5 from 2 × 2 − s2 to 3 × 3 − s1, and use the `a trous algorithm [18] to ﬁll the ”holes”.
0.45 (we; convert; fc6; to convolutional layers)
0.53 (we; use; the `a trous algorithm [18] to ﬁll the "holes")

We remove all the dropout layers and the fc8 layer.
0.45 (We; remove; all the dropout layers and the fc8 layer)

We ﬁne-tune the resulting model using SGD with initial learning rate 10−3, 0.9 momentum, 0.0005 weight decay, and batch size 32.

The learning rate decay policy is slightly different for each dataset, and we will describe details later.
0.94 (The learning rate decay policy; is; slightly different for each dataset)
0.45 (we; will describe; details; T:later)

On this dataset, we compare against Fast R-CNN [6] and Faster R-CNN [2] on VOC2007 test (4952 images).
0.74 (we; compare; against Fast R-CNN [6] and Faster R-CNN [2] on VOC2007 test; L:On this dataset)

We use conv4 3, conv7 (fc7), conv8 2, conv9 2, conv10 2, and conv11 2 to predict both location and conﬁdences.
0.57 (We; use; conv4 3, conv7 (fc7), conv8 2)

We set default box with scale 0.1 on conv4 33.
0.52 (We; set; default box)

We initialize the parameters for all the newly added convolutional layers with the ”xavier” method [20].
0.57 (We; initialize; the parameters for all the newly added convolutional layers with the "xavier" method [20)

For conv4 3, conv10 2 and conv11 2, we only associate 4 default boxes at each feature map location – omitting aspect ratios of 1 3 and 3.
0.57 (we; only associate; 4 default boxes; at each feature map location - omitting aspect ratios of 1 3 and 3)

For all other layers, we put 6 default boxes as described in Sec.
0.57 (we; put; 6 default boxes as described in Sec)

Since, as pointed out in [12], conv4 3 has a different feature scale compared to the other layers, we use the L2 normalization technique introduced in [12] to scale the feature norm at each location in the feature map to 20 and learn the scale during back propagation.
0.94 (conv4 3; has; a different feature scale)
0.43 (we; use; the L2 normalization technique)
0.29 Context(we use):(we; use the L2 normalization technique to learn; the scale; T:during back propagation)
0.96 (the L2 normalization technique; introduced; T:in [12; to scale the feature norm at each location in the feature map to 20 and learn the scale during back propagation)

We use the 10−3 learning rate for 40k iterations, then continue training for 10k iterations with 10−4 and 10−5.

When training on VOC2007 trainval, Table 1 shows that our low resolution SSD300 model is already more accurate than Fast R-CNN.
0.90 (Table 1; shows; that our low resolution SSD300 model is already more accurate than Fast R-CNN; T:When training on VOC2007 trainval)
0.74 Context(Table 1 shows):(our low resolution SSD300 model; is; T:already; more accurate than Fast R-CNN)

When we train SSD on a larger 512 × 512 input image, it is even more accurate, surpassing Faster R-CNN by 1.7% mAP.
0.50 (we; train; SSD; on a larger 512 × 512 input image)
0.67 (it; is; even more accurate; T:When we train SSD on a larger 512 × 512 input image)

If we train SSD with more (i.e.
0.50 (we; train; SSD; with more)

07+12) data, we see that SSD300 is already better than Faster R-CNN by 1.1% and that SSD512 is 3.6% better.
0.94 (SSD512; is; 3.6% better)
0.31 (we; see; that SSD300 is already better than Faster R-CNN by 1.1% and that SSD512 is 3.6% better)
0.93 Context(we see):(SSD300; is; T:already; better than Faster R-CNN by 1.1%)

If we take models trained on COCO trainval35k as described in Sec.
0.50 (we; take; models trained on COCO trainval35k)
0.91 (models; trained; on COCO trainval35k)

3.4 and ﬁne-tuning them on the 07+12 dataset with SSD512, we achieve the best results: 81.6% mAP.
0.45 (we; achieve; the best results)

To understand the performance of our two SSD models in more details, we used the detection analysis tool from [21].
0.39 (we; used; the detection analysis tool)

However, SSD has more confusions with similar object categories (especially for animals), partly because we share locations for multiple categories.
0.87 (SSD; has; more confusions with similar object categories (especially for animals; partly because we share locations for multiple categories)

In other words, it has much worse performance on smaller 3 For SSD512 model, we add extra conv12 2 for prediction, set smin to 0.15, and 0.07 on conv4 3.
0.90 (prediction; set; smin to 0.15, and 0.07 on conv4 3)
0.46 (we; add; extra conv12 2 for prediction, set smin; to 0.15)
0.46 Context(we add):(it; has; much worse performance on smaller 3)

On the positive side, we can clearly see that SSD performs really well on large objects.
0.33 (we; can clearly see; that SSD performs really well on large objects; L:On the positive side)
0.57 Context(we can clearly see):(SSD; performs really; )

And it is very robust to different object aspect ratios because we use default boxes of various aspect ratios per feature map location.
0.49 (it; is; very robust; because we use default boxes of various aspect ratios per feature map location)
0.57 (we; use; default boxes of various aspect ratios per feature map location)

To understand SSD better, we carried out controlled experiments to examine how each component affects performance.
0.39 (we; carried out; controlled experiments; to examine how each component affects performance)
0.39 Context(we carried out):(we; carried out controlled experiments to examine; how each component affects performance)
0.88 Context(we carried out to examine):(each component; affects; performance)

For all the experiments, we use the same settings and input size (300 × 300), except for speciﬁed changes to the settings or component(s).
0.57 (we; use; the same settings and input size (300 × 300)

We use a more extensive sampling strategy, similar to YOLO [5].
0.61 (We; use; a more extensive sampling strategy, similar to YOLO [5)

Table 2 shows that we can improve 8.8% mAP with this sampling strategy.
0.45 (we; can improve; 8.8% mAP with this sampling strategy)

We do not know how much our sampling strategy will beneﬁt Fast and Faster R-CNN, but they are likely to beneﬁt less because they use a feature pooling step during classiﬁcation that is relatively robust to object translation by design.
0.40 (We; do not know; how much our sampling strategy will beneﬁt Fast and Faster R-CNN)
0.64 Context(We do not know):(our sampling strategy; will beneﬁt; Fast and Faster R-CNN)
0.48 (they; use; a feature pooling step during classiﬁcation that is relatively robust to object translation by design)
0.83 (feature; pooling; step during classiﬁcation that is relatively robust to object translation by design)
0.94 (a feature pooling step during classiﬁcation; is; relatively robust to object translation by design)
0.36 (they; are; likely to beneﬁt less)
0.42 (they; to beneﬁt; less; because they use a feature pooling step during classiﬁcation)

2.2, by default we use 6 3 and 3 aspect ratios, the default boxes per location.
0.45 (we; use; 6 3 and 3 aspect ratios)

If we remove the boxes with 1 performance drops by 0.6%.
0.45 (we; remove; the boxes; with 1 performance drops by 0.6%)

3, we used the atrous version of a subsampled VGG16, following DeepLab-LargeFOV [17].
0.74 (we; used; the atrous version of a subsampled VGG16, following DeepLab-LargeFOV [17; T:3)

If we use the full VGG16, keeping pool5 with 2× 2− s2 and not subsampling parameters from fc6 and fc7, and add conv5 3 for prediction, the result is about the same while the speed is about 20% slower.
0.40 (we; use; the full VGG16)
0.30 Context(we use):(we; use the full VGG16 keeping; pool5; with 2× 2− s2)
0.30 Context(we use):(we; use the full VGG16 not subsampling; parameters from fc6 and fc7)
0.93 (the speed; is; about 20% slower)
0.81 (the result; is; about the same)

To measure the advantage gained, we progressively remove layers and compare results.
0.45 (we; progressively remove; layers)
0.41 (we; compare; results)

For a fair comparison, every time we remove a layer, we adjust the default box tiling to keep the total number of boxes similar to the original (8732).
0.60 (we; remove; a layer; T:every time)
0.56 (we; adjust; the default box tiling to keep the total number of boxes similar to the original (8732); T:every time we remove a layer)
0.75 (the default box; tiling; )

We do not exhaustively optimize the tiling for each setting.
0.45 (We; do not exhaustively optimize; the tiling)

When we stack boxes of multiple scales on a layer, many are on the image boundary and need to be handled carefully.
0.52 (we; stack; boxes of multiple scales; L:on a layer)
0.65 (many; are; on the image boundary; T:When we stack boxes of multiple scales on a layer)

We tried the strategy used in Faster R-CNN [2], ignoring boxes which are on the boundary.
0.92 (the strategy; used; L:in Faster R-CNN)
0.88 (boxes; are; on the boundary)
0.55 (We; tried; the strategy used in Faster R-CNN [2])
0.29 Context(We tried):(We; tried the strategy used in Faster R-CNN [2] ignoring; boxes which are on the boundary)

We observe some interesting trends.
0.45 (We; observe; some interesting trends)

For example, it hurts the performance by a large margin if we use very coarse feature maps (e.g.
0.52 (it; hurts; the performance by a large margin)
0.44 (we; use very coarse e.g.; feature maps)

The reason might be that we do not have enough large boxes to cover large objects after the pruning.
0.91 (enough large boxes; to cover; large objects after the pruning)
0.59 (The reason; might be; that we do not have enough large boxes)
0.50 Context(The reason might be):(we; do not have; enough large boxes to cover large objects after the pruning)

When we use primarily ﬁner resolution maps, the performance starts increasing again because even after pruning a sufﬁcient number of large boxes remains.
0.45 (we; use; primarily ﬁner resolution maps)
0.91 (the performance; starts; T:When we use primarily ﬁner resolution maps)
0.74 (the performance; increasing; T:again)

If we only use conv7 for prediction, the performance is the worst, reinforcing the message that it is critical to spread boxes of different scales over different layers.
0.45 (we; only use; conv7 for prediction)
0.81 (the performance; is; the worst)
0.64 (it; is; critical to spread boxes of different scales over different layers)

Besides, since our predictions do not rely on ROI pooling as in [6], we do not have the collapsing bins problem in low-resolution feature maps [23].
0.64 (our predictions; do not rely; on ROI pooling)

SSD: Single Shot MultiBox Detector We use the same settings as those used for our basic VOC2007 experiments above, except that we use VOC2012 trainval and VOC2007 trainval and test (21503 images) for training, and test on VOC2012 test (10991 images).
0.23 (We; use; the same settings as those)
0.17 (those; used; for our basic VOC2007 experiments above)
0.50 (we; use; VOC2012 trainval and VOC2007 trainval and test)

We train the models with 10−3 learning rate for 60k iterations, then 10−4 for 20k iterations.
0.44 (We; train then; the models with 10−3 learning rate for 60k iterations)
0.54 (10−3; learning; rate; for 60k iterations)

Table 4 shows the results of our SSD300 and SSD5124 model.
0.49 (Table 4; shows; the results of our SSD300 and SSD5124 model)

We see the same performance trend as we observed on VOC2007 test.
0.46 (We; see; the same performance trend as we observed on VOC2007 test)
0.50 (we; observed; L:on VOC2007 test)

Our SSD300 improves accuracy over Fast/Faster RCNN.
0.74 (Our SSD300; improves; accuracy over Fast/Faster RCNN)

By increasing the training and testing image size to 512× 512, we are 4.5% more accurate than Faster R-CNN.
0.50 (we; are; 4.5% more accurate than Faster R-CNN)

Compared to YOLO, SSD is signiﬁcantly more accurate, likely due to the use of convolutional default boxes from multiple feature maps and our matching strategy during training.
0.87 (SSD; is; signiﬁcantly more accurate, likely due to the use of convolutional default boxes from multiple feature maps and our matching strategy during training)

When ﬁne-tuned from models trained on COCO, our SSD512 achieves 80.0% mAP, which is 4.1% higher than Faster R-CNN.
0.91 (models; trained; on COCO)
0.89 (our SSD512; achieves; 80.0% mAP, which is 4.1% higher than Faster R-CNN; T:When ﬁne-tuned from models)
0.93 (80.0% mAP; is; 4.1% higher than Faster R-CNN)

To further validate the SSD framework, we trained our SSD300 and SSD512 architectures on the COCO dataset.
0.35 (we; trained; our SSD300 and SSD512 architectures; on the COCO dataset)

Since objects in COCO tend to be smaller than PASCAL VOC, we use smaller default boxes for all layers.
0.96 (objects in COCO; tend; to be smaller than PASCAL VOC)
0.93 (objects in COCO; to be; smaller than PASCAL VOC)
0.45 (we; use; smaller default boxes for all layers)

We follow the strategy mentioned in Sec.
0.50 (We; follow; the strategy mentioned in Sec)
0.92 (the strategy; mentioned; L:in Sec)

2.2, but now our smallest default box has a scale of 0.15 instead of 0.2, and the scale of the default box on conv4 3 is 0.07 (e.g.
0.84 (our smallest default box; has; a scale of 0.15 instead of 0.2; T:now)
0.86 (the scale of the default box on conv4 3; is e.g.; 0.07)

We use the trainval35k [24] for training.
0.45 (We; use; the trainval35k; for training)

We ﬁrst train the model with 10−3 learning rate for 160k iterations, and then continue training for 40k iterations with 10−4 and 40k iterations with 10−5.
0.45 (We; ﬁrst train; the model; with 10−3 learning rate; T:for 160k iterations)
0.67 (We; continue; training for 40k iterations with 10−4 and 40k iterations with 10−5; T:then)

Similar to what we observed on the PASCAL VOC dataset, SSD300 is better than Fast R-CNN in both mAP@0.5 and mAP@[0.5:0.95].
0.50 (we; observed; L:on the PASCAL VOC dataset)
0.95 (SSD300; is; better than Fast R-CNN in both mAP@0.5)
0.82 (SSD300; mAP; @[0.5:0.95)
0.38 (SSD300; is better than; Fast R - CNN)

By increasing the image size to 512 × 512, our SSD512 is better than Faster R-CNN [25] in both criteria.
0.79 (our SSD512; is; better than Faster R-CNN)
0.38 (our SSD512; is better than; Faster R)

Interestingly, we observe that SSD512 is 5.3% better in mAP@0.75, but is only 1.2% better in mAP@0.5.
0.48 (we; is; only 1.2% better in mAP@0.5.)

We also observe that it has much better AP (4.8%) and AR (4.6%) for large objects, but has relatively less improvement in AP (1.3%) and AR (2.0%) for 4 http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?cls=mean&challengeid=11&compid=4 5 For SSD512 model, we add extra conv12 2 for prediction, set smin to 0.1, and 0.04 on conv4 3.
0.64 (it; has; relatively less improvement in AP (1.3%) and AR (2.0%)
0.90 (prediction; set; smin to 0.1, and 0.04 on conv4 3)
0.46 (we; add; extra conv12 2 for prediction, set smin; to 0.1)
0.17 Context(we add):(We; observe; that it has much better AP (4.8%) and AR (4.6%) for large objects, but has relatively less improvement in AP (1.3%) and AR (2.0%) for 4 http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php)
0.62 Context(We observe we add):(it; has; much better AP (4.8%) and AR (4.6%)

We conjecture that Faster R-CNN is more competitive on smaller objects with SSD because it performs two box reﬁnement steps, in both the RPN part and in the Fast R-CNN part.
0.45 (it; performs; two box reﬁnement steps)
0.20 (We; conjecture; that Faster R-CNN is more competitive on smaller objects with SSD because it performs two box reﬁnement steps, in both the RPN part and in the Fast R-CNN part)
0.94 Context(We conjecture):(Faster R-CNN; is; more competitive on smaller objects with SSD because it performs two box reﬁnement steps, in both the RPN part and in the Fast R-CNN part)

5, we show some detection examples on COCO test-dev with the SSD512 model.
0.68 (we; show; some detection examples on COCO test-dev with the SSD512 model)

We applied the same network architecture we used for COCO to the ILSVRC DET dataset [16].
0.46 (We; applied; the same network architecture we used for COCO to the ILSVRC DET dataset [16])

We train a SSD300 model using the ILSVRC2014 DET train and val1 as used in [22].
0.61 (We; train; a SSD300 model using the ILSVRC2014 DET train and val1)

We ﬁrst train the model with 10−3 learning rate for 320k iterations, and then continue training for 80k iterations with 10−4 and 40k iterations with 10−5.
0.45 (We; ﬁrst train; the model; with 10−3 learning rate; T:for 320k iterations)
0.67 (We; continue; training for 80k iterations with 10−4 and 40k iterations with 10−5; T:then)

We can achieve 43.4 mAP on the val2 set [22].
0.45 (We; can achieve; 43.4 mAP; T:on the val2 set)

3.6 Data Augmentation for Small Object Accuracy Without a follow-up feature resampling step as in Faster R-CNN, the classiﬁcation task for small objects is relatively hard for SSD, as demonstrated in our analysis (see Fig.
0.87 (the classiﬁcation task for small objects; is; relatively hard)

To implement a ”zoom out” operation that creates more small training examples, we ﬁrst randomly place an image on a canvas of 16× of the original image size ﬁlled with mean values before we do any random crop operation.
0.92 (a "zoom out" operation; creates; more small training examples)
0.42 (we; ﬁrst randomly place; an image; on a canvas of 16× of the original image size; T:before we do any random crop operation)
0.81 (the original image size; ﬁlled; )
0.52 (we; do; any random crop operation)

Because we have more training images by introducing this new ”expansion” data augmentation trick, we have to double the training iterations.
0.41 (we; to double; the training iterations)
0.39 (we; have; more training images)
0.39 Context(we have):(we; have more training images by introducing; this new "expansion" data augmentation trick)

We have seen a consistent increase of 2%-3% mAP across multiple datasets, as shown in Table 6.
0.57 (We; have seen; a consistent increase of 2%-3% mAP across multiple datasets)

We leave this for future work.
0.45 (We; leave; this; for future work)

We show detections with scores higher than 0.6.
0.52 (We; show; detections)

SSD300 SSD512 SSD300* SSD512* Table 6: Results on multiple datasets when we add the image expansion data augmentation trick.
0.52 (we; add; the image expansion data augmentation trick)

Considering the large number of boxes generated from our method, it is essential to perform non-maximum suppression (nms) efﬁciently during inference.
0.82 (boxes; generated; from our method)

By using a conﬁdence threshold of 0.01, we can ﬁlter out most boxes.
0.45 (we; can ﬁlter out; most boxes)

We then apply nms with jaccard overlap of 0.45 per class and keep the top 200 detections per image.
0.60 (We; apply; nms with jaccard overlap of 0.45 per class; T:then)
0.55 (We; keep; the top 200 detections per image; T:then)

We measure the speed with batch size 8 using Titan X and cuDNN v4 with Intel Xeon E5-2667v3@3.20GHz.
0.45 (We; measure; the speed)

Both our SSD300 and SSD512 method outperforms Faster R-CNN in both speed and accuracy.

To the best of our knowledge, SSD300 is the ﬁrst real-time method to achieve above 70% mAP.
0.96 (SSD300; is; the ﬁrst real-time method to achieve above 70% mAP)
0.93 (the ﬁrst real-time method; to achieve; L:above 70% mAP)

Note that about 80% of the forward time is spent on the base network (VGG16 in our case).

Our SSD is very similar to the region proposal network (RPN) in Faster R-CNN in that we also use a ﬁxed set of (default) boxes for prediction, similar to the anchor boxes in the RPN.
0.86 (Our SSD; is; very similar to the region proposal network (RPN) in Faster R-CNN)
0.61 (we; also use; a ﬁxed set of (default) boxes for prediction, similar to the anchor boxes in the RPN)

But instead of using these to pool features and evaluate another classiﬁer, we simultaneously produce a score for each object category in each box.
0.55 (we; produce; a score for each object category in each box; T:simultaneously)

Thus, our approach avoids the complication of merging RPN with Fast R-CNN and is easier to train, faster, and straightforward to integrate in other tasks.
0.77 (our approach; avoids; the complication of merging RPN with Fast R-CNN)
0.76 (our approach; is; easier to train, faster, and straightforward to integrate in other tasks)

Another set of methods, which are directly related to our approach, skip the proposal step altogether and predict bounding boxes and conﬁdences for multiple categories directly.
0.80 (methods; are directly related; to our approach)
0.90 (Another set of methods; skip altogether; the proposal step)

Our SSD method falls in this category because we do not have the proposal step but use the default boxes.
0.75 (Our SSD method; falls; L:in this category; because we do not have the proposal step but use the default boxes)
0.45 (we; do not have; the proposal step)
0.41 (we; use; the default boxes)

However, our approach is more ﬂexible than the existing methods because we can use default boxes of different aspect ratios on each feature location from multiple feature maps at different scales.
0.67 (our approach; is; more ﬂexible than the existing methods; because we can use default boxes of different aspect ratios on each feature location from multiple feature maps at different scales)
0.45 (we; can use; default boxes of different aspect ratios; L:on each feature location)

If we only use one default box per location from the topmost feature map, our SSD would have similar architecture to OverFeat [4]; if we use the whole topmost feature map and add a fully connected layer for predictions instead of our convolutional predictors, and do not explicitly consider multiple aspect ratios, we can approximately reproduce YOLO [5].
0.45 (we; only use; one default box per location)
0.41 (we; do not explicitly consider; multiple aspect ratios)
0.43 (we; can approximately reproduce; YOLO [5)
0.65 Context(we can approximately reproduce):(our SSD; would have; similar architecture)
0.45 (we; use; the whole topmost feature map)
0.45 (we; add; a fully connected layer for predictions instead of our convolutional predictors)

A key feature of our model is the use of multi-scale convolutional bounding box outputs attached to multiple feature maps at the top of the network.
0.82 (A key feature of our model; is; the use of multi-scale convolutional bounding box outputs attached to multiple feature maps at the top of the network)
0.78 Context(A key feature of our model is):(the use of multi-scale convolutional bounding box; outputs attached; )

We experimentally validate that given appropriate training strategies, a larger number of carefully chosen default bounding boxes results in improved performance.
0.33 (We; experimentally validate; that given appropriate training strategies, a larger number of carefully chosen default bounding boxes results in improved performance)
0.76 (that given appropriate training strategies; bounding; )

We build SSD models with at least an order of magnitude more box predictions sampling location, scale, and aspect ratio, than existing methods [5,7].
0.50 (We; build; SSD models)
0.91 (more box predictions; sampling; location, scale, and aspect ratio)

We demonstrate that given the same VGG-16 base architecture, SSD compares favorably to its state-of-the-art object detector counterparts in terms of both accuracy and speed.
0.20 (We; demonstrate; that given the same VGG-16 base architecture, SSD compares favorably to its state-of-the-art object detector counterparts in terms of both accuracy and speed)
0.80 Context(We demonstrate):(SSD; compares favorably; to its state-of-the-art object detector counterparts in terms of both accuracy and speed)

Our SSD512 model signiﬁcantly outperforms the state-of-theart Faster R-CNN [2] in terms of accuracy on PASCAL VOC and COCO, while being 3× faster.
0.44 (Our SSD512 model; signiﬁcantly outperforms; )

Our real time SSD300 model runs at 59 FPS, which is faster than the current real time YOLO [5] alternative, while producing markedly superior detection accuracy.
0.78 (SSD300 model; runs; )
0.93 (59 FPS; is; faster than the current real time)

Apart from its standalone utility, we believe that our monolithic and relatively simple SSD model provides a useful building block for larger systems that employ an object detection component.
0.89 (larger systems; employ; an object detection component)

We would like to thank Alex Toshev for helpful discussions and are indebted to the Image Understanding and DistBelief teams at Google.
0.41 (We; are; indebted to the Image)
0.73 (the Image; Understanding; )
0.43 (We; would like; to thank Alex Toshev for helpful discussions)
0.43 Context(We would like):(We; would like to thank; Alex Toshev; for helpful discussions)

We also thank Philip Ammirato and Patrick Poirson for helpful comments.
0.53 (We; thank; Philip Ammirato and Patrick Poirson; for helpful comments)

We thank NVIDIA for providing GPUs and acknowledge support from NSF 1452851, 1446631, 1526367, 1533771
0.57 (We; thank; NVIDIA; for providing GPUs)
0.88 (NVIDIA; for providing; GPUs)
0.46 (We; acknowledge; support from NSF 1452851, 1446631)

