We present in this paper a systematic study on how to morph a well-trained neural network to a new one so that its network function can be completely preserved.
0.27 (its network function; can be preserved; )

We deﬁne this as network morphism in this research.
0.45 (We; deﬁne; this; as network morphism in this research)

To meet this requirement, we ﬁrst introduce the network morphism equations, and then develop novel morphing algorithms for all these morphing types for both classic and convolutional neural networks.
0.67 (we; develop; novel morphing algorithms for all these morphing types for both classic and convolutional neural networks; T:then)
0.39 (we; ﬁrst introduce; the network morphism equations)

We propose a family of parametric-activation functions to facilitate the morphing of any continuous non-linear activation neurons.
0.57 (We; propose; a family of parametric-activation functions to facilitate the morphing of any continuous non-linear activation neurons)

To accomplish such an ideal goal, we need to systematically study how to morph a well-trained neural network to a new one with its network function completely preserved.
0.33 (its network function; preserved; )
0.36 (we; need; to systematically study how to morph a well-trained neural network to a new one with its network function)
0.36 Context(we need):(we; need to systematically study; how to morph a well-trained neural network to a new one with its network function)
0.18 Context(we need to systematically study):(we; need to systematically study to morph; a well-trained neural network; to a new one with its network function)

We call such operations network morphism.
0.52 (We; call; such operations network morphism)

Although network morphism generally does not impose constraints on the architecture of the child network, we limit the investigation of network morphism to the expanding mode, which intuitively means that the child network is deeper and/or wider than its parent network.
0.90 (network morphism; generally does not impose; constraints; on the architecture of the child network)
0.45 (we; limit; the investigation of network morphism to the expanding mode)
0.68 (the expanding mode; intuitively means; that the child network is deeper and/or wider than its parent network)
0.85 Context(the expanding mode intuitively means):(the child network; is; deeper and/or wider than its parent network)

In this work, we derive network morphism equations for a successful morphing operation to follow, based on which novel network morphism algorithms can be developed for all these morphing types.
0.60 (we; derive; network morphism equations; for a successful morphing operation; L:In this work)
0.77 (a successful morphing operation; to follow; )
0.79 (which novel network morphism algorithms; can be developed; )

To overcome the issues associated with IdMorph, we introduce several practices for the morphism operation to follow, and propose a deconvolution-based algorithm for network depth morphing.
0.92 (the issues; associated; with IdMorph)
0.70 (several practices; to follow; )
0.41 (we; propose; a deconvolution-based algorithm for network depth morphing)
0.39 (we; introduce; several practices for the morphism operation to follow)

deal with the non-linearity, we introduce the concept of parametric-activation function family, which is deﬁned as an adjoint function family for arbitrary non-linear activation function.
0.45 (we; introduce; the concept of parametric-activation function family)
0.94 (parametric-activation function family; is deﬁned; as an adjoint function family for arbitrary non-linear activation function)

To the best of our knowledge, this is the ﬁrst work about network morphism, except the recent work (Chen et al., 2015) that introduces the IdMorph.
0.57 (this; is; the ﬁrst work about network morphism, except the recent work)
0.93 (Chen et al; introduces; the IdMorph)

We conduct extensive experiments to show the effectiveness of the proposed network morphism learning scheme on widely used benchmark datasets for both classic and convolutional neural networks.
0.50 (We; conduct; extensive experiments; to show the effectiveness of the proposed network morphism learning scheme on widely used benchmark datasets for both classic and convolutional neural networks)
0.50 Context(We conduct):(We; conduct extensive experiments to show; the effectiveness of the proposed network morphism learning scheme on widely used benchmark datasets for both classic and convolutional neural networks)

Furthermore, we show that the proposed network morphism is able to internally regularize the network, that typically leads to an improved performance.
0.85 (the proposed network morphism; to internally regularize; the network, that typically leads to an improved performance)
0.89 (the network; typically leads; to an improved performance)
0.33 (we; show; that the proposed network morphism is able to internally regularize the network)
0.92 Context(we show):(the proposed network morphism; is; able to internally regularize the network)

Finally, we also successfully morph the well-known 16layered VGG net (Simonyan & Zisserman, 2014) to a better 15 of the training time comperforming model, with only 1 paring against the training from scratch.
0.71 (we; successfully morph; the well-known 16layered VGG net; to a better 15 of the training time comperforming model; T:Finally)
0.90 (training time; comperforming; model)

Related Work We brieﬂy introduce recent work related to network morphism and identify the differences from this work.
0.45 (We; brieﬂy introduce; recent work related to network morphism)
0.90 (recent work; related; to network morphism)
0.41 (We; identify; the differences from this work)

Network Morphism We shall ﬁrst discuss the depth morphing in the linear case, which actually also involves with width and kernel size morphing.
0.60 (We; shall ﬁrst discuss; the depth morphing in the linear case; T:Network Morphism)
0.90 (the linear case; actually also involves; with width and kernel size morphing)

Then we shall describe how to deal with the non-linearities in the neural networks.
0.64 (we; shall describe; how to deal with the non-linearities in the neural networks; T:Then)
0.29 Context(we shall describe):(we; shall describe to deal; with the non-linearities in the neural networks)

Finally, we shall present the stand-alone versions for width morphing and kernel size morphing, followed by the subnet morphing.
0.57 (we; shall present; the stand-alone versions for width morphing and kernel size morphing, followed by the subnet morphing)
0.86 (the stand-alone versions for width morphing and kernel size morphing; followed; )

Next, we consider the case of a deep convolutional neural network (DCNN).

Thus, we call the hidden layers as blobs, and weight matrices as ﬁlters.
0.64 (we; call; the hidden layers as blobs, and weight matrices as ﬁlters)

We ﬁrst drop all the non-linear activation functions and consider a neural network only connected with fully connected layers.
0.52 (We; ﬁrst drop; all the non-linear activation functions)
0.41 (We; consider; a neural network only connected with fully connected layers)
0.75 (a neural network; only connected; )

For network morphism, we shall insert a new hidden layer Bl, so that the child network satisﬁes: Bl+1 = Fl+1 · Bl = Fl+1 · (Fl · Bl−1) = G · Bl−1, (2) where Bl ∈ RCl, Fl ∈ RCl×Cl−1, and Fl+1 ∈ RCl+1×Cl.
0.37 (we; shall insert; a new hidden layer Bl, so that the child network satisﬁes: Bl+1 = Fl+1 · Bl = Fl+1 · (Fl · Bl−1) = G · Bl−1, (2) where Bl ∈ RCl, Fl ∈ RCl×Cl−1, and Fl+1 ∈ RCl+1×Cl)

If ˜K = K, we will have ˜G = G.
0.50 (we; will have; ~G = G.)

Hence, we are able to unify them into one equation: ˜G = Fl+1 (cid:126) Fl, (6) where (cid:126) is a non-communicative operator that can either be an inner product or a multi-channel convolution.
0.27 (we; to unify; them; into one equation)
0.85 (a non-communicative operator; can be; an inner product or a multi-channel convolution)
0.92 (~G = Fl+1 (cid:126) Fl; is; a non-communicative operator that can either be an inner product or a multi-channel convolution)
0.26 Context(~G = Fl+1 ( cid:126 ) Fl is):(we; are; able to unify them into one equation)

We call Equation (6) as the network morphism equation (for depth in the linear case).
0.64 (We; call; Equation; as the network morphism equation (for depth in the linear case)

Network Morphism Algorithms: Linear Case In this section, we introduce two algorithms to solve for the network morphism equation (6).
0.51 (we; introduce; two algorithms to solve for the network morphism equation (6))
0.30 Context(we introduce):(we; introduce to solve; for the network morphism equation)

Since the solutions to Equation (6) might not be unique, we shall make the morphism operation to follow the desired practices that: 1) the parameters will contain as many nonzero elements as possible, and 2) the parameters will need to be in a consistent scale.
0.85 (the solutions to Equation (6; might not be; unique)
0.90 (the parameters; will need; to be in a consistent scale)
0.39 (we; shall make; the morphism operation to follow the desired practices)
0.82 Context(we shall make):(the morphism operation; to follow; the desired practices that: 1) the parameters will contain as many nonzero elements as possible, and 2) the parameters will need to be in a consistent scale)
0.89 (the parameters; to be; in a consistent scale)
0.90 (the parameters; will contain; as many nonzero elements as possible)

Next, we introduce two algorithms based on deconvolution to solve the network morphism equation (6), i.e., 1) general network morphism, and 2) practical network morphism.
0.64 (we; introduce; two algorithms based on deconvolution to solve the network morphism equation (6), i.e., 1) general network morphism, and 2) practical network morphism; L:Next)
0.40 Context(we introduce):(we; introduce to solve; the network morphism equation (6), i.e., 1) general network morphism, and 2) practical network morphism)

Then we iteratively solve Fl+1 and Fl by ﬁxing the other.
0.58 (we; iteratively solve; Fl+1 and Fl; by ﬁxing the other; T:Then)
0.18 Context(we iteratively solve):(we; iteratively solve Fl+1 and Fl by ﬁxing; the other)

We claim that if the parameter number of either Fl or Fl+1 is no less than ˜G, Algorithm 1 shall converge to 0.
0.97 (the parameter number of either Fl or Fl+1; is; no less than ~G)
0.32 (We; claim; that if the parameter number of either Fl or Fl+1 is no less than ~G, Algorithm 1 shall converge to 0)
0.78 Context(We claim):(Algorithm 1; shall converge; to 0)

Algorithm 2 Practical Network Morphism Input: G of shape (Cl+1, Cl−1, K, K); Cl, K1, K2 Output: Fl of shape (Cl, Cl−1, K1, K1), Fl+1 of shape (Cl+1, Cl, K2, K2) /* For simplicity, we illustrate this algorithm for the case ‘Fl expands G’ */ K r repeat Run Algorithm 1 with maxIter set to 1: l, Fl, F r NETMORPHGENERAL(G; Cl, K1, K r 2 ) K r 2 = K r until l = 0 Expand F r zeros.
0.58 (Fl; expands; )
0.40 Context(Fl expands):(we; illustrate; this algorithm; for the case)

Condition (7) claims that we have more unknowns than constraints, and hence it is an undetermined linear system.
0.65 (Condition; claims; that we have more unknowns than constraints, and hence it is an undetermined linear system)
0.40 Context(Condition claims):(we; have; more unknowns than constraints)

Next, we propose a variant of Algorithm 1 that can solve Equation (6) with a sacriﬁce in the non-sparse practice.
0.64 (we; propose; a variant of Algorithm 1; T:Next)
0.94 (a variant of Algorithm 1; can solve; Equation; with a sacriﬁce in the non-sparse practice)

Since we focus on network morphism in an expanding mode, we can assume that this condition is self-justiﬁed, namely, either Fl expands G, or Fl+1 expands G.
0.45 (we; focus; on network morphism)
0.88 (Fl+1; expands; G.)
0.31 (we; can assume; that this condition is self-justiﬁed, namely, either Fl expands G, or Fl+1 expands G.)
0.87 Context(we can assume):(this condition; is namely; self-justiﬁed)
0.88 (Fl; expands; G)

Thus, we can claim that this algorithm solves the network morphism equation (6).
0.19 (we; can claim; that this algorithm solves the network morphism equation)
0.88 Context(we can claim):(this algorithm; solves; the network morphism equation)

As described in Algorithm 2, for the case that Fl expands G, starting from 2 = K2, we iteratively call Algorithm 1 and shrink the K r size of K r 2 until the loss converges to 0.
0.77 (the loss; converges; to 0)
0.88 (Fl; expands; G)
0.86 (Fl; starting; from 2 = K2)
0.50 (we; iteratively call; Algorithm 1)
0.53 (we; shrink; the K r size of K r 2; T:until the loss converges to 0)

This iteration shall terminate as we are able to guarantee that if K r 2 = 1, the loss is 0.
0.78 (This iteration; shall terminate; T:as we are able to guarantee that if K r 2 = 1, the loss is 0)
0.37 (we; are; able to guarantee that if K r 2 = 1, the loss is 0)
0.73 (K; r; 2 = 1)
0.28 (we; to guarantee; that if K r 2 = 1, the loss is 0)
0.78 Context(we to guarantee):(the loss; is; 0)

We assume Cl+1 = O(Cl) (cid:44) O(C).
0.61 (We; assume; Cl+1 = O(Cl) (cid:44) O)

In practice, we shall also have C (cid:29) K, and thus NetMorph can asymptotically ﬁll in all parameters with non-zero elements.
0.39 (we; shall have; C (cid:29) K)
0.92 (NetMorph; can asymptotically ﬁll; L:in all parameters with non-zero elements)

Then we have ϕ(I (cid:126) ϕ(G (cid:126) Bl−1) = ϕ ◦ ϕ(G (cid:126) Bl+1) = ϕ(G (cid:126) Bl+1).
0.60 (we; have; ϕ; T:Then)

To handle arbitrary continuous non-linear activation functions, we propose to deﬁne the concept of P(arametric)activation function family.
0.51 (we; propose; to deﬁne the concept of P(arametric)activation function family)
0.51 Context(we propose):(we; propose to deﬁne; the concept of P(arametric)activation function family)

We deﬁne the canonical form for P-activation function family as follows: P -ϕ (cid:44) {ϕa}|a∈[0,1] = {(1 − a) · ϕ + a · ϕid}|a∈[0,1], (9) where a is the parameter to control the shape morphing of the activation function.
0.82 (the canonical form for P-activation function family; follows; )
0.57 (a; is; the parameter to control the shape morphing of the activation function)
0.90 (the parameter; to control; the shape morphing of the activation function)
0.82 (ϕ; +; a · ϕid)
0.51 Context(ϕ +):(We; deﬁne; the canonical form for P-activation function family as follows)

We have ϕ0 = ϕ, and ϕ1 = Figure 3: Non-zero element (indicated as gray) occupations of different algorithms: (a) IdMorph in O(C), (b) NetMorph worst case in O(C 2), and (c) NetMorph best case in O(C 2K 2).
0.41 (We; have ϕ0; ϕ)
0.74 (Non-zero element; indicated; as gray)

As shown, it is safe to add the non-linear activations indicated by the green boxes, but we need to make sure that the yellow box is equivalent to a linear activation initially.
0.93 (the yellow box; is; equivalent to a linear activation initially)
0.91 (the non-linear activations; indicated; by the green boxes)
0.28 (we; need; to make sure that the yellow box is equivalent to a linear activation initially)
0.28 Context(we need):(we; need to make; sure that the yellow box is equivalent to a linear activation initially)

Formally, we need to replace the layer Bl+1 = ϕ(G (cid:126) Bl+1) with two layers Bl+1 = ϕ(Fl+1 (cid:126) ϕa(Fl (cid:126) Bl−1)).
0.55 (we; need; to replace the layer Bl+1 = ϕ(G (cid:126) Bl+1) with two layers Bl+1 = ϕ)
0.39 Context(we need):(we; need to replace; the layer; with two layers Bl+1 = ϕ)

If we set a = 1, the morphing shall be successful as long as the network morphing equation (6) is satisﬁed: ϕ(Fl+1 (cid:126) ϕa(Fl (cid:126) Bl−1)) = ϕ(Fl+1 (cid:126) Fl (cid:126) Bl−1) The value of a shall be learned when we continue to train the model.
0.28 (we; set; a = 1)
0.94 (the morphing; shall be; successful; T:as long as the network morphing equation (6) is satisﬁed)
0.74 (the network morphing equation; is satisﬁed; )
0.39 (we; continue; to train the model)
0.39 Context(we continue):(we; continue to train; the model)

Therefore, we can conduct width and kernel size morphing by introducing an extra depth morphing via Algorithm 2.
0.45 (we; can conduct; width)
0.41 (we; kernel; size morphing)

Sometimes, we need to pay attention to stand-alone network width and kernel size morphing operations.
0.64 (we; need; to pay attention to stand-alone network width and kernel size morphing operations; T:Sometimes)
0.39 Context(we need):(we; need to pay; attention to stand-alone; network width and kernel size morphing operations)

In this section, we introduce solutions for these situations.
0.60 (we; introduce; solutions for these situations; L:In this section)

For width morphing, we assume Bl−1, Bl, Bl+1 are all parent network layers, and the target is to expand the width (channel size) of Bl from Cl to ˜Cl, ˜Cl ≥ Cl.
0.96 (the target; is; to expand the width (channel size) of Bl from Cl to ~Cl)
0.16 (we; assume; )
0.93 Context(we assume):(Bl−1, Bl, Bl+1; are; all parent network layers)

For the parent network, we have Figure 5: Network morphism in kernel size.
0.45 (we; have; Figure 5)

Thus, we only need to satisfy: Bl( ¯cl) ∗ ˜Fl+1(cl+1, ¯cl) Bl−1(cl−1) ∗ ˜Fl( ¯cl, cl−1) ∗ ˜Fl+1(cl+1, ¯cl), ˜Fl( ¯cl, cl−1) ∗ ˜Fl+1(cl+1, ¯cl) = 0.
0.18 (we; only need; to satisfy)
0.16 Context(we only need):(we; only need to satisfy to satisfy; )

is obvious that we can either set ˜Fl( ¯cl, cl−1) or It ˜Fl+1(cl+1, ¯cl) to 0, and the other can be set arbitrarily.
0.39 (we; can set; ~Fl( ¯cl, cl−1)
0.32 (the other; can be set; arbitrarily)

Following the non-sparse practice, we set the one with less parameters to 0, and the other one to random noises.
0.66 (we; set; the one; with less parameters to 0; T:Following the non-sparse practice)

To break this unwanted behavior, we perform a random permutation on ˜cl, which will not change Bl+1.
0.43 (we; perform; a random permutation; L:on ~cl)
0.33 Context(we perform):(we; perform a random permutation will not change; Bl+1)

For kernel size morphing, we propose a heuristic yet effective solution.
0.60 (we; propose; a heuristic; T:For kernel size morphing)

Suppose that a convolutional layer l has kernel size of Kl, and we want to expand it to ˜Kl.
0.36 (we; want; to expand it to ~Kl)
0.30 Context(we want):(we; want to expand; it; to ~Kl)

We study the problem of subnet morphing in this section, that is, network morphism from a minimal number (typically one) of layers in the parent network to a subnet in the child network.
0.45 (We; study; the problem of subnet morphing; L:in this section)

We ﬁrst describe the morphing operation for the sequential subnet, based on which its stacked version is then obtained.
0.45 (We; ﬁrst describe; the morphing operation)
0.40 (its stacked version; is obtained; T:then)

We can also develop a practical version of the algorithm that can solve for Equation (19), which is similar to Algorithm 2.
0.35 (We; can develop; a practical version of the algorithm)
0.90 (the algorithm; can solve; for Equation (19)
0.95 (Equation (19; is; similar to Algorithm 2)

For stacked sequential subnet morphing, we can follow the work ﬂow illustrated as Fig.
0.50 (we; can follow; the work ﬂow illustrated as Fig)
0.93 (the work ﬂow; illustrated; as Fig)

6(c), we illustrate an n-way stacked sequential subnet morphing, with the second path morphed into two layers.
0.45 (we; illustrate; an n-way stacked sequential subnet morphing)
0.91 (the second path; morphed; into two layers)

Experimental Results In this section, we conduct experiments on three datasets (MNIST, CIFAR10, and ImageNet) to show the effectiveness of the proposed network morphism scheme, on 1) different morphing operations, 2) both the classic and convolutional neural networks, and 3) both the idempotent activations (ReLU) and non-idempotent activations (TanH).
0.40 (we; conduct; experiments on three datasets)
0.30 Context(we conduct):(we; conduct experiments on three datasets to show; the effectiveness of the proposed network morphism scheme)

In this section, instead of using state-of-the-art DCNN solutions (LeCun et al., 1998; Chang & Chen, 2015), we adopt the simple softmax regression model as the parent network to evaluate the effectiveness of network morphism on classic networks.
0.74 (we; adopt; the simple softmax regression model as the parent network to evaluate the effectiveness of network morphism on classic networks; L:In this section, instead of using state-of-the-art DCNN solutions)

Then, we morphed this model into a multiple layer perception (MLP) model by adding a PReLU or PTanH hidden layer with the number of hidden neurons h = 50.
0.64 (we; morphed; this model; into a multiple layer perception (MLP) model; T:Then)

We can see that, for the PReLU activation, NetMorph works much better than Net2Net.
0.31 (We; can see; that, for the PReLU activation, NetMorph works much better than Net2Net)
0.72 Context(We can see):(NetMorph; works much better; )

NetMorph continues to 3In the experiments, we use NetMorph to represent the proimprove the performance from 92% to 97%, while Net2Net improves only to 94%.
0.92 (Net2Net; improves; only to 94%)
0.55 (we; use; NetMorph; to represent the proimprove the performance from 92% to 97%)
0.90 Context(we use):(NetMorph; continues; T:to 3In the experiments)
0.35 Context(we use):(we; use NetMorph to represent; the performance from 92% to 97%)

We also show the curve of NetMorph with the non-idempotent activation PTanH in Fig.
0.53 (We; show; the curve of NetMorph)

The baseline network we adopted is the Caffe (Jia et al., 2014) cifar10_quick model with an accuracy of 78.15%.
0.89 (The baseline network; adopted; we)
0.79 (The baseline network we adopted; is; the Caffe (Jia et al)

In the following, we use the uniﬁed notation cifar_ddd to represent a network architecture of three subnets, in which each digit d is the number of convolutional The deis described by tailed architecture of each subnet (<kernel_size>:<num_output>,...).
0.64 (we; use; the uniﬁed notation cifar_ddd to represent a network architecture of three subnets; L:In the following)
0.90 (the uniﬁed notation; to represent; a network architecture of three subnets)
0.96 (each digit d; is; the number of convolutional; L:three subnets)
0.94 (The deis; described; by tailed architecture of each subnet (<kernel_size>:<num_output>)

Additionally, we also use [<subnet>] to indicate the grouping of layers in a subnet, and x<times> to represent layers or subnets.
0.42 (we; use; subnet; to indicate the grouping of layers in a subnet, and x<times>)
0.26 Context(we use):(we; use to indicate; the grouping of layers in a subnet)
0.26 Context(we use):(we; use to represent; layers or subnets)

8(a) and (b), we can see the superiority of NetMorph over Net2Net.
0.50 (we; can see; the superiority of NetMorph over Net2Net)

We also veriﬁed this by comparing the histograms of the embedded ﬁlters (after morphing) for both methods.
0.16 (We; veriﬁed; this)
0.35 Context(We veriﬁed):(We; veriﬁed this by comparing; the histograms of the embedded ﬁlters (after morphing; for both methods)

Finally, we compare NetMorph with the model directly trained from scratch (denoted as Raw) in Fig.
0.64 (we; compare; NetMorph; with the model; T:Finally)
0.92 (the model; directly trained; from scratch; L:in Fig)
0.82 (scratch; denoted; as Raw)

We interpret this phenomena as the internal regularization ability of NetMorph.
0.50 (We; interpret; this phenomena; as the internal regularization ability of NetMorph)

We only need to explore for a relatively small region rather than the whole parameter space.
0.50 (We; only need; to explore for a relatively small region rather than the whole parameter space)
0.50 Context(We only need):(We; only need to explore; for a relatively small region rather than the whole parameter space)

We further double the number of channels (width) for the ﬁrst layer in each subnet (cifar_width).
0.45 (We; further double; the number of channels)

We also conducted width morphing directly from the parent network for TanH neurons, which results in about 4% accuracy improvement.
0.61 (We; also conducted; width morphing directly from the parent network for TanH neurons,)
0.84 (width; morphing directly; from the parent network for TanH neurons)
0.94 (the parent network for TanH neurons; results; in about 4% accuracy improvement)

We also conduct experiments on the ImageNet dataset (Russakovsky et al., 2014) with 1,000 object categories.
0.50 (We; also conduct; experiments on the ImageNet dataset)

Because the Caffe (Jia et al., 2014) implementation favors single-scale, for a fair comparison, we ﬁrst de-multiscale this model by continuing to train it on the ImageNet dataset with the images resized to 256 × 256.
0.91 (2014) implementation; favors; single-scale; for a fair comparison, we ﬁrst de-multiscale this model by continuing to train it on the ImageNet dataset with the images)
0.45 (we; ﬁrst de-multiscale; this model)
0.77 (the images; resized; to 256 × 256)

In this paper, we adopt the de-multiscaled version of the VGG16 net as the parent network to morph.
0.74 (we; adopt; the de-multiscaled version of the VGG16 net as the parent network to morph; L:In this paper)

The morphing operation we adopt is to add a convolutional layer at the end of the ﬁrst three subsets for each.
0.89 (The morphing operation; adopt; we)
0.83 (The morphing operation we adopt; is; to add a convolutional layer at the end of the ﬁrst three subsets for each)

We continue to train the child network after morphing, and the ﬁnal model is denoted as Table 3: Comparison results on ImageNet.
0.93 (the ﬁnal model; is denoted; as Table 3)
0.40 (We; continue; to train the child network after morphing)
0.40 Context(We continue):(We; continue to train; the child network after morphing)

We can see that, VGG16(NetMorph) not only outperforms its parent network, i.e, VGG16(baseline), but also outperforms the multi-scale version, i.e, VGG16(multi-scale).
0.10 (We; can see; that)
0.86 (VGG16(NetMorph; outperforms; its parent network)

Since VGG16(NetMorph) is a 19-layer network, we also list the VGG19 net in Table 3 for comparison.
0.93 (VGG16; is; a 19-layer network)
0.50 (we; also list; the VGG19 net in Table 3; for comparison)

Further, we are able to add more layers into VGG16(NetMorph), and a better performing model shall be expected.
0.50 (we; are; able to add more layers into VGG16)
0.46 (we; to add; more layers; into VGG16)
0.77 (a better performing model; shall be expected; )

We compare the training time cost for the NetMorph learning scheme and the training from scratch.
0.61 (We; compare; the training time cost for the NetMorph learning scheme and the training from scratch)

Conclusions In this paper, we have introduced the systematic study about network morphism.
0.45 (we; have introduced; the systematic study about network morphism)

We introduced diverse morphing operations, and developed novel morphing algorithms based on the morphism equations we have derived.
0.45 (We; introduced; diverse morphing operations)
0.41 (We; developed; novel morphing algorithms based on the morphism equations)
0.91 (novel morphing algorithms; based; on the morphism equations)
0.89 (the morphism equations; have derived; we)

Weisstein, Eric W

