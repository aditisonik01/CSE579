s c [     Deep Speech 2: End-to-End Speech Recognition in Baidu Research – Silicon Valley AI Lab∗ Dario Amodei, Rishita Anubhai, Eric Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Jingdong Chen, Mike Chrzanowski, Adam Coates, Greg Diamos, Erich Elsen, Jesse Engel, Linxi Fan, Christopher Fougner, Tony Han, Awni Hannun, Billy Jun, Patrick LeGresley, Libby Lin, Sharan Narang, Andrew Ng, Sherjil Ozair, Ryan Prenger, Jonathan Raiman, Sanjeev Satheesh, David Seetapun, Shubho Sengupta, Yi Wang, Zhiqian Wang, Chong Wang, Bo Xiao, Dani Yogatama, Jun Zhan, Zhenyao Zhu We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech—two vastly different languages.
0.95 (an end-to-end deep learning approach; to recognize; either English or Mandarin Chinese)
0.38 (We; show; that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech-two vastly different languages)
0.97 Context(We show):(an end-to-end deep learning approach; can be used; to recognize either English or Mandarin Chinese speech-two vastly different languages)
0.38 (Eric Battenberg; [is]; Carl Case)
0.38 (Jesse Engel; [is]; Linxi Fan)
0.38 (Jared Casper; [is] Case [of]; Carl)
0.38 (Christopher Fougner; [is] Fan [of]; Linxi)

Key to our approach is our application of HPC techniques, resulting in a 7x speedup over our previous system [26].

As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets.
0.81 (our system; is; competitive with the transcription of human workers; L:in several cases)

Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale.
0.92 (a technique; called; Batch Dispatch with GPUs)
0.32 (we; show; that our system can be inexpensively deployed in an online setting, delivering low latency; T:Finally)
0.58 Context(we show):(our system; can be inexpensively deployed; L:in an online setting)

We present the second generation of our speech system that exempliﬁes the major advantages of endto-end learning.
0.31 (We; present; the second generation of our speech system)
0.62 (our speech system; exempliﬁes; the major advantages of endto-end learning)

Since our system is built on end-to-end deep learning, we can employ a spectrum of deep learning techniques: capturing large training sets, training larger models with high performance computing, and methodically exploring the space of neural network architectures.
0.64 (our system; is built; L:on end-to-end deep learning)
0.50 (we; can employ; a spectrum of deep learning techniques; capturing large training sets, training larger models with high performance computing, and methodically exploring the space of neural network architectures)
0.29 Context(we can employ):(we; can employ a spectrum of deep learning techniques capturing; large training sets)
0.29 Context(we can employ):(we; can employ a spectrum of deep learning techniques training; larger models with high performance computing)

We show that through these techniques we are able to reduce error rates of our previous end-to-end system [26] in English by up to 43%, and can also recognize Mandarin speech with high accuracy.
0.38 (we; to reduce; error rates of our previous end-to-end system; by up to 43%)
0.46 (we; can also recognize; Mandarin speech with high accuracy)
0.25 (We; show; that through these techniques we are able to reduce error rates of our previous end-to-end system [26] in English by up to 43%, and can also recognize Mandarin speech with high accuracy)
0.41 Context(We show):(we; are; able to reduce error rates of our previous end-to-end system [26] in English by up to 43%)

To meet the expectations of speech recognition users, we believe that a single engine must learn to be similarly competent; able to handle most applications with only minor modiﬁcations and able to learn new languages from scratch without dramatic changes.
0.93 (only minor modiﬁcations and able; to learn; new languages; from scratch)
0.75 (a single engine; to be able; similarly competent)
0.27 (we; believe; that a single engine must learn to be similarly competent; able to handle most applications with only minor modiﬁcations and able to learn new languages from scratch without dramatic changes)
0.93 Context(we believe):(a single engine; must learn; to be similarly competent; able to handle most applications with only minor modiﬁcations and able to learn new languages from scratch without dramatic changes)
0.89 Context(we believe a single engine must learn):(a single engine; must learn to handle; most applications with only minor modiﬁcations and able to learn new languages from scratch without dramatic changes)

Our end-to-end system puts this goal within reach, allowing us to approach or exceed the performance of human workers on several tests in two very different languages: Mandarin and English.
0.35 (us; to exceed; the performance of human workers; on several tests in two very different languages)
0.74 (Our end-to-end system; puts; this goal; within reach)
0.56 Context(Our end - to - end system puts):(Our end-to-end system; puts this goal allowing; us to approach or exceed the performance of human workers on several tests in two very different languages)
0.16 Context(Our end - to - end system puts allowing):(us; to approach; )

Since Deep Speech 2 (DS2) is an end-to-end deep learning system, we can achieve performance gains by focusing on three crucial components: the model architecture, large labeled training datasets, and computational scale.
0.94 (Deep Speech 2; is; an end-to-end deep learning system)
0.45 (we; can achieve; performance gains)

This paper details our contribution to these three areas for speech recognition, including an extensive investigation of model architectures and the effect of data and model size on recognition performance.
0.89 (This paper; details; our contribution to these three areas for speech recognition, including an extensive investigation of model architectures and the effect of data and model size on recognition performance)

In particular, we describe numerous experiments with neural networks trained with the Connectionist Temporal Classiﬁcation (CTC) loss function [22] to predict speech transcriptions from audio.
0.45 (we; describe; numerous experiments with neural networks)
0.95 (neural networks; trained; with the Connectionist Temporal Classiﬁcation (CTC) loss function)

We consider networks composed of many layers of recurrent connections, convolutional ﬁlters, and nonlinearities, as well as the impact of a speciﬁc instance of Batch Normalization [63] (BatchNorm) applied to RNNs.
0.89 (networks; composed; of many layers of recurrent connections)
0.40 (We; consider; networks composed of many layers of recurrent connections)
0.98 Context(We consider):(convolutional ﬁlters, and nonlinearities, as well as the impact of a speciﬁc instance of Batch Normalization; applied; to RNNs)

We not only ﬁnd networks that produce much better predictions than those in previous work [26], but also ﬁnd instances of recurrent models that can be deployed in a production setting with no signiﬁcant loss in accuracy.
0.35 (We; ﬁnd; networks that produce much better predictions than those in previous work)
0.72 (networks; produce; much better predictions than those in previous work)
0.57 (26; ﬁnd; instances of recurrent models)
0.89 (recurrent models; can be deployed; L:in a production)
0.73 (a production; setting; )

We detail our data capturing pipeline that has enabled us to create larger datasets than what is typically used to train speech recognition systems.
0.21 (We; detail; our data capturing pipeline that has enabled us to create larger datasets than what is typically used to train speech recognition systems)
0.61 (our data capturing pipeline; has enabled; us; to create larger datasets than what is typically used to train speech recognition systems)
0.57 (us; to create; larger datasets than what is typically used to train speech recognition systems)

Our English speech system is trained on 11,940 hours of speech, while the Mandarin system is trained on 9,400 hours.
0.74 (Our English speech system; is trained; T:on 11,940 hours of speech)
0.93 (the Mandarin system; is trained; T:on 9,400 hours)

We use data synthesis to further augment the data during training.
0.45 (We; use; data synthesis; to further augment the data during training)

Indeed, our models have many more parameters than those used in our previous system.
0.39 (our models; have; many more parameters than those)
0.14 (those; used; L:in our previous system)

This makes model exploration a very time consuming exercise, so we have built a highly optimized training system that uses 8 or 16 GPUs to train one model.
0.38 (This; makes; model exploration a very time consuming exercise)
0.90 (very time; consuming; exercise)
0.37 (we; have built; a highly optimized training system that uses 8 or 16 GPUs to train one model)
0.93 (a highly optimized training system; uses; 8 or 16; GPUs to train one model)

In contrast to previous large-scale training approaches that use parameter servers and asynchronous updates [18, 10], we use synchronous SGD, which is easier to debug while testing new ideas, and also converges faster for the same degree of data parallelism.
0.92 (previous large-scale training approaches; use; parameter servers and asynchronous updates)
0.50 (we; use; synchronous SGD)
0.93 (synchronous SGD; is; easier to debug while testing new ideas)

To make the entire system efﬁcient, we describe optimizations for a single GPU as well as improvements to scalability for multiple GPUs.
0.50 (we; describe; optimizations for a single GPU as well as improvements)

We employ optimization techniques typically found in High Performance Computing to improve scalability.
0.50 (We; employ; optimization techniques typically found in High Performance Computing)
0.94 (optimization techniques; found; L:in High Performance Computing; T:typically)

We also use carefully integrated compute nodes and a custom implementation of all-reduce to accelerate inter-GPU communication.
0.57 (We; also use; carefully integrated compute nodes and a custom implementation of all-reduce; to accelerate inter-GPU communication)

This scalability and efﬁciency cuts training times down to 3 to 5 days, allowing us to iterate more quickly on our models and datasets.
0.87 (This scalability and efﬁciency; allowing; us to iterate more quickly on our models and datasets)
0.26 Context(This scalability and efﬁciency allowing):(us; to iterate more quickly; on our models and datasets)

We benchmark our system on several publicly available test sets and compare the results to our previous end-to-end system [26].
0.31 (We; benchmark; our system; on several publicly available test sets)
0.27 (We; compare; the results; to our previous end-to-end system)

Our goal is to eventually reach human-level performance not only on speciﬁc benchmarks, where it is possible to improve through dataset-speciﬁc tuning, but on a range of benchmarks that reﬂects a diverse set of scenarios.
0.67 (Our goal; is; to eventually reach human-level performance not only on speciﬁc benchmarks, where it is possible to improve through dataset-speciﬁc tuning, but on a range of benchmarks)
0.91 (a range of benchmarks; reﬂects; a diverse set of scenarios)

To that end, we have also measured the performance of human workers on each benchmark for comparison.
0.46 (we; have also measured; the performance of human workers on each benchmark for comparison; T:To that end)

We ﬁnd that our system outperforms humans in some commonly-studied benchmarks and has signiﬁcantly closed the gap in much harder cases.
0.22 (We; ﬁnd; that our system outperforms humans in some commonly-studied benchmarks and has signiﬁcantly closed the gap in much harder cases)
0.59 Context(We ﬁnd):(our system; outperforms; humans)

In addition to public benchmarks, we show the performance of our Mandarin system on internal datasets that reﬂect real-world product scenarios.
0.42 (we; show; the performance of our Mandarin system on internal datasets)

Through model exploration, we ﬁnd high-accuracy, deployable network architectures, which we detail here.
0.49 (we; ﬁnd; high-accuracy, deployable network architectures, which we detail here)
0.82 (high-accuracy, deployable network architectures; detail; L:here)

We also employ a batching scheme suitable for GPU 11 exaFLOP = 1018 FLoating-point OPerations.
0.61 (We; also employ; a batching scheme suitable for GPU 11 exaFLOP = 1018 FLoating-point OPerations)

hardware called Batch Dispatch that leads to an efﬁcient, real-time implementation of our Mandarin engine on production servers.

Our implementation achieves a 98th percentile compute latency of 67 milliseconds, while the server is loaded with 10 simultaneous audio streams.
0.64 (Our implementation; achieves; a 98th percentile compute latency of 67 milliseconds)
0.90 (the server; is loaded; with 10 simultaneous audio streams)

We begin with a review of related work in deep learning, end-to-end speech recognition, and scalability in Section 2.
0.57 (We; begin; with a review of related work in deep learning, end-to-end speech recognition, and scalability in Section 2)

We discuss the training data and steps taken to further augment the training set in Section 5.
0.45 (We; discuss; the training data and steps taken)
0.93 (the training data and steps; taken; to further augment the training)
0.93 (the training; set; in Section 5)

We end with a description of the steps needed to deploy DS2 to real users in Section 7.
0.52 (We; end; with a description of the steps)

In contrast, we train the CTC-RNN networks from scratch without the need of framewise alignments for pre-training.
0.50 (we; train; the CTC-RNN networks; from scratch)

We take advantage of work in increasing individual GPU efﬁciency for low-level deep learning primitives [9].
0.61 (We; take; advantage; of work in increasing individual GPU efﬁciency for low-level deep learning primitives)

We build on the past work in using modelparallelism [15], data-parallelism [18] or a combination of the two [64, 26] to create a fast and highly scalable system for training deep RNNs in speech recognition.
0.45 (We; build; L:on the past work in using modelparallelism)

We draw inspiration from these past approaches in bootstrapping larger datasets and data augmentation to increase the effective amount of labeled data for our system.
0.49 (We; draw; inspiration; from these past approaches in  bootstrapping larger datasets and data augmentation to increase the effective amount of labeled data for our system)

In order to learn from datasets this large, we increase the model capacity via depth.
0.45 (we; increase; the model capacity via depth)

We explore architectures with up to 11 layers including many bidirectional recurrent layers and convolutional layers.
0.45 (We; explore; architectures)

In order to optimize these models successfully, we use Batch Normalization for RNNs and a novel optimization curriculum we call SortaGrad.
0.50 (we; use; Batch Normalization for RNNs and a novel optimization curriculum)
0.94 (a novel optimization curriculum; call; SortaGrad)

We also exploit long strides between RNN inputs to reduce computation per example by a factor of 3.
0.55 (We; also exploit; long strides between RNN inputs; to reduce computation per example by a factor of 3)
0.39 Context(We also exploit):(We; also exploit long strides between RNN inputs to reduce; computation per example; by a factor of 3)

Finally, though many of our research results make use of bidirectional recurrent layers, we ﬁnd that excellent models exist using only unidirectional recurrent layers—a feature that makes such models much easier to deploy.
0.34 (we; ﬁnd; that excellent models exist using only unidirectional recurrent layers-a feature)
0.69 Context(we ﬁnd):(excellent models; exist; )
0.89 (a feature; makes; such models much easier to deploy)

We use a spectrogram of power t normalized audio clips as the features to the system, so x(i) t,p denotes the power of the p’th frequency bin in the audio frame at time t.
0.45 (We; use; a spectrogram of power t normalized audio clips; as the features to the system)
0.63 (i) t; p; )
0.91 (i) t,p; denotes; the power of the p'th frequency bin in the audio frame at time)

For notational convenience, we drop the superscripts and use x to denote a chosen utterance and y the corresponding label.
0.52 (we; drop; the superscripts)
0.53 (we; use; x; to denote a chosen utterance and y the corresponding label)

In English we have (cid:96)t ∈ {a, b, c, .
0.60 (we; have; t; L:In English)

, z, space, apostrophe, blank}, where we have added the apostrophe as well as a space symbol to denote word boundaries.
0.45 (we; have added; the apostrophe as well as a space symbol)

We describe this in more detail in Section 3.9.
0.23 (We; describe; this)

The architectures we experiment with consist of one or more convolutional layers, followed by one or more recurrent layers, followed by one or more fully connected layers.
0.45 (we; experiment; with consist of one or more convolutional layers)
0.76 (one or more convolutional layers; followed; )
0.76 (one or more recurrent layers; followed; )

We use the clipped rectiﬁedlinear (ReLU) function σ(x) = min{max{x, 0}, 20} as our nonlinearity.
0.45 (We; use; the clipped rectiﬁedlinear)

In some layers, usually the ﬁrst, we sub-sample by striding the convolution by s frames.

We explore variants of this architecture by varying the number of convolutional layers from 1 to 3 and the number of recurrent or GRU layers from 1 to 7.
0.39 (We; explore; variants of this architecture)
0.55 Context(We explore):(We; explore variants of this architecture by varying; the number of convolutional layers; from 1 to 3 and the number of recurrent or GRU layers from 1 to 7)

After the bidirectional recurrent layers we apply one or more fully connected layers with t = f (W lhl−1 t = f (W lhl−1 hl k · hL−1 j · hL−1 The output layer L is a softmax computing a probability distribution over characters given by The model is trained using the CTC loss function [22].
0.70 (we; apply; one or more fully connected layers with t = f; T:After the bidirectional recurrent layers)
0.99 (W lhl−1 t = f (W lhl−1 hl k · hL−1 j · hL−1 The output layer; is; a softmax computing a probability distribution over characters given by The model is trained using the CTC loss function [22])
0.77 Context(W lhl−1 t = f ( W lhl−1 hl k · hL−1 j · hL−1 The output layer is):(a softmax computing a probability distribution over characters; is trained; )
0.90 (a softmax; computing; a probability distribution over characters)
0.89 (characters; given; by The model)

Given an input-output pair (x, y) and the current parameters of the network θ, we compute the loss function L(x, y; θ) and its derivative with respect to the parameters of the network ∇θL(x, y; θ).
0.45 (we; compute; the loss function)

In the following subsections we describe the architectural and algorithmic improvements made relative to DS1 [26].
0.74 (we; describe; the architectural and algorithmic improvements made relative to DS1 [26]; L:In the following subsections)

We report results on an English speaker held out development set which is an internal dataset containing 2048 utterances of primarily read speech.
0.94 (development set; is; an internal dataset containing 2048 utterances of primarily read speech)
0.91 (an internal dataset; containing; 2048 utterances of primarily read speech)
0.44 (We; report; results on an English speaker held out development set)
0.93 Context(We report):(results on an English speaker; held out; development set which is an internal dataset)

We report Word Error Rate (WER) for the English system and Character Error Rate (CER) for the Mandarin system.
0.50 (We; report; Word Error Rate)

In both cases we integrate a language model in a beam search decoding step as described in Section 3.8.
0.60 (we; integrate; a language model; in a beam search decoding step; L:In both cases)

3.2 Batch Normalization for Deep RNNs To efﬁciently scale our model as we scale the training set, we increase the depth of the networks by adding more hidden layers, rather than making each layer larger.
0.45 (we; scale; the training set)
0.39 (we; increase; the depth of the networks)
0.80 Context(we increase):(3.2 Batch Normalization for Deep RNNs; To efﬁciently scale; our model; T:as we scale the training set)
0.39 Context(we increase):(we; increase the depth of the networks by adding; more hidden layers)

We explore Batch Normalization (BatchNorm) as a technique to accelerate training for such networks [63] since they often suffer from optimization issues.
0.50 (We; explore; Batch Normalization)
0.71 (they; suffer; from optimization issues; T:often)

In contrast, we demonstrate that when applied to very deep networks of simple RNNs on large data sets, batch normalization substantially improves ﬁnal generalization error while greatly accelerating training.
0.27 (we; demonstrate; that when applied to very deep networks of simple RNNs on large data sets, batch normalization substantially improves ﬁnal generalization error while greatly accelerating training)
0.95 Context(we demonstrate):(batch normalization; improves; ﬁnal generalization error; T:while greatly accelerating training; T:when applied to very deep networks of simple RNNs on large data sets)

In a typical feed-forward layer containing an afﬁne transformation followed by a non-linearity f (·), we insert a BatchNorm transformation by applying f (B(W h)) instead of f (W h + b), where The terms E and Var are the empirical mean and variance over a minibatch.
0.96 (a typical feed-forward layer; containing; an afﬁne transformation followed by a non-linearity f (·))
0.89 (The terms; are; the empirical mean and variance over a minibatch)
0.75 (an afﬁne transformation; followed; )
0.58 (we; insert; a BatchNorm transformation; L:In a typical feed-forward layer)
0.39 Context(we insert):(we; insert a BatchNorm transformation by applying; f)
0.78 (The terms; E; )

In our convolutional layers the mean and variance are estimated over all the temporal output units for a given convolutional ﬁlter on a minibatch.
0.95 (the mean and variance; are estimated; over all the temporal output units for a given convolutional ﬁlter on a minibatch; L:In our convolutional layers)

We consider two methods of extending BatchNorm to bidirectional RNNs [37].
0.50 (We; consider; two methods of extending BatchNorm to bidirectional RNNs)

We ﬁnd that this technique does not lead to improvements in optimization.
0.34 (We; ﬁnd; that this technique does not lead to improvements in optimization)
0.88 Context(We ﬁnd):(this technique; does not lead; to improvements in optimization)

We also tried accumulating an average over successive time-steps, so later time-steps are normalized over all present and previous timesteps.
0.93 (time-steps; are normalized; L:over all present and previous timesteps; T:so later)
0.35 Context(time - steps are normalized):(We; tried; accumulating an average over successive time-steps)
0.35 Context(We tried time - steps are normalized):(We; tried accumulating; an average over successive time-steps)

We ﬁnd that sequence-wise normalization [37] overcomes these issues.
0.24 (We; ﬁnd; that sequence-wise normalization [37] overcomes these issues)
0.86 Context(We ﬁnd):(sequence-wise normalization; overcomes; these issues)

The recurrent computation is given by t = f (B(W lhl−1 −→h l t + −→U l−→h l t = f (B(W lhl−1 −→h l ) + −→U l−→h l For each hidden unit, we compute the mean and variance statistics over all items in the minibatch over the length of the sequence.
0.95 (The recurrent computation; is given; by t = f (B(W lhl−1 −→h l t + −→U l−→h l t = f (B(W lhl−1 −→h l ) + −→U l−→h l For each hidden unit)
0.92 (B(W lhl−1 −→h; l; t)
0.57 (we; compute; the mean and variance statistics over all items in the minibatch over the length of the sequence)

When comparing depth, in order to control for model size we hold constant the total Figure 2: Training curves of two models trained with and without BatchNorm.
0.66 (we; hold; constant; T:When comparing depth)
0.92 (two models; trained; with and without BatchNorm)

We start the plot after the ﬁrst epoch of training as the curve is more difﬁcult to interpret due to the SortaGrad curriculum method mentioned in Section 3.3 Table 2: Comparison of WER on a training and development set with and without SortaGrad, and with and without batch normalization.
0.52 (We; start; the plot after the ﬁrst epoch of training)
0.96 (the curve; is; more difﬁcult to interpret due to the SortaGrad curriculum method)
0.90 (the curve; to interpret; due to the SortaGrad curriculum method)
0.94 (the SortaGrad curriculum method; mentioned; L:in Section 3.3)
0.97 (a training and development; set; with and without SortaGrad, and with and without batch normalization)

We would expect to see even larger improvements from depth if we held constant the number of activations per layer and added layers.
0.48 (we; held; the number of activations per layer and added layers)
0.39 (We; would expect; to see even larger improvements from depth)
0.36 Context(We would expect):(We; would expect to see; even larger improvements from depth if we held constant the number of activations per layer and added layers)

We also ﬁnd that BatchNorm harms generalization error for the shallowest network just as it converges slower for shallower networks.
0.24 (We; ﬁnd; that BatchNorm harms generalization error for the shallowest network)
0.92 Context(We ﬁnd):(BatchNorm; harms; generalization error)

We ﬁnd that normalizing each neuron to its mean and variance over just the sequence degrades performance.
0.22 (We; ﬁnd; that normalizing each neuron to its mean and variance over just the sequence degrades performance)
0.79 Context(We ﬁnd):(normalizing each neuron to its mean and variance over just the sequence; degrades; performance)

Instead, we store a running average of the mean and variance for the neuron collected during training, and use these for evaluation in deployment [63].
0.57 (we; store; a running average of the mean and variance for the neuron)
0.90 (the neuron; collected; T:during training)
0.41 (we; use; these; for evaluation in deployment)

Using this technique, we can evaluate a single utterance at a time with better results than evaluating with a large batch.
0.50 (we; can evaluate; a single utterance; T:at a time with better results than evaluating with a large batch)

The CTC cost function that we use implicitly depends on the length of the utterance, L(x, y; θ) = − log where Align(x, y) is the set of all possible alignments of the characters of the transcription y to frames of input x under the CTC operator.
0.93 (The CTC cost function; use implicitly; we)
0.93 (The CTC cost function; depends; on the length of the utterance)
0.97 (Align(x, y; is; the set of all possible alignments of the characters of the transcription y to frames of input)
0.94 (frames of input; x; L:under the CTC operator)

This motivates a curriculum learning strategy we title SortaGrad.
0.29 (This; motivates; a curriculum learning strategy we title SortaGrad)
0.92 (a curriculum learning strategy; title; SortaGrad)

In the ﬁrst training epoch, we iterate through the training set in increasing order of the length of the longest utterance in the minibatch.
0.60 (we; iterate; through the training; L:In the ﬁrst training epoch)
0.95 (the training; set; in increasing order of the length of the longest utterance in the minibatch)

In some sense the two techniques substitute for one another, though we still ﬁnd gains when applying SortaGrad and BatchNorm together.
0.60 (we; ﬁnd; gains; T:when applying SortaGrad and BatchNorm together; T:still)
0.39 Context(we ﬁnd):(we; ﬁnd gains applying together; SortaGrad and BatchNorm)

Even with BatchNorm we ﬁnd that this curriculum improves numerical stability and sensitivity to small changes in training.
0.33 (we; ﬁnd; that this curriculum improves numerical stability and sensitivity to small changes in training)
0.88 Context(we ﬁnd):(this curriculum; improves; numerical stability and sensitivity to small changes in training)

We suspect that these beneﬁts occur primarily because long utterances tend to have larger gradients, yet we use a ﬁxed learning rate independent of utterance length.
0.17 (We; suspect; that these beneﬁts occur primarily because long utterances tend to have larger gradients, yet we use a ﬁxed learning rate independent of utterance length)
0.88 Context(We suspect):(these beneﬁts; occur primarily; because long utterances tend to have larger gradients)
0.93 (long utterances; tend; to have larger gradients)
0.89 (long utterances; to have; larger gradients)
0.45 (we; use; a ﬁxed learning rate independent of utterance length)

3.4 Comparison of simple RNNs and GRUs The models we have shown so far are simple RNNs that have bidirectional recurrent layers with the recurrence for both the forward in time and backward in time directions modeled by Equation 3.
0.74 (The models; have shown; T:so far)
0.84 (3.4 Comparison of simple RNNs and GRUs; are; simple RNNs that have bidirectional recurrent layers with the recurrence for both the forward in time and backward in time directions)
0.93 (simple RNNs; have; bidirectional recurrent layers with the recurrence for both the forward in time and backward in time directions)
0.92 (time directions; modeled; by Equation 3)

We decided to examine GRUs because experiments on smaller data sets showed the GRU and LSTM reach similar accuracy for the same number of parameters, but the GRUs were faster to train and less likely to diverge.
0.76 (the GRUs; to diverge; )
0.92 (the GRUs; were less likely; faster to train)
0.55 (We; decided; to examine GRUs; because experiments on smaller data sets showed the GRU and LSTM reach similar accuracy for the same number of parameters)
0.43 Context(We decided):(We; decided to examine; GRUs)
0.95 (experiments on smaller data sets; showed; the GRU and LSTM reach similar accuracy for the same number of parameters)
0.86 Context(experiments on smaller data sets showed):(the GRU and LSTM; reach; similar accuracy)

The GRUs we use are computed by zt = σ(Wzxt + Uzht−1 + bz) rt = σ(Wrxt + Urht−1 + br) ˜ht = f (Whxt + rt ◦ Uhht−1 + bh) ht = (1 − zt)ht−1 + zt where σ(·) is the sigmoid function, z and r represent the update and reset gates respectively, and we drop the layer superscripts for simplicity.
0.91 (The GRUs; use; we)
0.91 (The GRUs; are computed; by zt = σ)
0.52 (we; drop; the layer superscripts for simplicity)
0.98 (~ht = f (Whxt + rt ◦ Uhht−1 + bh) ht = (1 − zt)ht−1; represent; the update)
0.99 Context(~ht = f ( Whxt + rt ◦ Uhht−1 + bh ) ht = ( 1 − zt ) ht−1 represent):(~ht = f (Whxt + rt ◦ Uhht−1 + bh) ht = (1 − zt)ht−1; represent the update zt; where σ(·) is the sigmoid function, z and r)
0.52 Context(~ht = f ( Whxt + rt ◦ Uhht−1 + bh ) ht = ( 1 − zt ) ht−1 zt represent):(σ; is; )

We differ slightly from the standard GRU in that we multiply the hidden state ht−1 by Uh prior to scaling by the reset gate.
0.19 (We; differ slightly; )
0.45 (we; multiply; the hidden state ht−1)

However, we ﬁnd similar performance for tanh and clippedReLU nonlinearities and choose to use the clipped-ReLU for simplicity and uniformity with the rest of the network.
0.57 (we; ﬁnd; similar performance for tanh and clippedReLU nonlinearities)
0.57 (we; choose; to use the clipped-ReLU for simplicity and uniformity with the rest of the network)

As we discuss in Section 3.8, even simple RNNs are able to implicitly learn a language model due to the large amount of training data.
0.50 (we; discuss; L:in Section 3.8)
0.94 (even simple RNNs; are; able to implicitly learn a language model due to the large amount of training data)
0.90 (even simple RNNs; to implicitly learn; a language model; due to the large amount of training data)

We attribute this to the thinning from 1728 hidden units per layer for 1 recurrent layer to 768 hidden units per layer for 7 recurrent layers, to keep the total number of parameters constant.
0.45 (We; attribute; this; to the thinning)

However, in later results (Section 6) we ﬁnd that as we scale up the model size, for a ﬁxed computational budget the simple RNN networks perform slightly better.
0.45 (we; scale; up; the model size)
0.37 (we; ﬁnd; that as we scale up the model size, for a ﬁxed computational budget the simple RNN networks perform slightly better; L:in later results (Section 6)
0.78 Context(we ﬁnd):(the simple RNN networks; perform slightly better; )

We experiment with adding between one and three layers of convolution.
0.45 (We; experiment; with adding between one and three layers of convolution)

In all cases we use a same convolution, preserving the number of input features in both frequency and time.
0.53 (we; use; a same convolution; L:In all cases)
0.29 Context(we use):(we; use a same convolution preserving; the number of input features)

In some cases, we specify a stride across either dimension which reduces the size of the output.
0.70 (we; specify; a stride across either dimension which reduces the size of the output; L:In some cases)
0.91 (a stride across either dimension; reduces; the size of the output)

We do not explicitly control for the number of parameters, since convolutional layers add a small fraction of parameters to our networks.
0.42 (We; do not explicitly control; T:for the number of parameters; since convolutional layers add a small fraction of parameters to our networks)
0.87 (convolutional layers; add; a small fraction of parameters to our networks)

We report results on two datasets—a development set of 2048 utterances (“Regular Dev”) and a much noisier dataset of 2048 utterances (“Noisy Dev”) randomly sampled from the CHiME 2015 development datasets [4].
0.45 (We; report; results on two datasets)
0.93 (Noisy Dev; randomly sampled; from the CHiME 2015 development datasets)

We ﬁnd that multiple layers of 1D-invariant convolutions provides a very small beneﬁt.
0.34 (We; ﬁnd; that multiple layers of 1D-invariant convolutions provides a very small beneﬁt)
0.92 Context(We ﬁnd):(multiple layers of 1D-invariant convolutions; provides; a very small beneﬁt)

In the convolutional layers, we apply a longer stride and wider context to speed up training as fewer time-steps are required to model a given utterance.
0.60 (we; apply; a longer stride and wider context; L:In the convolutional layers)
0.92 (fewer time-steps; are required; to model a given utterance)
0.91 (fewer time-steps; to model; a given utterance)

In our Mandarin models, we employ striding in the straightforward way.
0.49 (we; employ striding; in the straightforward way; L:In our Mandarin models)

However, in English, striding can reduce accuracy simply because the output of our network requires at least one timestep per output character, and the number of characters in English speech per time-step is high enough to cause problems when striding2.
0.72 (striding; can reduce; accuracy; L:in English)
0.98 (the number of characters in English speech per time-step; is; high enough to cause problems)
0.96 (the number of characters in English speech per time-step; to cause; problems; T:when striding2)

To overcome this, we can enrich the English alphabet with symbols representing alternate labellings like whole words, syllables or non-overlapping ngrams.
0.93 (symbols; representing; alternate labellings like whole words, syllables or non-overlapping ngrams)
0.43 (we; can enrich; the English alphabet)

In practice, we use non-overlapping bi-graphemes or bigrams, since these are simple to construct, unlike syllables, and there are few of them compared to alternatives such as whole words.
0.45 (we; use; non-overlapping bi-graphemes or bigrams; since these are simple to construct)
0.30 (these; are; simple to construct)
0.23 (these; to construct; )

We transform unigram labels into bigram labels through a simple isomorphism.
0.45 (We; transform; unigram labels; into bigram labels)

In Table 5 we show results for both the bigram and unigram systems for various levels of striding, with or without a language model.
0.79 (we; show; results for both the bigram and unigram systems for various levels of striding, with or without a language model; L:In Table 5)

We observe that bigrams allow for larger strides without any sacriﬁce in in the word error rate.
0.28 (We; observe; that bigrams allow for larger strides without any sacriﬁce in in the word error rate)
0.87 Context(We observe):(bigrams; allow; for larger strides)

We have found an unidirectional architecture that performs as well as our bidirectional models.
0.21 (We; have found; an unidirectional architecture that performs as well as our bidirectional models)
0.72 (an unidirectional architecture; performs; )

This allows us to use unidirectional, forward-only RNN layers in our deployment system.
0.34 (This; allows; us to use unidirectional, forward-only RNN layers in our deployment system)
0.41 Context(This allows):(us; to use; unidirectional, forward-only RNN layers in our deployment system)

To accomplish this, we employ a special layer that we call row convolution, shown in  Suppose at time-step t, we use a future context of τ steps.
0.52 (we; call; row convolution)
0.78 (a special layer that we call row convolution; shown; L:in  Suppose; L:at time-step t)
0.39 (we; use; a future context of τ steps)
0.17 Context(we use):(we; employ; a special layer that we call row convolution, shown in  Suppose at time-step t)

We now have a feature matrix ht:t+τ = [ht, ht+1, ..., ht+τ ] of size d × (τ + 1).
0.55 (We; have; a feature matrix ht; T:now)

We deﬁne a parameter matrix W of the same size as ht:t+τ .
0.57 (We; deﬁne; a parameter matrix W of the same size as ht)

This is reﬂected in our training data, where there are on average 14.1 characters/s in English, while only 3.3 characters/s in Mandarin.
0.25 (This; is reﬂected; L:in our training data)

11 is row oriented for both W and ht:t+τ , we call this layer row convolution.
0.62 (11; is; row oriented for both W and ht:t+τ , we call this layer row convolution)
0.82 (row; oriented; for both W and ht:t+τ , we call this layer row convolution)
0.66 (we; call; this layer row convolution; T:t+τ)

We place the row convolution layer above all recurrent layers.
0.45 (We; place; the row convolution layer above all recurrent layers)

We conjecture that the recurrent layers have learned good feature representations, so the row convolution layer simply gathers the appropriate information to feed to the classiﬁer.
0.92 (the row convolution layer; simply gathers; the appropriate information to feed to the classiﬁer)
0.91 (the appropriate information; to feed; to the classiﬁer)
0.28 (We; conjecture; that the recurrent layers have learned good feature representations, so the row convolution layer simply gathers the appropriate information)
0.93 Context(We conjecture):(the recurrent layers; have learned; good feature representations; so the row convolution layer simply gathers the appropriate information)

We train our RNN Models over millions of unique utterances, which enables the network to learn a powerful implicit language model.
0.35 (We; train; our RNN Models; over millions of unique utterances)
0.90 (the network; to learn; a powerful implicit language model)

Our best models are quite adept at spelling, without any external language constraints.
0.66 (Our best models; are; quite adept at spelling)

Further, in our development datasets we ﬁnd many cases where our models can implicitly disambiguate homophones—for example, “he expects the Japanese agent to sell it for two hundred seventy ﬁve thousand dollars”.
0.76 (our models; can implicitly disambiguate; homophones; L:many cases)
0.36 (he; expects; the Japanese agent to sell it for two hundred seventy ﬁve thousand dollars)
0.81 Context(he expects):(the Japanese agent; to sell; it; for two hundred seventy ﬁve thousand dollars)

Thus we ﬁnd that WER improves when we supplement our system with a language model trained from external text.
0.31 (we; supplement; our system; with a language model)
0.91 (a language model; trained; from external text)
0.27 (we; ﬁnd; that WER improves)
0.71 Context(we ﬁnd):(WER; improves; T:when we supplement our system with a language model)

We use an n-gram language model since they scale well to large amounts of unlabeled text [26].
0.42 (We; use; an n-gram language model; since they scale well to large amounts of unlabeled text)
0.62 (they; scale well; to large amounts of unlabeled text)

For English, our language model is a Kneser-Ney smoothed 5-gram model with pruning that is trained using the KenLM toolkit [28] on cleaned text from the Common Crawl Repository3.
0.93 (pruning; is trained; using the KenLM toolkit [28] on cleaned text from the Common Crawl Repository3)
0.90 (pruning; using; the KenLM toolkit; L:on cleaned text from the Common Crawl Repository3)
0.79 (our language model; is; a Kneser-Ney smoothed 5-gram model with pruning)
0.92 Context(our language model is):(a Kneser-Ney; smoothed; 5-gram model)

During inference we search for the transcription y that maximizes Q(y) shown in Equation 12.
0.60 (we; search; for the transcription y; T:During inference)
0.91 (the transcription y; maximizes; Q)
0.85 (y; shown; L:in Equation 12)

We use a beam search to ﬁnd the optimal transcription [27].
0.39 (We; use; a beam search; to ﬁnd the optimal transcription)
0.39 Context(We use):(We; use a beam search to ﬁnd; the optimal transcription)

The relative improvement given by the language model drops from 48% to 36% in English and 27% to 23% in Mandarin, as we go from a model with 5 layers and 1 recurrent layer to a model with 9 layers and 7 recurrent layers.
0.91 (The relative improvement; given; by the language model)
0.96 (The relative improvement given by the language model; drops; from 48%)
0.64 (we; go; from a model with 5 layers and 1 recurrent layer to a model with 9 layers and 7 recurrent layers)

We hypothesize that the network builds a stronger implicit language model with more recurrent layers.
0.28 (We; hypothesize; that the network builds a stronger implicit language model with more recurrent layers)
0.88 Context(We hypothesize):(the network; builds; a stronger implicit language model with more recurrent layers)

We attribute this to the fact that a Chinese character represents a larger block of information than an English character.
0.37 (We; attribute; this; to the fact that a Chinese character represents a larger block of information than an English character)
0.93 (a Chinese character; represents; a larger block of information than an English character)

For example, if we output directly to syllables or words in English, the model would make fewer spelling mistakes and the language model would likely help less.
0.50 (we; output directly; to syllables or words in English)
0.92 (the model; would make; fewer spelling mistakes and the language model would likely help less)
0.80 Context(the model would make):(fewer spelling mistakes and the language model; would likely help; less)

The techniques that we have described so far can be used to build an end-to-end Mandarin speech recognition system that outputs Chinese characters directly.
0.74 (The techniques; have described; T:so far)
0.86 (The techniques that we have described so far; can be used; to build an end-to-end Mandarin speech recognition system)
0.84 (The techniques that we have described so far; to build; an end-to-end Mandarin speech recognition system)
0.96 (an end-to-end Mandarin speech recognition system; outputs directly; Chinese characters)

For example we do not need to model Mandarin tones explicitly, as some speech systems must do [59, 45].
0.75 (some speech systems; must do; 59, 45)
0.43 (we; do not need; to model Mandarin tones explicitly)
0.43 Context(we do not need):(we; do not need to model explicitly; Mandarin tones)

The only architectural changes we make to our networks are due to the characteristics of the Chinese character set.
0.31 (we; make; to our networks)
0.78 (The only architectural changes we make to our networks; are; due to the characteristics of the Chinese character set)

We incur an out of vocabulary error at evaluation time if a character is not contained in this set.
0.45 (We; incur; an out of vocabulary error; T:at evaluation time)
0.90 (a character; is not contained; in this set)

This is not a major concern, as our test set has only 0.74% out of vocab characters.
0.38 (This; is not; a major concern)
0.41 (our test; set; )
0.72 (our test set; has; only 0.74%)

We use a character level language model in Mandarin as words are not usually segmented in text.
0.50 (We; use; a character level language model; L:in Mandarin)
0.84 (words; are not segmented; L:in text; T:usually)

In addition, we ﬁnd that the performance of the beam search during decoding levels off at a smaller beam size.
0.39 (we; ﬁnd; that the performance of the beam search during decoding levels off at a smaller beam size)

In Section 6.2, we show that our Mandarin speech models show roughly the same improvements to architectural changes as our English speech models.
0.24 (we; show; that our Mandarin speech models show roughly the same improvements to architectural changes as our English speech models)
0.76 Context(we show):(our Mandarin speech models; show; roughly the same improvements to architectural changes as our English speech models)

Our networks have tens of millions of parameters, and the training algorithm takes tens of singleprecision exaFLOPs to converge.
0.64 (Our networks; have; tens of millions of parameters)
0.91 (the training algorithm; takes; tens of singleprecision exaFLOPs)

Since our ability to evaluate hypotheses about our data and models depends on the ability to train models quickly, we built a highly optimized training system.
0.79 (our ability to evaluate hypotheses about our data and models; depends; on the ability)
0.45 (we; built; a highly optimized training system)

Our optimized software, running on dense compute nodes with 8 Titan X GPUs per node, allows us to sustain 24 single-precision teraFLOP/second when training a single model on one node.
0.77 (Our optimized software, running on dense compute nodes with 8 Titan X GPUs per node; allows; us to sustain 24 single-precision teraFLOP/second)
0.72 Context(Our optimized software , running on dense compute nodes with 8 Titan X GPUs per node allows):(Our optimized software; running; on dense compute nodes with 8 Titan X GPUs per node)
0.35 Context(Our optimized software , running on dense compute nodes with 8 Titan X GPUs per node allows):(us; to sustain; T:when training a single model on one node)

We also can scale to multiple nodes, as outlined in the next subsection.
0.41 (We; can scale; to multiple nodes)

We use the standard technique of data-parallelism to train on multiple GPUs using synchronous SGD.
0.92 (multiple GPUs; using; synchronous SGD)
0.39 (We; use; the standard technique of data-parallelism; to train on multiple GPUs)
0.39 Context(We use):(We; use the standard technique of data-parallelism to train; on multiple GPUs)

Our most common conﬁguration uses a minibatch of 512 on 8 GPUs.
0.72 (Our most common conﬁguration; uses; a minibatch of 512; L:on 8 GPUs)

Our training pipeline binds one process to each GPU.
0.76 (Our training pipeline; binds; one process; to each GPU)

We ﬁnd synchronous SGD useful because it is reproducible and deterministic.
0.42 (We; ﬁnd; synchronous SGD useful; because it is reproducible and deterministic)
0.28 (it; is; reproducible and deterministic)

We have found that the appearance of non-determinism in our system often signals a serious bug, and so having reproducibility as a goal has greatly facilitates debugging.
0.17 (We; have found; that the appearance of non-determinism in our system often signals a serious bug, and so having reproducibility as a goal has greatly facilitates debugging)
0.78 Context(We have found):(the appearance of non-determinism in our system; signals; a serious bug; T:often)

It scales well as we add multiple nodes to the training process.
0.19 (It; scales well; )
0.52 (we; add; multiple nodes; to the training process)

Figure 4 shows that time taken to train one epoch halves as we double the number of GPUs that we train on, thus achieving near-linear weak scaling.
0.78 (Figure 4; shows; that time taken to train one epoch halves)
0.50 (we; double; the number of GPUs)
0.93 (the number of GPUs; train; we)

We keep the minibatch per GPU constant at 64 during this experiment, effectively doubling the minibatch as we double the number of GPUs.
0.50 (we; double; the number of GPUs)
0.43 (We; keep; the minibatch per GPU; T:at 64; T:during this experiment)
0.21 Context(We keep):(We; keep the minibatch per GPU effectively doubling; the minibatch; T:as we double the number of GPUs)

Although we have the ability to scale to large minibatches, we typically use either 8 or 16 GPUs during training with a minibatch of 512 or 1024, in order to converge to the best result.
0.45 (we; have; the ability to scale to large minibatches)
0.52 (we; typically use; GPUs; T:during training with a minibatch of 512 or 1024, in order)

Since all-reduce is critical to the scalability of our training, we wrote our own implementation of the ring algorithm [48, 65] for higher performance and better stability.
0.88 (all-reduce; is; critical to the scalability of our training)
0.31 (we; wrote; our own implementation of the ring algorithm; for higher performance and better stability)

Our implementation avoids extraneous copies between CPU and GPU, and is fundamental to our scalability.
0.68 (Our implementation; avoids; extraneous copies between CPU and GPU)
0.52 (Our implementation; is; fundamental to our scalability)

We conﬁgure OpenMPI with the smcuda transport that can send and receive buffers residing in the memory of two different GPUs by using GPUDirect.
0.50 (We; conﬁgure; OpenMPI; with the smcuda transport)
0.72 (the smcuda transport; can send; )
0.90 (the smcuda transport; receive; buffers residing in the memory of two different GPUs)
0.89 (buffers; residing; in the memory of two different GPUs)

We built our implementation using MPI send and receive, along with CUDA kernels for the elementwise operations.
0.26 (We; built; our implementation)
0.55 Context(We built):(We; built our implementation using; MPI; send and receive, along with CUDA kernels for the elementwise operations)
0.11 Context(We built using):(We; built our implementation using MPI receive; )

Table 7 compares the performance of our all-reduce implementation with that provided by OpenMPI version 1.8.5.
0.30 (Table 7; compares; the performance of our all-reduce implementation with that)
0.16 (that; provided; by OpenMPI version 1.8.5)

We report the time spent in all-reduce for a full training run that ran for one epoch on our English dataset using a 5 layer, 3 recurrent layer architecture with 2560 hidden units for all layers.
0.57 (We; report; the time spent in all-reduce for a full training run)
0.93 (the time; spent; L:in all-reduce)
0.93 (a full training run; ran; for one epoch)

In this table, we use a minibatch of 64 per GPU, expanding the algorithmic minibatch as we scale to more GPUs.
0.50 (we; scale; to more GPUs)
0.58 (we; use; a minibatch of 64 per GPU; L:In this table)
0.29 Context(we use):(we; use a minibatch of 64 per GPU expanding; the algorithmic minibatch)

We see that our implementation is considerably faster than OpenMPI’s when the communication is within a node (8 GPUs or less).
0.93 (the communication; is; within a node)
0.14 (We; see; that our implementation is considerably faster than OpenMPI's)
0.69 Context(We see):(our implementation; is; considerably faster than OpenMPI's)

As we increase the number of GPUs and increase the amount of inter-node communication, the gap shrinks, although our implementation is still 2-4X faster.
0.50 (we; increase; the number of GPUs)
0.41 (we; increase; the amount of inter-node communication)
0.78 (the gap; shrinks; )
0.74 (our implementation; is; T:still; 2-4X faster)

All of our training runs use either 8 or 16 GPUs, and in this regime, our all-reduce implementation results in 2.5× faster training for the full training run, compared to using OpenMPI directly.
0.74 (All of our training runs; use; either 8 or 16 GPUs)
0.86 (our all-reduce implementation results in 2.5× faster training for the full training run; compared; to using OpenMPI directly)

Optimizing all-reduce has thus resulted in important productivity beneﬁts for our experiments, and has made our simple synchronous SGD approach scalable.
0.87 (all-reduce; has resulted; in important productivity beneﬁts)
0.82 (all-reduce; has made; our simple synchronous SGD approach scalable)

Performance gain is the ratio of OpenMPI all-reduce time to our all-reduce time.
0.94 (Performance gain; is; the ratio of OpenMPI)
0.13 (all; reduce; )

4.2 GPU implementation of CTC loss function Calculating the CTC loss function is more complicated than performing forward and back propagation on our RNN architectures.
0.93 (4.2 GPU implementation of CTC loss function; is; more complicated than performing forward and back propagation on our RNN architectures)

Originally, we transferred activations from the GPUs to the CPU, where we calculated the loss function using an OpenMP parallelized implementation of CTC.
0.64 (we; transferred; activations; from the GPUs; to the CPU; T:Originally)
0.93 (OpenMP; parallelized; implementation of CTC)
0.53 (we; calculated; the loss function; L:the CPU)
0.43 Context(we calculated):(we; calculated the loss function using; an OpenMP parallelized implementation of CTC)

However, this implementation limited our scalability rather signiﬁcantly, for two reasons.
0.84 (this implementation; limited rather signiﬁcantly; for two reasons)

Firstly, it became computationally more signiﬁcant as we improved efﬁciency and scalability of the RNN itself.
0.35 (it; became; computationally more signiﬁcant; T:as we improved efﬁciency and scalability of the RNN)
0.50 (we; improved; efﬁciency and scalability of the RNN)

To overcome this, we wrote a GPU implementation of the CTC loss function.
0.43 (we; wrote; a GPU implementation of the CTC loss function)

Our parallel implementation relies on a slight refactoring to simplify the dependences in the CTC calculation, as well as the use of optimized parallel sort implementations from ModernGPU [5].
0.79 (Our parallel implementation; relies; on a slight refactoring; to simplify the dependences in the CTC calculation, as well as the use of optimized parallel sort implementations from ModernGPU)

We give more details of this parallelization in the Appendix.
0.50 (We; give; more details of this parallelization in the Appendix)

This reduces overall training time by 10-20%, which is also an important productivity beneﬁt for our experiments.
0.38 (This; reduces; overall training time; by 10-20%)
0.81 (10-20%; is also; an important productivity beneﬁt for our experiments)

Our system makes frequent use of dynamic memory allocations to GPU and CPU memory, mainly to store activation data for variable length utterances, and for intermediate results.
0.64 (Our system; makes; frequent use of dynamic memory allocations)

For these very large allocations we found that CUDA’s memory allocator and even std::malloc introduced signiﬁcant overhead into our application—over a 2x slowdown from using std::malloc in some cases.
0.30 (we; found; that CUDA's memory allocator and even std::malloc introduced signiﬁcant overhead into our application-over a 2x slowdown from using std; L:For these very large allocations)
0.78 Context(we found):(malloc; introduced; signiﬁcant; overhead into our application)

This is a good optimization for systems running multiple applications, all sharing memory resources, but editing page tables is pure overhead for our system where nodes are dedicated entirely to running a single model.
0.45 (This; is; a good optimization for systems)
0.89 (nodes; are dedicated entirely; to running a single model; L:our system)
0.89 (systems; running; multiple applications)
0.88 (nodes; to running; a single model)
0.88 (editing page tables; is; pure overhead for our system)

To get around this limitation, we wrote our own memory allocator for both CPU and GPU allocations.
0.35 (we; wrote; our own memory allocator; for both CPU and GPU allocations)

Our implementation follows the approach of the last level shared allocator in jemalloc: all allocations are carved out of contiguous memory blocks using the buddy algorithm [34].
0.74 (Our implementation; follows; the approach of the last level shared allocator in jemalloc)
0.73 (all allocations; are carved; )

To avoid fragmentation, we preallocate all of GPU memory at the start of training and subdivide individual allocations from this block.
0.50 (we; preallocate; all of GPU memory; T:at the start of training)
0.41 (we; subdivide; individual allocations from this block)

Similarly, we set the CPU memory block size that we forward to mmap to be substantially larger than std::malloc, at 12GB.
0.46 (we; set; the CPU memory block size)
0.44 Context(we set):(we; forward to be; substantially larger than std::malloc, at 12GB)

When a requested memory allocation exceeds available GPU memory, we allocate page-locked GPU-memory-mapped CPU memory using cudaMallocHost instead.
0.93 (a requested memory allocation; exceeds; available GPU memory)
0.49 (we; allocate instead; T:When a requested memory allocation exceeds available GPU memory)
0.70 (page-locked GPU-memory-mapped CPU memory using; cudaMallocHost; )

We have collected an extensive training dataset for both English and Mandarin speech models, in addition to augmenting our training with publicly available datasets.
0.61 (We; have collected; an extensive training dataset for both English and Mandarin speech models)

In English we use 11,940 hours of labeled speech data containing 8 million utterances summarized in Table 9.
0.64 (we; use; 11,940 hours of labeled speech data; L:In English)
0.93 (labeled speech data; containing; 8 million utterances summarized in Table 9)
0.93 (8 million utterances; summarized; L:in Table 9)

For the Mandarin system we use 9,400 hours of labeled audio containing 11 million utterances.
0.93 (For the Mandarin system; use; 9,400 hours of labeled audio)
0.90 (labeled audio; containing; 11 million utterances)

To solve this problem, we developed an alignment, segmentation and ﬁltering pipeline that can generate a training set with shorter utterances and few erroneous transcriptions.
0.45 (we; developed; an alignment, segmentation and ﬁltering pipeline)
0.93 (ﬁltering pipeline; can generate; a training set with shorter utterances and few erroneous transcriptions)
0.93 (a training; set; with shorter utterances and few erroneous transcriptions)

For a given audio-transcript pair, (x, y), we ﬁnd the alignment that maximizes This is essentially a Viterbi alignment found using a RNN model trained with CTC.
0.24 (we; ﬁnd; the alignment that maximizes)
0.36 Context(we ﬁnd):(This; is essentially; a Viterbi alignment)
0.94 (a RNN model; trained; with CTC)
0.70 (the alignment; maximizes; )

However, we found that CTC produces an accurate alignment when trained with a bidirectional RNN.
0.22 (we; found; that CTC produces an accurate alignment)
0.85 Context(we found):(CTC; produces; an accurate alignment; T:when trained with a bidirectional RNN)

By tuning the number of consecutive blanks, we can tune the length of the utterances generated.
0.45 (we; can tune; the length of the utterances)
0.73 (the utterances; generated; )

For the English speech data, we also require a space token to be within the stretch of blanks in order to segment only on word boundaries.
0.57 (we; also require; a space token to be within the stretch of blanks in order)
0.91 (a space token; to be; within the stretch of blanks)

We tune the segmentation to generate utterances that are on average 7 seconds long.
0.88 (utterances; are; on average 7 seconds long)
0.39 (We; tune; the segmentation; to generate utterances)
0.19 Context(We tune):(We; tune the segmentation to generate; utterances that are on average 7 seconds long)

We crowd source the ground truth transcriptions for several thousand examples.
0.47 (We; crowd; source the ground truth transcriptions for several thousand examples)
0.40 Context(We crowd):(We; crowd source; the ground truth transcriptions; for several thousand examples)

We then train a linear classiﬁer to accurately predict bad examples given the input features generated from the speech recognizer.
0.67 (We; train; a linear classiﬁer to accurately predict bad examples given the input features; T:then)
0.91 (the input features; generated; from the speech recognizer)

We ﬁnd the following features useful: the raw CTC cost, the CTC cost normalized by the sequence length, the CTC cost normalized by the transcript length, the ratio of the sequence length to the transcript length, the number of words in the transcription and the number of characters in the transcription.
0.52 (We; ﬁnd; the following features useful)
0.93 (the CTC cost; normalized; by the sequence length)
0.93 (the CTC cost; normalized; by the transcript length)

For the English dataset, we ﬁnd that the ﬁltering pipeline reduces the WER from 17% to 5% while retaining more than 50% of the examples.
0.51 (we; ﬁnd; that the ﬁltering pipeline reduces the WER from 17% to 5% while retaining more than 50% of the examples; T:For the English dataset)
0.89 Context(we ﬁnd):(the ﬁltering pipeline; reduces; the WER; from 17% to 5%; T:while retaining more than 50% of the examples)

We augment our training data by adding noise to increase the effective size of our training data and to improve our robustness to noisy speech [26].
0.26 (We; augment; our training data)
0.39 Context(We augment):(We; augment our training data by adding; noise)

Although the training data contains some intrinsic noise, we can increase the quantity and variety of noise through augmentation.
0.91 (the training data; contains; some intrinsic noise)
0.45 (we; can increase; the quantity and variety of noise)

We ﬁnd that a good balance is to add noise to 40% of the utterances that are chosen at random.
0.74 (the utterances; are chosen; at random)
0.34 (We; ﬁnd; that a good balance is to add noise to 40% of the utterances)
0.92 Context(We ﬁnd):(a good balance; is; to add noise to 40% of the utterances)

Our English and Mandarin corpora are substantially larger than those commonly reported in speech recognition literature.
0.29 (Our English and Mandarin corpora; are; substantially larger than those)
0.32 (those; reported; L:in speech recognition literature; T:commonly)

In Table 10, we show the effect of increasing the amount of labeled training data on WER.
0.79 (we; show; the effect of increasing the amount of labeled training data on WER; L:In Table 10)

We note that the WER decreases with a power law for both the regular and noisy development sets.
0.32 (We; note; that the WER decreases with a power law for both the regular and noisy development sets)
0.94 Context(We note):(the WER; decreases; with a power law for both the regular and noisy development sets)

We also observe a consistent gap in WER (∼60% relative) between the regular and noisy datasets, implying that more data beneﬁts both cases equally.
0.51 (We; observe; a consistent gap in WER (~60% relative) between the regular and noisy datasets)
0.11 Context(We observe):(We; observe a consistent gap in WER (~60% relative) between the regular and noisy datasets implying; that more data beneﬁts both cases equally)
0.87 Context(We observe implying):(more data; beneﬁts equally; both cases)

We hypothesize that equally as important as increasing raw number of hours is increasing the number of speech contexts that are captured in the dataset.
0.10 (We; hypothesize; that)
0.89 (speech contexts; are captured; L:in the dataset)

While we do not have the labels needed to validate this claim, we suspect that measuring WER as a function of speakers in the dataset would lead to much larger relative gains than simple random sampling.
0.89 (the labels; to validate; this claim)
0.39 (we; do not have; the labels needed to validate this claim)
0.88 Context(we do not have):(the labels; needed; to validate this claim)
0.31 (we; suspect; that measuring WER as a function of speakers in the dataset would lead to much larger relative gains than simple random sampling)
0.95 Context(we suspect):(measuring WER as a function of speakers in the dataset; would lead; to much larger relative gains than simple random sampling)

To better assess the real-world applicability of our speech system, we evaluate on a wide range of test sets.
0.45 (we; evaluate; on a wide range of test sets)

We use several publicly available benchmarks and several test sets collected internally.
0.45 (We; use; several publicly available benchmarks and several test sets)
0.75 (several test sets; collected internally; )

We use stochastic gradient descent with Nesterov momentum [61] along with a minibatch of 512 utterances.
0.50 (We; use; stochastic gradient descent with Nesterov momentum; along with a minibatch of 512 utterances)

We use a momentum of 0.99 for all models.
0.45 (We; use; a momentum of 0.99; for all models)

We use a beam size of 500 for the English decoder and a beam size of 200 for the Mandarin decoder.
0.61 (We; use; a beam size of 500; for the English decoder and a beam size of 200 for the Mandarin decoder)

We report results on several test sets for both the DS2 and DS1 model.
0.61 (We; report; results on several test sets for both the DS2 and DS1 model)

We do not tune or adapt either model to any of the speech conditions in the test sets.
0.19 (We; do not tune; )
0.53 (We; adapt; either model to any of the speech conditions in the test sets)

To put the performance of our system in context, we benchmark most of our results against human workers, since speech recognition is an audio perception and language understanding problem that humans excel at.
0.42 (we; benchmark; most of our results against human workers; since speech recognition is an audio perception and language understanding problem)
0.88 (speech recognition; is; an audio perception and language understanding problem that humans excel at)
0.96 (an audio perception and language understanding problem; excel; humans)

We obtain a measure of human level performance by paying workers from Amazon Mechanical Turk to hand-transcribe all of our test sets.
0.40 (We; obtain; a measure of human level performance)
0.44 Context(We obtain):(We; obtain a measure of human level performance by paying; workers; from Amazon Mechanical Turk)

Two workers transcribe the same audio clip, that is typically about 5 seconds long, and we use the better of the two transcriptions for the ﬁnal WER calculation.
0.90 (Two workers; transcribe; the same audio clip)
0.14 (that; is typically; about 5 seconds long)
0.61 (we; use; the better of the two transcriptions for the ﬁnal WER calculation)

Our English speech training set is substantially larger than the size of commonly used speech datasets.
0.87 (Our English speech training set; is; substantially larger than the size of commonly used speech datasets)

To get the best generalization error, we expect that the model size must increase to fully exploit the patterns in the data.
0.90 (the model size; to fully exploit; the patterns in the data)
0.27 (we; expect; that the model size must increase to fully exploit the patterns in the data)
0.89 Context(we expect):(the model size; must increase; to fully exploit the patterns in the data)

In Section 3.2 we explored the effect of model depth while ﬁxing the number of parameters.
0.53 (we; explored; the effect of model depth; T:while ﬁxing the number of parameters; L:In Section 3.2)
0.29 Context(we explored):(we; explored the effect of model depth ﬁxing; the number of parameters)

In contrast, here we show the effect of varying model size on the performance of the speech system.
0.76 (we; show; the effect of varying model size on the performance of the speech system; L:here)

We only vary the size of each layer, while keeping the depth and other architectural parameters constant.
0.39 (We; only vary; the size of each layer; T:while keeping the depth and other architectural parameters constant)
0.29 Context(We only vary):(We; only vary the size of each layer keeping; the depth and other architectural parameters)

We evaluate the models on the same Regular and Noisy development sets that we use in Section 3.5.
0.57 (We; evaluate; the models on the same Regular and Noisy development sets)
0.93 (the same Regular and Noisy development sets; use; L:in Section 3.5)

The models in Table 11 differ from those in Table 3 in that we increase the the stride to 3 and output to bigrams.
0.83 (The models in Table 11; differ; )
0.45 (we; increase; the the stride; to 3)

Because we increase the model size to as many as 100 million parameters, we ﬁnd that an increase in stride is necessary for fast computation and memory constraints.
0.45 (we; increase; the model size; to as many as 100 million parameters)
0.24 (we; ﬁnd; that an increase in stride is necessary)
0.80 Context(we ﬁnd):(an increase in stride; is; necessary)

However, in this regime we note that the performance advantage of the GRU networks appears to diminish over the Model size Model type Regular Dev Noisy Dev 18 × 106 38 × 106 70 × 106 70 × 106 100 × 106 100 × 106 Table 11: Comparing the effect of model size on the WER of the English speech system on both the regular and noisy development sets.
0.44 (we; note; that the performance advantage of the GRU networks appears to diminish over the  Model size Model type Regular Dev Noisy Dev 18 × 106 38 × 106 70 × 106 70 × 106 100 × 106 100 × 106 Table 11: Comparing the effect of model size on the WER of the English speech system on both the regular and noisy development sets; L:in this regime)
0.82 Context(we note):(the performance advantage of the GRU networks; appears; )

We vary the number of hidden units in all but the convolutional layers.
0.57 (We; vary; the number of hidden units in all but the convolutional layers)

We benchmark our system on two test sets from the Wall Street Journal (WSJ) corpus of read news articles.
0.31 (We; benchmark; our system)

We also take advantage of the recently developed LibriSpeech corpus constructed using audio books from the LibriVox project [46].
0.46 (We; take; advantage; of the recently developed LibriSpeech)
0.82 (the recently developed LibriSpeech; corpus constructed; )

Given this result, we suspect that there is little room for a generic speech system to further improve on clean read speech without further domain adaptation.
0.17 (we; suspect; that there is little room for a generic speech system)

Note that we use only one of the six channels to test each utterance.

Our source for accented speech is the publicly available VoxForge (http://www.voxforge.org) dataset, which has clean speech read from speakers with many different accents.
0.79 (Our source for accented speech; is; the publicly available VoxForge (http://www.voxforge.org) dataset)
0.96 (the publicly available VoxForge (http://www.voxforge.org) dataset; has; clean speech read from speakers with many different accents)
0.90 (clean speech; read; from speakers with many different accents)

We group these accents into four categories.
0.45 (We; group; these accents; into four categories)

We construct a test set from the VoxForge data with 1024 examples from each accent group for a total of 4096 examples.
0.61 (We; construct; a test set from the VoxForge data with 1024 examples from each accent group for a total of 4096 examples)
0.96 (a test; set; from the VoxForge data with 1024 examples from each accent group for a total of 4096 examples)

Performance on these test sets is to some extent a measure of the breadth and quality of our training data.
0.94 (Performance on these test sets; is; to some extent)

Table 14 shows that our performance improved on all the accents when we include more accented training data and use an architecture that can effectively train on that data.
0.56 (our performance; improved; T:when we include more accented training data and use an architecture)
0.45 (we; include; more accented training data)
0.74 (an architecture; can effectively train; on that data)

We test our performance on noisy speech using the publicly available test sets from the recently completed third CHiME challenge [4].
0.37 (We; test; our performance on noisy speech)
0.95 (noisy speech; using; the publicly available test sets from the recently completed third CHiME challenge)

We use a single channel for all our results, since multi-channel audio is not pervasive on most devices.
0.31 (We; use; a single channel for all our results; since multi-channel audio is not pervasive on most devices)
0.77 (multi-channel audio; is not; pervasive)

In Table 16 we compare several architectures trained on the Mandarin Chinese speech, on a development set of 2000 utterances as well as a test set of 1882 examples of noisy speech.
0.74 (we; compare; several architectures trained on the Mandarin Chinese speech, on a development set of 2000 utterances as well as a test set of 1882 examples of noisy speech; T:In Table 16)
0.92 (several architectures; trained; on the Mandarin Chinese speech)

This development set was also used to tune the decoding parameters We see that the deepest model with 2D-invariant convolution and BatchNorm outperforms the shallow RNN by 48% relative, thus continuing the trend that we saw with the English system—multiple layers of bidirectional recurrence improves performance substantially.
0.91 (This development set; was also used; to tune the decoding parameters)
0.77 (This development set; to tune; the decoding parameters We see that the deepest model with 2D-invariant  convolution and BatchNorm outperforms the shallow RNN by 48% relative, thus continuing the trend that we saw with the English system-multiple layers of bidirectional recurrence improves performance substantially)
0.95 (the trend; saw; with the English system-multiple layers of bidirectional recurrence)
0.20 (We; see; that the deepest model with 2D-invariant  convolution and BatchNorm outperforms the shallow RNN by 48% relative, thus continuing the trend that we saw with the English system-multiple layers of bidirectional recurrence improves performance substantially)
0.95 Context(We see):(the deepest model with 2D-invariant  convolution and BatchNorm outperforms the shallow RNN by 48% relative; improves substantially; performance)
0.91 (BatchNorm; outperforms; the shallow RNN)
0.69 Context(BatchNorm outperforms):(BatchNorm; outperforms the shallow RNN thus continuing; the trend that we saw with the English system-multiple layers of bidirectional recurrence)

All the models in the table have about 80 million parameters each We ﬁnd that our best Mandarin Chinese speech system transcribes short voice-query like utterances better than a typical Mandarin Chinese speaker.
0.84 (All the models in the table; have; about 80 million parameters each We ﬁnd that our best Mandarin Chinese speech system transcribes short voice-query like utterances better than a typical Mandarin Chinese speaker)
0.25 (We; ﬁnd; that our best Mandarin Chinese speech system transcribes short voice-query like utterances better than a typical Mandarin Chinese speaker)
0.84 Context(We ﬁnd):(our best Mandarin Chinese speech system; transcribes; short voice-query like utterances better than a typical Mandarin Chinese speaker)

To benchmark against humans we ran a test with 100 randomly selected utterances and had a group of 5 humans label all of them together.
0.52 (we; ran; a test with 100 randomly selected utterances)
0.48 (we; had; a group of 5 humans)

We also compared a single human transcriber to the speech system on 250 randomly selected utterances.
0.45 (We; also compared; a single human transcriber; to the speech system on 250 randomly selected utterances)

Second, since we use a wide beam when decoding with a language model, beam search can be expensive, particularly in Mandarin where the number of possible next characters is very large (around 6000).
0.45 (we; use; a wide beam; T:when decoding with a language model)
0.77 (beam search; can be; expensive)
0.87 (the number of possible next characters; is; very large)

Third, as described in Section 3, we normalize power across an entire utterance, which again requires the entire utterance to be available in advance.
0.45 (we; normalize; power; L:across an entire utterance)
0.94 (an entire utterance; requires; the entire utterance; to be available in advance; T:again)
0.91 (the entire utterance; to be; available; T:in advance)

We solve the power normalization problem by using some statistics from our training set to perform an adaptive normalization of speech inputs during online transcription.
0.79 (our training; set; to perform an adaptive normalization of speech inputs during online transcription)
0.26 (We; solve; the power normalization problem; by using some statistics from our training)
0.26 Context(We solve):(We; solve the power normalization problem by using; some statistics from our training)

We can solve the other problems by modifying our network and decoding procedure to produce a model that performs almost as well while having much lower latency.
0.31 (We; can solve; the other problems; by modifying our network and decoding procedure)
0.86 (a model; performs almost as well; T:while having much lower latency)
0.80 Context(a model performs almost as well):(a model; performs almost as well having; much lower latency)

We focus on our Mandarin system since some aspects of that system are more challenging to deploy (e.g.
0.24 (We; focus; on our Mandarin system; since some aspects of that system are more challenging to deploy (e.g.)
0.81 (some aspects of that system; are; more challenging to deploy (e.g.)
0.67 (some aspects of that system; to deploy e.g.; )

In this section, latency refers to the computational latency of our speech system as measured from the end of an utterance until the transcription is produced.
0.89 (latency; refers; to the computational latency of our speech system; L:In this section)
0.73 (the transcription; is produced; )

We focus on latency from end of utterance to transcription because it is important to applications using speech recognition.
0.31 (We; focus; on latency; because it is important to applications using speech recognition)
0.52 (it; is; important to applications)

In order to deploy our relatively large deep neural networks at low latency, we have paid special attention to efﬁciency during deployment.
0.45 (we; have paid; special attention; to efﬁciency; T:during deployment)

To overcome these issues, we built a batching scheduler called Batch Dispatch that assembles streams of data from user requests into batches before performing forward propagation on these batches.
0.37 (we; built; a batching scheduler called Batch Dispatch that assembles streams of data from user requests into batches before performing forward propagation on these batches)
0.94 (a batching scheduler called Batch Dispatch; assembles; streams of data from user requests into batches)

The more we buffer user requests to assemble a large batch, the longer users must wait for their results.
0.45 (we; buffer; user requests to assemble a large batch)
0.85 (the longer users; must wait; for their results)

This places constraints on the amount of batching we can perform.
0.25 (This; places; constraints on the amount of batching we can perform)
0.19 (we; can perform; )

We use an eager batching scheme that processes each batch as soon as the previous batch is completed, regardless of how much work is ready by that point.
0.32 (We; use regardless; an eager batching scheme that processes each batch as soon)
0.91 (an eager batching scheme; processes; each batch; T:as soon as the previous batch is completed)
0.75 (the previous batch; is completed; )
0.83 (how much work; is; ready by that point)

Figure 5 shows the probability that a request is processed in a batch of given size for our production system running on a single NVIDIA Quadro K1200 GPU, with 10-30 concurrent user requests.
0.80 (Figure 5; shows; the probability that a request is processed in a batch of given size for our production system)
0.90 (a request; is processed; L:in a batch of given size)
0.79 (our production system; running; L:on a single NVIDIA Quadro K1200 GPU, with 10-30 concurrent user requests)

However, even with a light load of only 10 concurrent user requests, our system performs more than half the work in batches with at least 2 samples.
0.74 (our system; performs; more than half the work in batches with at least 2 samples)

Figure 6: Median and 98 percentile latencies as a function of server load We see in Figure 6, that our system achieves a median latency of 44 ms, and a 98 percentile latency of 70 ms when loaded with 10 concurrent streams.
0.45 (We; see; L:in Figure 6)
0.76 (a function of server load We see in Figure 6; achieves; a median latency of 44 ms)

7.2 Deployment Optimized Matrix Multiply Kernels We have found that deploying our models using half-precision (16-bit) ﬂoating-point arithmetic does not measurably change recognition accuracy.
0.30 (We; have found; that deploying our models using half-precision (16-bit) ﬂoating-point arithmetic does not measurably change recognition accuracy; T:7.2 Deployment Optimized Matrix Multiply Kernels)
0.84 Context(We have found):(deploying our models using half-precision (16-bit) ﬂoating-point arithmetic; does not measurably change; recognition accuracy)

We found that standard BLAS libraries are inefﬁcient at this batch size.
0.32 (We; found; that standard BLAS libraries are inefﬁcient at this batch size)
0.92 Context(We found):(standard BLAS libraries; are; inefﬁcient; L:at this batch size)

To overcome this, we wrote our own half-precision matrix-matrix multiply kernel.
0.36 (we; wrote; our own half-precision matrix-matrix multiply kernel)

We store the A matrix transposed to maximize bandwidth by using the widest possible vector loads while avoiding transposition after loading.
0.50 (We; store; the A matrix)
0.96 (the A matrix; transposed; to maximize bandwidth by using the widest possible vector loads)
0.92 (the A matrix; to maximize; bandwidth)

Figure 7 shows that our deployment kernel sustains a higher computational throughput than those from Nervana Systems [44] on the K1200 GPU, across the entire range of batch sizes that we use in deployment.
0.89 (batch sizes; use; L:in deployment)
0.79 (Figure 7; shows; that our deployment kernel sustains a higher computational throughput than those from Nervana Systems [44] on the K1200 GPU, across the entire range of batch sizes)
0.41 Context(Figure 7 shows):(our deployment kernel; sustains; a higher computational throughput than those from Nervana Systems)

Both our kernels and the Nervana kernels are signiﬁcantly faster than NVIDIA CUBLAS version 7.0, more details are found here [20].
0.72 (more details; are found; L:here)
0.79 Context(more details are found):(Both our kernels and the Nervana kernels; are; signiﬁcantly faster than NVIDIA CUBLAS version 7.0)

To deal with this problem, we use a heuristic to further prune the beam search.
0.39 (we; use; a heuristic; to further prune the beam search)
0.29 Context(we use):(we; use a heuristic to prune; the beam search)

Rather than considering all characters as viable additions to the beam, we only consider the fewest number of characters whose cumulative probability is at least p.
0.45 (we; only consider; the fewest number of characters)
0.90 (characters; is; at least p.)

In practice, we have found that p = 0.99 works well.
0.29 (we; have found; that p = 0.99 works well; L:In practice)
0.38 Context(we have found):(we; have found p well; 0.99 works)

Additionally, we limit ourselves to no more than 40 characters.
0.31 (we; limit; ourselves; to no more than 40 characters)

We can deploy our system at low latency and high throughput without sacriﬁcing much accuracy.
0.31 (We; can deploy; our system; L:at low latency and high throughput)

On a held-out set of 2000 utterances, our research system achieves 5.81 character error rate whereas the deployed system achieves 6.10 character error rate.
0.85 (our research system; achieves; 5.81 character error rate whereas the deployed system achieves 6.10 character error rate; L:On a held-out set of 2000 utterances)
0.91 (the deployed system; achieves; 6.10 character error rate)

In order to accomplish this, we employ a neural network architecture with low deployment latency, reduce the precision of our network to 16-bit, built a batching scheduler to more efﬁciently evaluate RNNs, and ﬁnd a simple heuristic to reduce beam search cost.
0.48 (we; ﬁnd; a simple heuristic; to reduce beam search cost)
0.45 (we; employ; a neural network architecture)
0.27 (we; reduce; the precision of our network; to 16-bit)
0.46 (we; built; a batching scheduler; to more efﬁciently evaluate RNNs)

Indeed, our results show that, compared to the previous incarnation, Deep Speech has signiﬁcantly closed the gap in transcription performance with human workers by leveraging more data and larger models.
0.56 (our results; show; that, compared to the previous incarnation, Deep Speech has signiﬁcantly closed the gap in transcription performance with human workers by leveraging more data and larger models)
0.92 Context(our results show):(Deep Speech; has closed; the gap in transcription performance with human workers; T:signiﬁcantly)

Finally, we have also shown that this approach can be efﬁciently deployed by batching user requests together on a GPU server, paving the way to deliver end-to-end Deep Learning technologies to users.
0.34 (we; have shown; that this approach can be efﬁciently deployed by batching user requests together on a GPU server, paving the way; T:Finally)
0.68 Context(we have shown):(this approach; can be efﬁciently deployed; )

To achieve these results, we have explored various network architectures, ﬁnding several effective techniques: enhancements to numerical optimization through SortaGrad and Batch Normalization, evaluation of RNNs with larger strides with bigram outputs for English, searching through both bidirectional and unidirectional models.
0.95 (larger strides with bigram outputs for English; searching; through both bidirectional and unidirectional models)
0.39 (we; have explored; various network architectures)
0.29 Context(we have explored):(we; have explored various network architectures ﬁnding; several effective techniques)

This exploration was powered by a well optimized, High Performance Computing inspired training system that allows us to train new, full-scale models on our large datasets in just a few days.
0.95 (This exploration; was powered; by a well optimized, High Performance Computing inspired training system)
0.82 (High Performance Computing; inspired; training system that allows us to train new, full-scale models on our large datasets in just a few days)
0.94 (a well optimized, High Performance Computing inspired training system; allows; us to train new, full-scale models on our large datasets in just a few days)
0.36 Context(a well optimized , High Performance Computing inspired training system allows):(us; to train; new, full-scale models on our large datasets; T:in just a few days)

Overall, we believe our results conﬁrm and exemplify the value of end-to-end Deep Learning methods for speech recognition in several settings.
0.74 (our results; exemplify; the value of end-to-end Deep Learning methods for speech recognition in several settings)

In those cases where our system is not already comparable to humans, the difference has fallen rapidly, largely because of application-agnostic Deep Learning techniques.
0.53 (our system; is not; already comparable to humans; L:those cases)
0.88 (the difference; has fallen; rapidly; largely because of application-agnostic Deep Learning techniques; L:In those cases)

We believe these techniques will continue to scale, and thus conclude that the vision of a single speech system that outperforms humans in most scenarios is imminently achievable.
0.70 (these techniques; to scale; )
0.91 (a single speech system; outperforms; humans)
0.28 (We; believe; these techniques will continue to scale, and thus conclude that the vision of a single speech system that outperforms humans in most scenarios is imminently achievable)
0.69 Context(We believe):(these techniques; will continue; )
0.78 (these techniques; conclude; that the vision of a single speech system that outperforms humans in most scenarios is imminently achievable)
0.82 Context(these techniques conclude):(the vision of a single speech system; is; imminently achievable)

We are grateful to Baidu’s speech technology group for help with data preparation and useful conversations.
0.61 (We; are; grateful to Baidu's speech technology group for help with data preparation and useful conversations)

We would like to thank Scott Gray, Amir Khosrowshahi and all of Nervana Systems for their excellent matrix multiply routines and useful discussions.
0.40 (We; would like; to thank Scott Gray, Amir Khosrowshahi and all of Nervana Systems for their excellent matrix)
0.40 Context(We would like):(We; would like to thank; Scott Gray, Amir Khosrowshahi and all of Nervana Systems; for their excellent matrix)

We would also like to thank Natalia Gimelshein of NVIDIA for useful discussions and thoughts on implementing our fast deployment matrix multiply.
0.40 (We; would also like; to thank Natalia Gimelshein of NVIDIA for useful discussions and thoughts on implementing our fast deployment matrix multiply)
0.40 Context(We would also like):(We; would also like to thank; Natalia Gimelshein of NVIDIA; for useful discussions and thoughts on implementing our fast deployment matrix multiply)

Weston.

Weinstein, P.

In this section, we discuss some of our scalability improvements in more detail.
0.44 (we; discuss; some of our scalability improvements in more detail; L:In this section)

We use the CPU memory to cache our input data so that we are not directly exposed to the low bandwidth and high latency of spinning disks.
0.57 (we; are not directly exposed; to the low bandwidth and high latency of spinning disks)
0.17 (We; use; the CPU memory; to cache our input data so that we are not directly exposed to the low bandwidth and high latency of spinning disks)
0.17 Context(We use):(We; use the CPU memory to cache; our input data; so that we are not directly exposed to the low bandwidth and high latency of spinning disks)

We replicate our English and Mandarin datasets on each node’s local hard disk.
0.35 (We; replicate; our English and Mandarin datasets)

This allows us to use our network only for weight updates and avoids having to rely on centralized ﬁle servers.
0.30 (This; allows; us to use our network only for weight updates and avoids)
0.26 Context(This allows):(us; to use; our network; only for weight updates and avoids)

Figure 8: Schematic of our training node where PLX indicates a PCI switch and the dotted box includes all devices that are connected by the same PCI root complex.
0.87 (PLX; indicates; a PCI switch and the dotted box; L:our training node)
0.85 (Figure 8; includes; all devices that are connected by the same PCI root complex)
0.90 (all devices; are connected; by the same PCI root complex)

Figure 8 shows a schematic diagram of one our nodes, where all devices connected by the same PCI root complex are encapsulated in a dotted box.
0.87 (Figure 8; shows; a schematic diagram of one our nodes)
0.92 (all devices; connected; by the same PCI root complex)
0.96 (all devices connected by the same PCI root complex; are encapsulated; L:in a dotted box)

We have tried to maximize the number of GPUs within the root complex for faster communication between GPUs using GPUDirect.
0.56 (We; have tried; to maximize the number of GPUs within the root complex for faster communication between GPUs using GPUDirect)
0.56 Context(We have tried):(We; have tried to maximize; the number of GPUs; L:within the root complex for faster communication between GPUs using GPUDirect)

GPUGPUGPUGPUGPUGPUGPUGPUPLXPLXPLXCPUCPUPLXAll the nodes in our cluster are connected through Fourteen Data Rate (FDR) Inﬁniband which is primarily used for gradient transfer during back-propagation.
0.43 (All the nodes in our cluster; are connected; )
0.90 (Fourteen Data Rate (FDR) Inﬁniband; is primarily used; for gradient transfer during back-propagation)

A.2 GPU Implementation of CTC Loss Function The CTC loss function that we use to train our models has two passes: forward and backward, and the gradient computation involves element-wise addition of two matrices, α and β, generated during the forward and backward passes respectively.
0.91 (the gradient computation; involves; element-wise addition of two matrices, α and β)
0.61 (α and β; generated; T:during the forward and backward passes respectively)
0.84 (The CTC loss function; use; to train our models)
0.23 Context(The CTC loss function use):(we; to train; our models)

Finally, we sum the gradients using the character in the utterance label as the key, to generate one gradient per character.
0.60 (we; sum; the gradients; T:Finally)
0.39 Context(we sum):(we; sum the gradients using; the character; L:in the utterance label as the key)
0.29 Context(we sum):(we; sum the gradients to generate; one gradient per character)

The input to the CTC loss function are probabilities calculated by the softmax function which can be very small, so we compute in log probability space for better numerical stability.
0.75 (the softmax function; can be; very small)
0.45 (we; compute; L:in log probability space; for better numerical stability)

Our CPU-based implementation of the CTC algorithm assigns one thread to each utterance label in a minibatch, performing the CTC calculation for the utterances in parallel.
0.67 (Our CPU-based implementation of the CTC algorithm; assigns; one thread; to each utterance label in a minibatch)

Firstly, since the remainder of our network is computed on the GPU, the output of the softmax function has to be copied to the CPU for CTC calculation.
0.74 (the remainder of our network; is computed; L:on the GPU)
0.93 (the output of the softmax function; to be copied; to the CPU for CTC calculation)

Furthermore, we need as much interconnect bandwidth as possible for synchronizing the gradient updates with data parallelism, so this copy incurs a substantial opportunity cost.
0.88 (this copy; incurs; a substantial opportunity cost)
0.51 Context(this copy incurs):(we; need; as much interconnect bandwidth as possible for synchronizing the gradient updates with data parallelism)

We wrote a GPU-based implementation of CTC in order to overcome these two problems.
0.50 (We; wrote; a GPU-based implementation of CTC; in order)

The key insight behind our implementation is that we can compute all elements in each column of the α matrix, rather than just the valid entries.
0.47 (The key insight behind our implementation; is; that we can compute all elements in each column of the α matrix, rather than just the valid entries)
0.51 Context(The key insight behind our implementation is):(we; can compute; all elements in each column of the α matrix, rather than just the valid entries)

If we do so, Figure 9 shows that invalid elements either contain a ﬁnite garbage value (G), or −∞ (I), when we use a special summation function that adds probabilities in log space that discards inputs that are −∞.
0.28 (we; do; so)
0.74 (log space; discards; inputs that are −∞)
0.87 (Figure 9; shows; that invalid elements either contain a ﬁnite garbage value (G), or −∞)
0.92 Context(Figure 9 shows):(invalid elements; contain; a ﬁnite garbage value (G), or −∞)
0.72 (inputs; are; −∞)
0.33 (we; use; a special summation function that adds probabilities in log space)
0.93 (a special summation function; adds; probabilities; L:in log space)

However, when we compute the ﬁnal gradient by element-wise summing α and β, all ﬁnite garbage values will be added with a corresponding −∞ value from the other matrix, which results in −∞, effectively ignoring the garbage value and computing the correct result.
0.45 (we; compute; the ﬁnal gradient)
0.95 (the other matrix, which results in −∞; effectively ignoring; the garbage value)
0.94 (all ﬁnite garbage values; will be added; T:when we compute the ﬁnal gradient by element-wise summing α and β)

One important observation is that this element-wise sum of α and β is a simple sum and does not use our summation function.
0.78 (One important observation; is; that this element-wise sum of α and β is a simple sum and does not use our summation function)
0.94 Context(One important observation is):(this element-wise sum of α and β; is; a simple sum)

To compute the gradient, we take each column of the matrix generated from element-wise addition of α and β matrices, and do a key-value reduction using the character as key, using the ModernGPU library [5].
0.45 (we; take; each column of the matrix)
0.90 (the matrix; generated; from element-wise addition of α and β matrices)
0.60 (we; do; a key-value reduction using the character as key)

Since our summation function in log space effectively ignores the −∞ elements, only the valid elements are combined in the reduction.
0.81 (our summation function in log space; effectively ignores; the −∞ elements, only the valid elements are combined in the reduction)
0.92 (only the valid elements; are combined; L:in the reduction)

In our GPU implementation, we map each utterance in the minibatch to a CUDA thread block.
0.56 (we; map; each utterance; L:In our GPU implementation)

values, with each character as key, we must deal with data dependencies due to repeated characters in an utterance label.
0.45 (we; must deal; with data dependencies due to repeated characters; L:in an utterance label)

We solve this problem by performing a key-value sort, where the keys are the characters in the utterance label, and the values are the indices of each character in the utterance.
0.90 (the keys; are; the characters in the utterance label)
0.90 (the values; are; the indices of each character in the utterance)
0.39 (We; solve; this problem; by performing a key-value sort)
0.50 Context(We solve):(We; solve this problem by performing; a key-value sort, where the keys are the characters in the utterance label)

We only need to do the sort once for each utterance.
0.39 (We; only need; to do the sort once for each utterance)
0.39 Context(We only need):(We; only need to do; the sort; T:once; for each utterance)

Our GPU implementation uses fast shared memory and registers to achieve high performance when performing this task.
0.72 (Our GPU implementation; uses; fast shared memory and registers; to achieve high performance when performing this task)

However, as we go backward in time, we only need to keep one column of the β matrix as we compute the gradient, adding element-wise the column of the β matrix with the corresponding column of the α matrix.
0.52 (we; go; backward; T:in time)
0.45 (we; compute; the gradient)
0.36 (we; only need; to keep one column of the β matrix as we compute the gradient, adding element-wise the column of the β matrix with the corresponding column of the α matrix)
0.26 Context(we only need):(we; only need to keep; one column of the β matrix; T:as we compute the gradient)
0.40 Context(we only need to keep):(we; only need to keep one column of the β matrix adding; L:element-wise the column of the β matrix with the corresponding column of the α matrix)

Due to on-chip memory space constraints, we read the output of the softmax function directly from off-chip global memory.
0.35 (we; read directly; from off-chip global memory)

Due to inaccuracies in ﬂoating-point arithmetic, especially in transcendental functions, our GPU and CPU implementation are not bit-wise identical
0.37 (our GPU and CPU implementation; are not; bit-wise identical)

