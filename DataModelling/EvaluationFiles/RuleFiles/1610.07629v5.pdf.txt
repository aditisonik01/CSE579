The degree to which one may learn and parsimoniously capture this visual vocabulary measures our understanding of the higher level features of paintings, if not images in general.
0.90 (The degree; to may learn; which one)
0.94 (The degree to which one may learn and parsimoniously capture this visual vocabulary; measures; our understanding of the higher level features of paintings)

In this work we investigate the construction of a single, scalable deep network that can parsimoniously capture the artistic style of a diversity of paintings.
0.60 (we; investigate; the construction of a single, scalable deep network; L:In this work)
0.92 (a single, scalable deep network; can parsimoniously capture; the artistic style of a diversity of paintings)

We demonstrate that such a network generalizes across a diversity of artistic styles by reducing a painting to a point in an embedding space.
0.28 (We; demonstrate; that such a network generalizes across a diversity of artistic styles by reducing a painting to a point in an embedding space)
0.89 Context(We demonstrate):(such a network; generalizes; across a diversity of artistic styles)

We hope that this work provides a useful step towards building rich models of paintings and offers a window on to the structure of the learned representation of artistic style.
0.89 (this work; offers; a window)
0.28 (We; hope; that this work provides a useful step towards building rich models of paintings and offers a window on to the structure of the learned representation of artistic style)
0.88 Context(We hope):(this work; provides; a useful step towards building rich models of paintings)

with respect to color preservation (Gatys et al., 2016a) or style transfer quality (Ulyanov et al., 2016b), but to our knowledge the problem of the single-purpose nature of style transfer networks remains untackled.
0.90 (the problem of the single-purpose nature of style transfer networks; remains; untackled)

We think this is an important problem that, if solved, would have both scientiﬁc and practical importance.
0.34 (We; think; this is an important problem that, if solved, would have both scientiﬁc and practical importance)
0.28 Context(We think):(this; is; an important problem that, if solved, would have both scientiﬁc and practical importance)

Furthermore, the degree to which an artistic styling model might generalize across painting styles would directly measure our ability to build systems that parsimoniously capture the higher level features and statistics of photographs and images (Simoncelli & Olshausen, 2001).
0.89 (the degree; might generalize; across painting styles)
0.92 (systems; parsimoniously capture; the higher level features and statistics of photographs and images)

In this work, we show that a simple modiﬁcation of the style transfer network, namely the introduction of conditional instance normalization, allows it to learn multiple styles (Figure 1a).We demonstrate that this approach is ﬂexible yet comparable to single-purpose style transfer networks, both qualitatively and in terms of convergence properties.
0.27 (We; demonstrate; that this approach is ﬂexible yet comparable to single-purpose style transfer networks, both qualitatively and in terms of convergence properties)
0.37 Context(We demonstrate):(we; show; that a simple modiﬁcation of the style transfer network, namely the introduction of conditional instance normalization, allows it to learn multiple styles (Figure 1a; L:In this work)
0.88 Context(we show We demonstrate):(a simple modiﬁcation of the style transfer network; allows; it to learn multiple styles (Figure 1a)
0.43 Context(we show a simple modiﬁcation of the style transfer network allows We demonstrate):(it; to learn; multiple styles (Figure 1a)
0.94 Context(We demonstrate):(this approach; is; ﬂexible yet comparable to single-purpose style transfer networks, both qualitatively and in terms of convergence properties)

Finally, we show that the embeddding space representation permits one to arbitrarily combine artistic styles in novel ways not previously observed (Figure 1b).
0.53 (we; show; that the embeddding space representation permits one to arbitrarily combine artistic styles in novel ways; T:Finally)
0.54 (one; to arbitrarily combine; artistic styles)
0.92 (novel ways; not observed; Figure 1b; T:previously)

In practice, we set λc = 1.0 and and leave λs as a free hyper-parameter.
0.66 (we; set; λc = 1.0; L:In practice)
0.41 (we; leave; λs as a free hyper-parameter)

2.1 N-STYLES FEEDFORWARD STYLE TRANSFER NETWORKS Our work stems from the intuition that many styles probably share some degree of computation, and that this sharing is thrown away by training N networks from scratch when building an Nstyles style transfer system.
0.62 (Our work; stems; from the intuition that many styles probably share some degree of computation, and that this sharing is thrown away by training N networks from scratch)
0.90 (many styles; probably share; some degree of computation)
0.92 (this sharing; is thrown; away; by training N networks from scratch)
0.90 (this sharing; by training; N networks; from scratch; T:when building an Nstyles style transfer system)
0.38 (NETWORKS; [is] TRANSFER [of]; STYLES FEEDFORWARD STYLE)

To take this into account, we propose to train a single conditional style transfer network T (c, s) for N styles.
0.51 (we; propose; to train a single conditional style transfer network T (c, s) for N styles)
0.51 Context(we propose):(we; propose to train; a single conditional style transfer network T (c, s; for N styles)

In exploring this question, we found a very surprising fact about the role of normalization in style transfer networks: to model a style, it is sufﬁcient to specialize scaling and shifting parameters after normalization to each speciﬁc style.

We call this approach conditional instance normalization.
0.52 (We; call; this approach; conditional instance normalization)

(2016b), we augment the γ and β parameters so that they’re N × C matrices, where N is the number of styles being modeled and C is the number of output feature maps.
0.24 (we; augment; the γ and β parameters; so that they're N × C matrices, where N is the number of styles)
0.89 (C; is; the number of output feature maps)
0.71 (styles; being modeled; )
0.77 (they; 're; N × C matrices, where N is the number of styles)
0.85 Context(they 're):(N × C; matrices; where N is the number of styles)
0.86 Context(they 're N × C matrices):(N; is; the number of styles)

We used the same network architecture as in Johnson et al.
0.50 (We; used; the same network architecture; as in Johnson et al)

We ﬁnd that with these two improvements training the network no longer requires a total variation loss that was previously employed to remove high frequency noise as proposed in Johnson et al.
0.34 (We; ﬁnd; that with these two improvements training the network no longer requires a total variation loss)
0.91 (these two improvements; training; the network)
0.96 (a total variation loss; was employed; to remove high frequency noise as proposed in Johnson et al; T:previously)

Our training procedure follows Johnson et al.
0.70 (Our training procedure; follows; Johnson et al)

Brieﬂy, we employ the ImageNet dataset (Deng et al., 2009) as a corpus of training content images.
0.50 (we; employ; the ImageNet dataset; as a corpus of training content images)

We train the N-style network with stochastic gradient descent using the Adam optimizer (Kingma & Ba, 2014).
0.50 (We; train; the N-style network with stochastic gradient descent)

3.2 TRAINING A SINGLE NETWORK ON N STYLES PRODUCES STYLIZATIONS COMPARABLE As a ﬁrst test, we trained a 10-styles model on stylistically similar images, namely 10 impressionist paintings from Claude Monet.
0.62 (3.2; TRAINING; A SINGLE NETWORK ON N STYLES PRODUCES STYLIZATIONS COMPARABLE; T:As a ﬁrst test, we trained a 10-styles model on stylistically similar images, namely 10 impressionist paintings from Claude Monet)
0.61 (we; trained; a 10-styles model on stylistically similar images, namely 10 impressionist paintings from Claude Monet)

We emphasize that 99.8% of the parameters are shared across all styles in contrast to 0.2% of the parameters which are unique to each painting style.
0.89 (the parameters; are; unique to each painting style)
0.28 (We; emphasize; that 99.8% of the parameters are shared across all styles in contrast to 0.2% of the parameters)
0.75 Context(We emphasize):(99.8% of the parameters; are shared; )

To get a sense of what is being traded off by folding 10 styles into a single network, we trained a separate, single-style network on each style and compared them to the 10-styles network in terms of style transfer quality and training speed (Figure 5).
0.57 (we; trained; a separate, single-style network on each style)
0.38 (we; compared; them; to the 10-styles network in terms of style transfer quality and training speed (Figure 5)

By visual inspection, we observe that the 10-styles network converges as quickly as the single-style networks in terms of style loss, but lags slightly behind in terms of content loss.
0.76 (the 10-styles network; lags slightly behind; )
0.27 (we; observe; that the 10-styles network converges as quickly as the single-style networks in terms of style loss, but lags slightly behind in terms of content loss)
0.74 Context(we observe):(the 10-styles network; converges as quickly; )

In order to quantify this observation, we compare the ﬁnal losses for 10-styles and single-style models (center column).
0.57 (we; compare; the ﬁnal losses for 10-styles and single-style models (center column)

We see that both results are qualitatively similar.
0.19 (We; see; that both results are qualitatively similar)
0.72 Context(We see):(both results; are; qualitatively similar)

3.3 THE N-STYLES MODEL IS FLEXIBLE ENOUGH TO CAPTURE VERY DIFFERENT STYLES We evaluated the ﬂexibility of the N-styles model by training a style transfer network on 32 works of art chosen for their diversity.
0.86 (32 works of art; chosen; for their diversity)
0.91 (THE N-STYLES MODEL; IS; FLEXIBLE ENOUGH TO CAPTURE)
0.39 Context(THE N - STYLES MODEL IS):(We; evaluated; the ﬂexibility of the N-styles model)
0.39 Context(THE N - STYLES MODEL IS We evaluated):(We; evaluated the ﬂexibility of the N-styles model by training; a style transfer network; on 32 works of art)

To test the efﬁciency of this approach, we used it to incrementally incorporate Monet’s Plum Trees in Blossom painting to the network trained on 32 varied styles.
0.90 (the network; trained; on 32 varied styles)
0.40 (we; used; it; to incrementally incorporate Monet's Plum Trees in Blossom painting to the network)
0.43 Context(we used):(we; used it to incrementally incorporate; Monet's Plum Trees; to the network)

In learning a different set of γ and β parameters for every style, we are in some sense learning an embedding of styles.
0.35 (we; are learning; an embedding of styles)

To probe the utility of this embedding, we tried convex combinations of the 0500010000150002000025000300003500040000Parameter updates105Total content loss30000350004000045000Final content loss (1 style)30000350004000045000Final content loss (N styles)0500010000150002000025000300003500040000Parameter updates104105Total style loss500010000150002000025000Final style loss (1 style)500010000150002000025000Final style loss (N styles)5,000 steps40,000 stepsﬁne-tunedfrom scratchPublished as a conference paper at ICLR 2017 Figure 7: The N-styles network can arbitrarily combine artistic styles.
0.94 (5,000 steps40,000 stepsﬁne-tunedfrom scratch; Published; as a conference paper at ICLR 2017 Figure 7)
0.93 (The N-styles network; can arbitrarily combine; artistic styles)
0.55 Context(The N - styles network can arbitrarily combine):(we; tried; convex combinations of the 0500010000150002000025000300003500040000Parameter updates105Total content loss30000350004000045000Final content loss)

(Right) As we transition from one style to another (Bicentennial Print and Head of a Clown in this case), the style losses vary monotonically.
0.45 (we; transition; from one style; to another)
0.78 (the style losses; vary; monotonically)

We use γ = α × γ1 + (1 − α) × γ2 and β = α × β1 + (1 − α) × β2 to stylize an image.
0.45 (We; use; γ = α)
0.44 (We; β; α)

Figure 7 (right column) shows the style loss from the transformer network for a given source image, with respect to the Bicentennial Print and Head of a Clown paintings, as we vary α from 0 to 1.
0.95 (Figure 7; shows; the style loss from the transformer network for a given source image, with respect to the Bicentennial Print and Head of a Clown paintings)
0.23 (we; vary; α from 0 to 1)
0.25 (we; α; from 0; to 1)

We see evidence for this behavior in that pruning the architecture leads to qualitatively similar results.
0.33 (We; see; evidence for this behavior in that pruning the architecture leads to qualitatively similar results)

While this work does not attempt to verify this hypothesis, we think that this would constitute a very promising direction of research in understanding the computation behind style transfer networks as well as the representation of images in general.
0.88 (this work; does not attempt; to verify this hypothesis)
0.88 Context(this work does not attempt):(this work; does not attempt to verify; this hypothesis)
0.33 (we; think; that this would constitute a very promising direction of research in understanding the computation behind style transfer networks as well as the representation of images in general)
0.43 Context(we think):(this; would constitute; a very promising direction of research in understanding the computation behind style transfer networks as well as the representation of images in general)

In summary, we demonstrated that conditional instance normalization constitutes a simple, efﬁcient and scalable modiﬁcation of style transfer networks that allows them to model multiple styles at the same time.
0.40 (we; demonstrated; that conditional instance normalization constitutes a simple, efﬁcient and scalable modiﬁcation of style transfer networks; L:In summary)
0.73 Context(we demonstrated):(that conditional instance normalization; constitutes; a simple, efﬁcient and scalable modiﬁcation of style transfer networks that allows them to model multiple styles at the same time)
0.89 (a simple, efﬁcient and scalable modiﬁcation of style transfer networks; allows; them to model multiple styles at the same time)
0.39 Context(a simple , efﬁcient and scalable modiﬁcation of style transfer networks allows):(them; to model; multiple styles; T:at the same time)

We showed that despite its simplicity, the method is ﬂexible enough to capture very different styles while having very little impact on training time and ﬁnal performance of the trained network.
0.93 (the method; to capture; very different styles; T:while having very little impact on training time and ﬁnal performance of the trained network)
0.17 (We; showed; that despite its simplicity, the method is ﬂexible enough to capture very different styles)
0.91 Context(We showed):(the method; is; ﬂexible enough to capture very different styles)

Finally, we showed that the learned representation of style is useful in arbitrarily combining artistic styles.
0.40 (we; showed; that the learned representation of style is useful in arbitrarily combining artistic styles; T:Finally)
0.82 Context(we showed):(the learned representation of style; is; useful)

We would like to thank Fred Bertsch, Douglas Eck, Cinjon Resnick and the rest of the Google Magenta team for their feedback; Peyman Milanfar, Michael Elad, Feng Yang, Jon Barron, Bhavik Singh, Jennifer Daniel as well as the the Google Brain team for their crucial suggestions and advice; an anonymous reviewer for helpful suggestions about applying this model in a mobile domain.
0.43 (We; would like; to thank Fred Bertsch)
0.43 Context(We would like):(We; would like to thank; Fred Bertsch)

Finally, we would like to thank the Google Cultural Institute, whose curated collection of art photographs was very helpful in ﬁnding exciting style images to train on
0.69 (we; would like; to thank the Google Cultural Institute, whose curated collection of art photographs was very helpful in ﬁnding exciting style images; T:Finally)
0.55 Context(we would like):(we; would like to thank; the Google Cultural Institute; whose curated collection of art photographs was very helpful in ﬁnding exciting style images)
0.93 Context(we would like to thank):(the Google Cultural Institute; was; very helpful in ﬁnding exciting style images)

