In this work we formulate the navigation question as a reinforcement learning problem and show that data efﬁciency and task performance can be dramatically improved by relying on additional auxiliary tasks leveraging multimodal sensory inputs.
In particular we consider jointly learning the goal-driven reinforcement learning problem with auxiliary depth prediction and loop closure classiﬁcation tasks.
We provide detailed analysis of the agent behaviour1, its ability to localise, and its network activity dynamics, showing that the agent implicitly learns key navigation abilities.
Whilst conventional robotics methods, such as Simultaneous Localisation and Mapping (SLAM), tackle navigation through an explicit focus on position inference and mapping (Dissanayake et al., 2001), here we follow recent work in deep reinforcement learning (Mnih et al., 2015; 2016) and propose that navigational abilities could emerge as the by-product of an agent learning a policy that maximizes reward.
To improve statistical efﬁciency we bootstrap the reinforcement learning procedure by augmenting our loss with auxiliary tasks that provide denser training signals that support navigation-relevant representation learning.
We consider two additional losses: the ﬁrst one involves reconstruction of a low-dimensional depth map at each time step by predicting one input modality (the depth channel) from others (the colour channels).
To address the memory requirements of the task we rely on a stacked LSTM architecture (Graves et al., 2013; Pascanu et al., 2013).
We evaluate our approach using ﬁve 3D maze environments and demonstrate the accelerated learning and increased performance of the proposed agent architecture.
We also provide detailed analysis of the trained agent to show that critical navigation skills are acquired.
In particular, we show that the proposed agent resolves ambiguous observations and quickly localizes itself in a complex maze, and that this localization capability is correlated with higher task reward.
We rely on a end-to-end learning framework that incorporates multiple objectives.
The agent setup closely follows the work of (Mnih et al., 2016) and we refer to this work for the details (e.g.
The baseline that we consider in this work is an A3C agent (Mnih et al., 2016) that receives only RGB input from the environment, using either a recurrent or a purely feed-forward model (see Figure 2a,b).
To support the navigation capability of our approach, we also rely on the Nav A3C agent (Figure 2c) which employs a two-layer stacked LSTM after the convolutional encoder.
We expand the observations of the agents to include agent-relative velocity, the action sampled from the stochastic policy and the immediate reward, from the previous time step.
We opt to feed the velocity and previously selected action directly to the second recurrent layer, with the ﬁrst layer only receiving the reward.
We postulate that the ﬁrst layer might be able to make associations between reward and visual observations that are provided as context to the second layer from which the policy is computed.
In particular we consider predicting depth from the convolutional layer (we will refer to this choice as D1), or from the top LSTM layer (D2) or predicting loop closure (L).
While depth could be directly used as an input, we argue that if presented as an additional loss it is actually more valuable to the learning process.
Since we know from (Eigen et al., 2014) that a single frame can be enough to predict depth, we know this auxiliary task can be learnt.
Since the role of the auxiliary loss is just to build up the representation of the model, we do not necessarily care about the speciﬁc performance obtained or nature of the prediction.
We do care about the data efﬁciency aspect of the problem and also computational complexity.
If the loss is to be useful for the main task, we should converge faster on it compared to solving the RL problem (using less data samples), and the additional computational cost should be minimal.
To achieve this we use a low resolution variant of the depth map, reducing the screen resolution to 4x16 pixels2.
We explore two different variants for the loss.
To address this possible issue, we also consider a classiﬁcation loss, where depth at each position is discretised into 8 different bands.
The bands are non-uniformally distributed such that we pay more attention to far-away objects (details in Appendix B).
To produce the training targets, we detect loop closures based on the similarity of local position information during an episode, which is obtained by integrating 2D velocity over time.
, pT}, where pt is the position of the agent at time t, we deﬁne a loop closure label lt that is equal to 1 if the position pt of the agent is close to the position pt(cid:48) at an earlier time t(cid:48).
In order to avoid trivial loop closures on consecutive points of the trajectory, we add an extra condition on an intermediary position pt(cid:48)(cid:48) being far from pt.
However, here we focus on related work in deep RL.
In contrast, our contribution addresses fundamental questions of how to learn an intrinsic representation of space, geometry, and movement while simultaneously maximising rewards through reinforcement learning.
Our method is validated in challenging maze domains with random start and goal locations.
We consider a set of ﬁrst-person 3D mazes from the DeepMind Lab environment (Beattie et al., 2016) (see Fig.
For both variants (static and random goal) we consider a small and large map.
In each case we ran 64 experiments with randomly sampled hyper-parameters (for ranges and details please see the appendix).
(2015) reward clipping is used to stabilize learning, technique which we employed in this work as well.
We clearly indicate whether reward clipping is used by adding an asterisk to the agent name.
We can see that the regression agent (Nav A3C*+D1[MSE]) performs worse than one that does classiﬁcation (Nav A3C*+D1).
This result extends to other maps, and we therefore only use the classiﬁcation formulation in all our other results4.
Also we see that predicting depth from the last LSTM layer (hence providing structure to the recurrent layer, not just the convolutional ones) performs better.
We note some particular results from these learning curves.
We believe that this is mainly due to the slower, data inefﬁcient learning that is generally seen in pure RL approaches.
Supporting this we see that adding the auxiliary prediction targets of depth and loop closure (Nav A3C+D1D2L, black curve) speeds up learning dramatically on most of the mazes (see Table 1: AUC metric).
Although we place more value on the task performance than on the auxiliary losses, we report the results from the loop closure prediction task.
While it does worse, we include it because some analysis is based on this agent.
In order to evaluate the internal representation of location within the agent (either in the hidden units ht of the last LSTM, or, in the case of the FF A3C agent, in the features ft on the last layer of the conv-net), we train a position decoder that takes that representation as input, consisting of a linear classiﬁer with multinomial probability distribution over the discretized maze locations.
Note that we do not backpropagate the gradients from the position decoder through the rest of the network.
We observe that position entropy spikes after a respawn, then decreases once the agent acquires certainty about its location.
We observe that once the goal is ﬁrst found, the Nav A3C*+D1L agent is capable of directly returning to the correct branch in order to achieve the maximal score.
We hypothesize that the symmetry of the I-maze will induce a symmetric policy that need not be sensitive to the exact position of the agent (see analysis below).
A desired property of navigation agents in our Random Goal tasks is to be able to ﬁrst ﬁnd the goal, and reliably return to the goal via an efﬁcient route after subsequent re-spawns.
We visualize the agent’s policy by applying tSNE dimension reduction (Maaten & Hinton, 2008) to the cell activations at each step of the agent for each of the four goal locations.
INVESTIGATING DIFFERENT COMBINATIONS OF AUXILIARY TASKS Our results suggest that depth prediction from the policy LSTM yields optimal results.
However, several other auxiliary tasks have been concurrently introduced in (Jaderberg et al., 2017), and thus we provide a comparison of reward prediction against depth prediction.
Following that paper, we implemented two additional agent architectures, one performing reward prediction from the convnet using a replay buffer, called Nav A3C*+R, and one combining reward prediction from the convnet and depth prediction from the LSTM (Nav A3C+RD2).
We report the AUC (Area under learning curve), averaged over the best 5 hyperparameters.
We proposed a deep RL method, augmented with memory and auxiliary learning targets, for training agents to navigate within large and visually rich environments that include frequently changing start and goal locations.
Our results and analysis highlight the utility of un/self-supervised auxiliary objectives, namely depth prediction and loop closure, in providing richer training signals that bootstrap learning and enhance data efﬁciency.
Further, we examine the behavior of trained agents, their ability to localise, and their network activity dynamics, in order to analyse their navigational abilities.
Our approach of augmenting deep RL with auxiliary objectives allows end-end learning and may encourage the development of more general navigation strategies.
Notably, our work with auxiliary losses is related to (Jaderberg et al., 2017) which independently looks at data efﬁciency when exploiting auxiliary losses.
One difference between the two works is that our auxiliary losses are online (for the current frame) and do not rely on any form of replay.
Finally our focus is on the navigation domain and understanding if navigation emerges as a bi-product of solving an RL problem, while Jaderberg et al.
Whilst our best performing agents are relatively successful at navigation, their abilities would be stretched if larger demands were placed on rapid memory (e.g.
ACKNOWLEDGEMENTS Under review as a conference paper at ICLR 2017 We would like to thank Alexander Pritzel, Thomas Degris and Joseph Modayil for useful discussions, Charles Beattie, Julian Schrittwieser, Marcus Wainwright, and Stig Petersen for environment design and development, and Amir Sadik and Sarah York for expert human game testing.
Under review as a conference paper at ICLR 2017 A VIDEOS OF TRAINED NAVIGATION AGENTS We show the behaviour of Nav A3C*+D1L agent in 5 videos, corresponding to the 5 navigation environments: I-maze5, (small) static maze6, (large) static maze7, (small) random goal maze8 and (large) random goal maze9.
B NETWORK ARCHITECTURE AND TRAINING B.1 THE ONLINE MULTI-LEARNER ALGORITHM FOR MULTI-TASK LEARNING We introduce a class of neural network-based agents that have modular structures and that are trained on multiple tasks, with inputs coming from different modalities (vision, depth, past rewards and past actions).
Implementing our agent architecture is simpliﬁed by its modular nature.
Essentially, we construct multiple networks, one per task, using shared building blocks, and optimise these networks jointly.
Within each thread, we use a ﬂag-based system to subordinate gradient updates to the A3C reinforcement learning procedure.
B.2 NETWORK AND TRAINING DETAILS For all the experiments we use an encoder model with 2 convolutional layers followed by a fully connected layer, or recurrent layer(s), from which we predict the policy and value function.
We illustrate on Figure 8 the architecture of the Nav A3C+D+L+Dr agent.
We empirically decided to use the following quantization: {0, 0.05, 0.175, 0.3, 0.425, 0.55, 0.675, 0.8, 1} to ensure a uniform 5Video of the Nav A3C*+D1L agent on the I-maze: https://youtu.be/PS4iJ7Hk_BU 6Video of the Nav A3C*+D1L agent on static maze 1: https://youtu.be/-HsjQoIou_c 7Video of the Nav A3C*+D1L agent on static maze 2: https://youtu.be/kH1AvRAYkbI 8Video of the Nav A3C*+D1L agent on random goal maze 1: https://youtu.be/5IBT2UADJY0 9Video of the Nav A3C*+D1L agent on random goal maze 2: https://youtu.be/e10mXgBG9yo Under review as a conference paper at ICLR 2017 Figure 8: Details of the architecture of the Nav A3C+D+L+Dr agent, taking in RGB visual inputs xt, past reward rt−1, previous action at−1 as well as agent-relative velocity vt, and producing policy π, value function V , depth predictions gd(ft) and g(cid:48) d(ht) as well as loop closure detection gl(ht).
We optimise these parameters using an asynchronous version of RMSProp (Tieleman & Hinton, 2012).
(Nair et al., 2015) was a recent example of asynchronous and parallel gradient updates in deep reinforcement learning; in our case, we focus on the speciﬁc Asynchronous Advantage Actor Critic (A3C) reinforcement learning procedure in (Mnih et al., 2016).
We use 16 workers and the same RMSProp algorithm without momentum or centering of the variance.
For this reason through out the paper we actually report results in terms of agent perceived steps rather than environment steps.
That is, the maximal number of agent perceived step that we do for any particular run is 2.5e7.
In our grid we sample hyper-parameters from categorical distributions: • Learning rate was sampled from [10−4, 5 · 10−4].
In our previous set of experiments, rewards were scaled by a factor from {0.3, 0.5} and clipped to 1 prior to back-propagation in the Advantage Actor-Critic algorithm.
In our previous set of experiments, we used chunks of 100 steps.
In particular in the left plot we show that the baselines (A3C FF and A3C LSTM) as well as Nav A3C agent without auxiliary losses, perform worse without reward clipping than with reward clipping.
For this particular reason we chose to show the baselines with reward clipping in our main results.
C.3 NON-NAVIGATION TASKS IN 3D MAZE ENVIRONMENTS We have evaluated the behaviour of the agents introduced in this paper, as well as agents with reward prediction, introduced in (Jaderberg et al., 2017) (Nav A3C*+R) and with a combination of reward prediction from the convnet and depth prediction from the policy LSTM (Nav A3C+RD2), on different 3D maze environments with non-navigation speciﬁc tasks.
C.5 ASYMPTOTIC PERFORMANCE OF THE AGENTS Finally, we compared the asymptotic performance of the agents, both in terms of navigation (ﬁnal rewards obtained at the end of the episode) and in terms of their representation in the policy LSTM.
Rather than visualising the convolutional ﬁlters, we quantify the change in representation, with and Under review as a conference paper at ICLR 2017 Table 3: Asymptotic performance analysis of two agents in the Random Goal 2 maze, comparing training for 120M Labyrinth frames vs.
Speciﬁcally, we compare the baseline agent (LSTM A3C*) to a navigation agent with one auxiliary task (depth prediction), that gets about twice as many gradient updates for the same number of frames seen in the environment: once for the RL task and once for the auxiliary depth prediction task.
For this reason, we believe that the auxiliary task do more than simply accelerate training.