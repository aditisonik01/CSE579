In this work we break this constraint by decoupling modules by introducing a model of the future computation of the network graph.
In particular we focus on modelling error gradients: by using the modelled synthetic gradient in place of true backpropagated error gradients we decouple subgraphs, and can update them independently and asynchronously i.e.
we realise decoupled neural interfaces.
We show results for feed-forward models, where every layer is trained asynchronously, recurrent neural networks (RNNs) where predicting one’s future gradient extends the time over which the RNN can effectively model, and also a hierarchical RNN system with ticking at different timescales.
Finally, we demonstrate that in addition to predicting gradients, the same framework can be used to predict inputs, resulting in models which are decoupled in both the forward and backwards pass – amounting to independent networks which co-learn such that they can be composed into a single functioning corporation.
This process results in several forms of locking, namely: (i) Forward Locking – no module can process its incoming data before the previous nodes in the directed forward graph have executed; (ii) Update Locking – no module can be updated before all dependent modules have executed in forwards mode; also, in many credit-assignment algorithms (including backpropagation [18]) we have (iii) Backwards Locking – no module can be updated before all dependent modules have executed in both forwards mode and backwards mode.
To update weights θi of module i we drastically approximate the function implied by backpropagation: = fBprop((hi, xi, yi, θi), (hi+1, xi+1, yi+1, θi+1), .
While the communication protocol is general with respect to the means of generating a training signal, here we focus on a speciﬁc implementation for networks trained with gradient descent – we replace a standard neural interface (a connection between two modules in a neural network) with a Decoupled Neural Interface (DNI).
And by removing updateand backwards locking in this way, we can train networks without a synchronous backward pass.
We also show preliminary results that extend this idea to also remove forward locking – resulting in networks whose modules can also be trained without a synchronous forward pass.
When applied to RNNs we show that using synthetic gradients allows RNNs to model much greater time horizons than the limit imposed by truncating backpropagation through time (BPTT).
We also show that using DNI to decouple a system of two RNNs running at different timescales can greatly increase training speed of the faster RNN.
Our synthetic gradient model is most analogous to a value function which is used for gradient ascent [2] or a value function used for bootstrapping.
In contrast, by framing the interaction between layers as a local communication problem with DNI, we remove the need for global knowledge of the learning system.
We begin by describing the high-level communication protocol that is used to allow asynchronously learning agents to communicate.
We can apply this protocol to neural networks communicating, resulting in what we call Decoupled Neural Interfaces (DNI).
domain, we concentrate our empirical study on differentiable networks trained with backpropagation and gradient-based updates.
Therefore, we focus on producing error gradients as the feedback ˆδA which we dub synthetic gradients.
2.1 Synthetic Gradient for Feed-Forward Networks We now apply the above process to the case of a feed-forward neural net consisting of N layers fi, i ∈ {1, .
To remove the update locking of layer i to F N i+1 we can use the communication protocol described previously.
To train the parameters φi+1 of the synthetic gradient model Mi+1 (which is also a neural network), we simply wait for the true error gradient δi to be computed (after a full forwards and backwards execution of F N i+1), and ﬁt the synthetic gradient to the true gradients by minimising a loss Lδi = d(ˆδi, δi).
In our experiments we found using L2 distance as a loss worked well.
Furthermore, for a feed-forward network, we can use synthetic gradients as communication feedback to decouple every layer in the network, as shown in Fig.
However, using the Decoupled Neural Interfaces framework, we can approximate δT with a synthetic gradient model ˆδT = MT (hT ) as shown and described in Fig.
3.2 we show results on sequence-to-sequence tasks and language modelling.
We also propose an extension to aid learning of synthetic gradient models for RNNs, which is to introduce another auxiliary task from the RNN, described in Fig.
Arbitrary Network Graphs Although we have explicitly described the application of DNI for communication between layers in feed-forward networks, and between recurrent cores in recurrent networks, there is nothing to restrict the use of DNI for arbitrary network graphs.
3.3 where we show communication between two RNNs, which tick at different rates, where the communication can be learnt by using DNI.
In this section we apply DNIs to feed-forward networks.
Every layer DNI We ﬁrst look at training an FCN for MNIST digit classiﬁcation [14].
We use DNI as in the scenario illustrated in Fig.
We perform experiments where we vary the depth of the model (between 3 and 6 layers), on MNIST digit classiﬁcation and CIFAR-10 object recognition [12].
Looking at the results in Table 1 we can see that DNI does indeed work, successfully update-decoupling all layers at a small cost in accuracy, demonstrating that it is possible to produce effective gradients without either label or true gradient information.
Further, once we condition the synthetic gradients on the labels, we can successfully train deep models with very little degradation in accuracy.
For example, on CIFAR-10 we can train a 5 layer model, with backpropagation achieving 42% error, with DNI achieving 47% error, and when conditioning the synthetic gradient on the label (cDNI) get 44%.
In fact, on MNIST we successfully trained up to 21 layer FCNs with cDNI to 2% error (the same as with using backpropagation).
As another baseline, we tried using historical, stale gradients with respect to activations, rather than synthetic gradients.
We took an exponential average historical gradient, searching over the entire spectrum of decay rates and the best results attained on MNIST classiﬁcation were 9.1%, 11.8%, 15.4%, 19.0% for 3 to 6 layer FCNs respectively – marginally better than using zero gradients (no backpropagation) and far worse than the associated cDNI results of 2.2%, 1.9%, 1.7%, 1.6%.
Thus, we believe that DNI, which uses a parametric approximation to the gradient with respect to activations, is the most desirable approach.
The spatial resolution of activations from layers in a CNN results in high dimensional activations, so we use synthetic gradient models which themselves are CNNs without pooling and with resolution-preserving zero-padding.
Sparse Updates To demonstrate the gains by decoupling layers given by DNI, we perform an experiment on a four layer FCN model on MNIST, where the backwards pass and update for every layer occurs in random order and only with some probability pupdate (i.e.
Right: The same setup as previously described however we also use a synthetic input model before every layer, which allows the network to also be forwards decoupled.
We ran 100 experiments with different values of pupdate uniformly sampled between 0 and 1.
Complete Unlock As a drastic extension, we look at making feed-forward networks completely asynchronous, by removing forward locking as well.
We now turn our attention to experiments on the application of DNI to recurrent neural networks as discussed in Sect.
We test our models on the Copy task, Repeat Copy task, as well as character-level language modelling.
For all experiments we use an LSTM [9] of the form in [7], whose output is used for the task at hand, and additionally as input to the synthetic gradient model (which is shared over all timesteps).
We also look at incorporating an auxiliary task which predicts the output of the synthetic gradient model T steps in the future as explained in Sect.
We train the RNNs with different BPTT unroll lengths with DNI (solid lines) and without DNI (dashed lines).
Copy and Repeat Copy We ﬁrst look at two synthetic tasks – Copy and Repeat Copy tasks from [8].
While normally the RNN would be unrolled for the length of the episode before BPTT is performed, T = Ttask, we wish to test the length of time the RNN is able to model with and without DNI bridging the BPTT limit.
We train the RNN with truncated BPTT: T ∈ {2, 3, 4, 5} with and without DNI.
However, by introducing DNI we can extend the time dependency that is able to be modelled by an RNN.
The additional computational complexity is negligible but we require an additional recurrent core to be stored in memory (this is illustrated in Fig.
Because we can model larger time dependencies with a smaller T , our models become more data-efﬁcient, learning faster and having to see less data samples to solve a task.
Furthermore, when we include the extra task of predicting the synthetic gradient that will be produced T steps in the future (DNI + Aux), the RNNs with DNI are able to model even larger time dependencies.
Language Modelling We also applied our DNI-enabled RNNs to the task of character-level language modelling, using the Penn Treebank dataset [16].
We use an LSTM with 1024 units, which at every timestep reads a character and must predict the next character in the sequence.
We train with BPTT with and without DNI, as well as when using future synthetic gradient prediction (DNI + Aux), with T ∈ {2, 3, 4, 5, 8} as well as strong baselines with T = 20, 40.
We measure error in bits per character (BPC) as in [7], perform early stopping based on validation set error, and for simplicity do not perform any learning rate decay.
When adding DNI, we see an increase in speed of learning (learning curves can be found in Fig.
Although we report results only with LSTMs, we have found DNI to work similarly for vanilla RNNs and Leaky RNNs [17].
In this section, we explore the use of DNI for communication between arbitrary graphs of networks.
As a simple proof-of-concept, we look at a system of two RNNs, Network A and Network B, where Network B is executed at a slower rate than Network A, and must use communication from Network A to complete its task.
First, we test this system trained end-to-end, with full backpropagation through all connections, which requires the joint Network A-Network B system to be unrolled for T 2 timesteps before a single weight update to both Network A and Network B, as the communication between Network A to Network B causes Network A to be update locked to Network B.
We the train the same system but using DNI to create a learnable bridge between Network A and Network B, thus decoupling Network A from Network B.
In this work we introduced a method, DNI using synthetic gradients, which allows decoupled communication between components, such that they can be indepedently updated.
We demonstrated this in feed-forward nets with all layers decoupled – allowing them to be trained non-sequentially, without locking.
We also showed large gains from the increased time horizon that DNI-enabled RNNs are able to model, as well as faster convergence.
Finally we demonstrated application to a multi-network system: a communicating pair of fastand slow-ticking RNNs can be decoupled, greatly accelarating learning.
To our knowledge this is the ﬁrst time that neural net modules have been decoupled, and the update locking has been broken.
In this section we give the implementation details of the experimental setup used in the experiments from Sect.
Conditional DNI (cDNI) In order to provide DNI module with the label information in FCN, we simply concatenate the one-hot representation of a sample’s label to the input of the synthetic gradient model.
For convolutional networks we add label information in the form of one-hot encoded channel masks, thus we simply concatenate ten additional channels to the activations, nine out of which are ﬁlled with zeros, and one (corresponding to sample’s label) is ﬁlled with ones.
We perform a hyperparameter search over the number of hidden layers in the synthetic gradient model (from 0 to 2, where 0 means we use a linear model such that ˆδ = M (h) = φwh + φb) and select the best number of layers for each experiment type (given below) based on the ﬁnal test performance.
We used cross entropy loss for classiﬁcation and L2 loss for synthetic gradient regression which was weighted by a factor of 1 with respect to the classiﬁcation loss.
In the completely unlocked model, we use the identical architecture used for the synthetic gradient model, but for simplicity both synthetic gradient and synthetic input models use a single hidden layer (for both DNI and cDNI), and train it to produce synthetic inputs ˆhi such that ˆhi (cid:39) hi.
Single DNI We look at training an FCN for MNIST digit classiﬁcation using a network with 6 layers (5 hidden layers, one classiﬁcation layer), but splitting the network into two unlocked sub-networks by inserting a single DNI at a variable position, as illustrated in Fig.
When training this 6 layer FCN with vanilla backpropagation we attain 1.6% test error.
If we decouple the layers without DNI, by just not backpropagating any gradient between them, this results in bad performance – between 2.9% and 23.7% error for after layer 1 and layer 5 respectively.
However, as we show in Sect.
We also plot the synthetic gradient regression error (L2 distance), cosine distance, and the sign error (the number of times the sign of a gradient dimension is predicted incorrectly) compared to the true error gradient in Fig.
We perform a hyperparameter search of whether or not to backpropagate synthetic gradient model error into the LSTM core (the model was not particularly sensitive to this, but occasionally backpropagating synthetic gradient model error resulted in more unstable training).
Copy and Repeat Copy Task In these tasks we use 256 LSTM units and the model was optimised with Adam with a learning rate of 7 × 10−5 and a batch size of 256.
Learning is performed with the use of Adam with learning rate of 7 × 10−5 (which we select to maximise the score of the baseline model through testing also 1 × 10−4 and 1 × 10−6) without any learning rate decay or additional regularisation.
Each 5k iterations we record validation error (in terms of average bytes per character) and store the network which achieved the smallest one.
Once validation error starts to increase we stop training and report test error using previously saved network.
We train the RNNs with different BPTT unroll lengths with DNI (solid lines) and without DNI (dashed lines).
We performed a hyperparameter search over the factor by which the synthetic gradient should by multiplied by before being backpropagated through Network A, which we selected as 10 by choosing the system with the lowest training error.
We run all models for 2.5M optimisation steps