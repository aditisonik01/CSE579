Thus, we increase the quality of images generated by avoiding common glitches, make the results look signiﬁcantly more plausible, and extend the functional range of these algorithms—whether for portraits or landscapes, etc.
Through our social media bot that ﬁrst provided these algorithms as a service (Champandard 2015), we observe that users have clear expectations how style transfer should ∗This research was funded out of the marketing budget.
We attribute these problems to two underlying causes: 1.
To remedy this, we introduce an architecture that bridges the gap between generative algorithms and pixel labeling neural networks.
Then we explain how existing algorithms can be adapted to include such annotations, and ﬁnally we showcase some applications in style transfer as well as image synthesis by analogy (e.g.
Figure 3: Our augmented CNN that uses regular ﬁlters of N channels (top), concatenated with a semantic map of M=1 channel (bottom) either output from another network capable of labeling pixels or as manual annotations.
Before concatenation, the semantic channels are weighted by parameter γ to provide an additional user control point: Our approach (Li and Wand 2016) to style transfer, using optimization to minimize content reconstruction error Ec (weighted by α) and style remapping error Es (weight β).
E = αEc + βEs First we introduce an augmented CNN (Figure 6) that incorporates semantic information, then we deﬁne the input semantic map and its representation, and ﬁnally show how the algorithm is able to exploit this additional information.
Our augmented network concatenates additional semantic channels ml of size M at the same resolution, computed by down-sampling a static semantic map speciﬁed as input.
Typical results from our solution are shown in portraits from Figure 2, which contains both success cases (top row) and sub-optimal results (bottom row).
Figure 5 shows a grid with visualizations of results as β and γ vary; we note the following: • The quality and variety of the style degenerates as γ increases too far, without noticeably improving the precision wrt.
Here we report observations from working with the algorithm, and provide our interpretations.
Authored Representations We noticed that when users are asked to annotate images, after a bit of experience with the system, they implicitly create “semantic embeddings” that compactly describe pixel classes.
Weight Sensitivity The algorithm is less fragile to adjustments in style weight; typically as the weight increases, the image degenerates and becomes a patchwork of the style content.
Performance Due to the additional channels in the model, our algorithm requires more memory as well as extra computation compared to its predecessor.
In this paper, we resolved these issues by annotating input images with a semantic map, either manually authored or from pixel labeling algorithms.
We introduced an augmented CNN architecture to leverage this information at runtime, while further tying advances in image segmentation to image synthesis.
We showed that existing patch-based algorithms require minor adjustments and perform very well using this additional information.
The examples shown for style transfer show how this technique helps deal with completely opposite patterns/colors in corresponding image regions, and we analyzed how it helps users control the output of these algorithms better