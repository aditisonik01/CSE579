Controlling Perceptual Factors in Neural Style Transfer Figure 1: Overview of our control methods.
Here we extend the existing method beyond the paradigm of transferring global style information between pairs of images.
In particular, we introduce control over spatial location, colour information and across spatial scale.
We demonstrate how this enhances the method by allowing high-resolution controlled stylisation and helps to alleviate common failure cases such as applying ground textures to sky regions.
Furthermore, by decomposing style into these perceptual factors we enable the combination of style information from multiple sources to generate new, perceptually appealing styles from existing ones.
Finally we show how the introduced control measures can be applied in recent methods for Fast Neural Style Transfer.
In order to identify these factors, we observe some of the different ways that one might describe an artwork such as Vincent van Gogh’s A Wheatﬁeld with Cypresses (Fig.
These observation motivates our hypothesis: image style can be perceptually factorised into style in different spatial regions, colour and luminance information, and across spatial scales, making them meaningful control dimensions for image stylisation.
Here we build on this hypothesis to introduce meaningful control to a recent image stylisation method known as Neural Style Transfer [8] in which the image statistics that capture content and style are deﬁned on feature responses in a Convolutional Neural Network (CNN) [22].
Namely, we (a) Content(b) Spatial Control(c) Colour Control(d) Scale Controlintroduce methods for controlling image stylisation independently in different spatial regions (Fig.
We show how they can be applied to improve Neural Style Transfer and to alleviate some of its common failure cases.
Moreover, we demonstrate how the factorisation of style into these aspects can gracefully combine style information from multiple images and thus enable the creation of new, perceptually interesting styles.
We also show a method for creating high-resolution stylisation while transferring coarse-scale structure.
Finally, we show that in addition to the original optimisation-based style transfer, these control methods can also be applied to recent fast approximations of Neural Style Transfer [13, 23] There is a large body of work on image stylisation techniques.
Our main goal in this work is to introduce intuitive ways to control Neural Style Transfer to combine the advantages of that method with the more ﬁnegrained user control of earlier stylisation methods.
We deﬁne a content image xC and a style image xS with corresponding feature representations F(cid:96)(xC) and F(cid:96)(xS) in layer (cid:96) of a Convolutional Neural Network (CNN).
As in the original work [8], we use the VGG-19 Network and include “conv4 2” as the layer (cid:96)C for the image content and Gram Matrices from layers “conv1 1”,“conv2 1”,“conv3 1”,“conv4 1”,“conv5 1” as the image statistics that model style.
We ﬁrst introduce means to spatially control Neural Style Transfer.
Our goal is to control which region of the style image is used to stylise each region in the content image.
For example, we would like to apply one style to the sky region and another to the ground region of an image to either avoid artefacts (Fig.
We take as input R spatial guidance channels Tr for both the content and style image (small insets in (Fig.
This can be done by simple re-sampling or more involved methods as we explain later in this section.
We ﬁrst discuss algorithms for synthesis given the guidance maps.
Guided Gram Matrices In the ﬁrst method we propose, we multiply the feature maps of each layer included in the style features with R guidance channels Tr (cid:96) and compute one spatially guided Gram Matrix for each of the R regions in the style image.
Formally we deﬁne a spatially guided feature map as (cid:96) ◦ F(cid:96)(x)[:,i] (cid:96) (x)[:,i] is the ith column vector of Fr (5) (cid:96) (x), r ∈ R and ◦ Here Fr denotes element-wise multiplication.
We normalise Tr (cid:96) i = 1.
We address this by dividing both images into a sky and a ground region (Fig.
Given the input guidance channels Tr, we need to ﬁrst propagate this channel to produce guidance channels Tr (cid:96) for each layer.
However, we often ﬁnd that doing so fails to keep the desired separation of styles by region, e.g., ground texture Figure 2: Spatial guidance in Neural Style Transfer.
Instead we use an eroded version of the spatial guiding channels.
We deﬁne spatial guidance channels only on the neurons whose receptive ﬁeld is entirely inside the guidance region and add another global guidance channel that is constant over the entire image.
In that way we only enforce spatial guidance for neurons that have a clear assignment to a region and use the unguided style loss for neurons whose receptive ﬁeld overlaps with the boundary.
Guided Sums Alternatively, instead of computing a Gram Matrix for each guidance channel, we can also just stack the guidance channels with the feature maps as it is done in [2] to spatially guide neural patches [16].
In practice we ﬁnd that this method can often give decent results but also does not quite capture the texture of the style image — as would be expected from the inferior texture model.
Here we present two simple methods to preserve the colours of the source image during Neural Style Transfer—in other words, to transfer the style without transferring the colours.
We compare two different approaches to colour preservation: colour histogram matching and luminance-only transfer (Fig.
Luminance-only transfer The ﬁrst method we consider is to perform style transfer only in the luminance channel, as used in Image Analogies [12].
For that we simply match mean and variance of the content luminance.
Here we use linear methods, which are simple and effective for colour style transfer.
In general, we ﬁnd that the colour matching method works reasonably well with Neural Style Transfer (Fig.
While we found that this is usually very difﬁcult to spot, it can be a problem for styles with prominent brushstrokes since a single brushstroke can change colour in an unnatural way.
For a more detailed discussion of colour preservation in Neural Style Transfer we refer the reader to a technical report in the Supplementary Material, section 2.1.
(LS − µS) + µC The second method we present works as follows.
Given the style image xS, and the content image xC, the style image’s colours are transformed to match the colours of the In this section, we describe methods for mixing different styles at different scales, and generating high-resolution output with style at desired scales.
Scale control for style mixing First we introduce a method to control the stylisation independently on different spatial scales.
Our goal is to pick separate styles for different scales.
For example, we want to combine the ﬁne-scale brushstrokes of one painting (Fig.
We deﬁne the style of an image at a certain scale as the distribution of image structures in image neighbourhoods of a certain size f.
To model image style on larger scales, we propose to use the Gram Matrices from different layers in the CNN.
Here we show a way to combine scales that avoids this problem.
We ﬁrst create a new style image that combines ﬁne-scale information from one image with coarse scale information from another (Fig.
We then use the new style image in the original Neural Style Transfer.
We do this by applying Neural Style Transfer from the ﬁne-scale style image to the coarse-scale style image, using only the Gram Matrices from lower layers in the CNN (e.g., only layer “conv1 1” and “conv2 1” in Fig.
We initialise the optimisation procedure with the coarse-scale image and omit the content loss entirely, so that the ﬁne-scale texture from the coarse-style image will be fully replaced.
While this is not guaranteed, as it depends on the optimiser, we empirically ﬁnd it to be effective for the L-BFGS method typically used in Neural Style Transfer.
For example, we combine the ﬁne scale of Style I with the coarse scale of Style II to re-paint the angular cubistic shapes in Fig.
Or we combine the ﬁne scale of Style II with the coarse scale of Style III to replace the angular shapes by round structures, giving the image a completely different “feel” (compare Fig.
We also noticed that this technique effectively removes low-level noise that is typical for neural image synthesis.
We now show how to apply the spatial and colour control described above to these Fast Neural Style Transfer methods.
We use Johnson’s excellent publicly-available implementation of Fast Neural Style Transfer [13]2.
The networks we train all use the well-tuned default parameters in that implementation including Instance Normalization [24] (for details see Supplementary Material, section 4).
For comparability and to stay in the domain of styles that give good results with Fast Neural Style Transfer, we use the styles published with that implementation.
For both methods we match the mean luminance of the output image to that of the content image.
In general, we ﬁnd that colour preservation with the luminance network better combines stylisation with structures in the content image (Fig.
For more examples of new styles and results of interpolating between styles, we refer the reader to the Supplementary Material, sections 3.2 and 3.3.
In practice, we ﬁnd that for the VGG-19 network, there is a sweet spot around 5002 pixels for the size of the input images, such that the stylisation is appealing but the content is well-preserved (Fig.
Here we show that the same scale separation principle from the previous section can be used in order to produce high-resolution outputs with large-scale stylisation.
We are given high-resolution content and style images xC and xS, both having the same size with N 2 pixels in total.
We downsample each image by a factor k such that N/k corresponds to the desired stylisation resolution, e.g., 5002 for VGG, and then perform stylisation.
We can then produce high-resolution output from this image by up-sampling the low-resolution output to N 2 pixels, and use this as initialisation for Neural Style (a) Content/Style(b) Low-res(c) High-res (ctf)(d) High-res7.2.
Spatial control We now describe training a feed-forward network to apply different styles to different regions.
We show that this can be done with a surprisingly small modiﬁcation to Johnson’s training procedure [13], which we illustrate with the following example.
We create the style image by vertically concatenating the Candy and Feathers images shown in Fig.
Surprisingly, we ﬁnd that the guidance channels can be kept constant during training: during training we required the feed-forward network to always stylise the lower half of the image with one style and the upper half with another.
Nevertheless the network robustly learns the correspondence between guidance channels and styles, so that at test time we can pass arbitrary masks to the feed-forward network to spatially guide the stylisation (Fig.
By providing an automaticallygenerated ﬁgure-ground segmentation [21] we can create an algorithm that performs fast spatially-varying stylisation automatically.
6(g),(h)) In this work, we introduce intuitive ways to control Neural Style Transfer.
We hypothesise that image style includes factors of space, colour, and scale, and presented ways to access these factors during style transfer, in both fast and slow versions.
Even though we have found ways to control some aspects of style, it may be necessary to enforce further factorisation during network training.
Wei and M.
Nevertheless, we managed to ﬁnd practical ways to intuitively control Neural Style Transfer that substantially improve both the quality and ﬂexibility of the existing method.
Welling, editors, Computer Vision – ECCV 2016, number 9906 in Lecture Notes in Computer Science, pages 694–711.
Webb, J