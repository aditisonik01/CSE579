In this paper, we present a new neural network architecture for model-free reinforcement learning.
Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function.
Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions.
Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain.
Here, we take an alternative but complementary approach of focusing primarily on innovating a neural network architecture that is better suited for model-free RL.
The proposed network architecture, which we name the dueling architecture, explicitly separates the representation of state values and (state-dependent) action advantages.
In one time step (leftmost pair of images), we see that the value network stream pays attention to the road and in particular to the horizon, where new cars appear.
In the experiments, we demonstrate that the dueling architecture can more quickly identify the correct action during policy evaluation as redundant or similar actions are added to the learning problem.
We also evaluate the gains brought in by the dueling architecture on the challenging Atari 2600 testbed.
Background We consider a sequential decision making setup, in which an agent interacts with an environment E over discrete time steps, see Sutton & Barto (1998) for an introduction.
(cid:80)∞ The agent seeks maximize the expected discounted return, where we deﬁne the discounted return as Rt = τ =t γτ−trτ .
We deﬁne the optimal Q∗(s, a) = maxπ Qπ(s, a).
To approximate them, we can use a deep Q-network: Q(s, a; θ) with parameters θ.
To estimate this network, we optimize the following sequence of loss functions at iteration i: = r + γ max (5) where θ− represents the parameters of a ﬁxed and separate target network.
We could attempt to use standard Qlearning to learn the parameters of the network Q(s, a; θ) online.
The sequence of losses thus takes the form We deﬁne another important quantity, the advantage function, relating the value and Q functions: Aπ(s, a) = Qπ(s, a) − V π(s).
In this paper, Dueling Network Architectures for Deep Reinforcement Learning we use the improved Double DQN (DDQN) learning algorithm of van Hasselt et al.
To strengthen the claim that our dueling architecture is complementary to algorithmic innovations, we show that it improves performance for both the uniform and the prioritized replay baselines (for which we picked the easier to implement rank-based variant), with the resulting prioritized dueling variant holding the new state-of-the-art.
The Dueling Network Architecture The key insight behind our new architecture, as illustrated in Figure 2, is that for many states, it is unnecessary to estimate the value of each action choice.
To bring this insight to fruition, we design a single Qnetwork architecture, as illustrated in Figure 1, which we refer to as the dueling network.
However, instead of following the convolutional layers with a single sequence of fully connected layers, we instead use two sequences (or streams) of fully connected layers.
Let us consider the dueling network shown in Figure 1, where we make one stream of fully-connected layers output a scalar V (s; θ, β), and the other stream output an |A|dimensional vector A(s, a; θ, α).
Using the deﬁnition of advantage, we might be tempted to construct the aggregating module as follows: Q(s, a; θ, α, β) = V (s; θ, β) + A(s, a; θ, α), (7) Note that this expression applies to all (s, a) instances; that is, to express equation (7) in matrix form we need to replicate the scalar, V (s; θ, β), |A| times.
However, we need to keep in mind that Q(s, a; θ, α, β) is only a parameterized estimate of the true Q-function.
Equation (7) is unidentiﬁable in the sense that given Q we cannot recover V and A uniquely.
To address this issue of identiﬁability, we can force the advantage function estimator to have zero advantage at the chosen action.
That is, we let the last module of the network implement the forward mapping Q(s, a; θ, α, β) = V (s; θ, β) + A(s, a; θ, α) − max a(cid:48)∈|A| A(s, a(cid:48); θ, α) Dueling Network Architectures for Deep Reinforcement Learning for a∗ = arg maxa(cid:48)∈A Q(s, a(cid:48); θ, α, β) = Now, arg maxa(cid:48)∈A A(s, a(cid:48); θ, α), we obtain Q(s, a∗; θ, α, β) = V (s; θ, β).
We also experimented with a softmax version of equation (8), but found it to deliver similar results to the simpler module of equation (9).
As the dueling architecture shares the same input-output interface with standard Q networks, we can recycle all learning algorithms with Q networks (e.g., DDQN and SARSA) to train the dueling architecture.
Experiments We now show the practical performance of the dueling network.
We start with a simple policy evaluation task and then show larger scale results for learning policies for general Atari game-playing.
We start by measuring the performance of the dueling architecture on a policy evaluation task.
We choose this particular task because it is very useful for evaluating network architectures, as it is devoid of confounding factors such as the choice of exploration strategy, and the interaction between policy improvement and policy evaluation.
In this experiment, we employ temporal difference learning (without eligibility traces, i.e., λ = 0) to learn Q values.
More speciﬁcally, given a behavior policy π, we seek to estimate the state-action value Qπ(·,·) by optimizing the sequence of costs of equation (4), with target yi = r + γEa(cid:48)∼π(s(cid:48)) [Q(s(cid:48), a(cid:48); θi)] .
We, however, do not modify the behavior policy as in Expected SARSA.
To evaluate the learned Q values, we choose a simple environment where the exact Qπ(s, a) values can be computed separately for all (s, a) ∈ S ×A.
This environment, which we call the corridor is composed of three connected corridors.
We also have the freedom of adding an arbitrary number of no-op actions.
In our setup, the two vertical sections both have 10 states while the horizontal section has 50.
We use an -greedy policy as the behavior policy π, which chooses a random action with probability  or an action according to the optimal Q function arg maxa∈A Q∗(s, a) with probability 1 − .
In our experiments,  is chosen to be 0.001.
We compare a single-stream Q architecture with the dueling architecture on three variants of the corridor environment with 5, 10 and 20 actions respectively.
We measure performance by Squared Error s∈S,a∈A(Q(s, a; θ) − Qπ(s, a))2.
The results of the comparison are summarized in  However, when we increase the number of actions, the dueling architecture performs better than the traditional Q-network.
This is a very promising result because many control tasks with large action spaces have this property, and consequently we should expect that the dueling network will often lead to much faster convergence than a traditional single stream network.
In the following section, we will indeed see that the dueling network results in substantial gains in performance in a wide-range of Atari games.
We perform a comprehensive evaluation of our proposed method on the Arcade Learning Environment (Bellemare et al., 2013), which is composed of 57 Atari games.
We follow closely the setup of van Hasselt et al.
We train the dueling network with the DDQN algorithm as presented in Appendix A.
At the end of this section, we incorporate prioritized experience replay (Schaul et al., 2016).
Our network architecture has the same low-level convolutional structure of DQN (Mnih et al., 2015; van Hasselt et al., 2015).
We combine the value and advantage streams using the module described by Equation (9).
We adopt the optimizers and hyper-parameters of van Hasselt et al.
(2015), with the exception of the learning rate which we chose to be slightly lower (we do not do this for double DQN as it can deteriorate its performance).
Since both the advantage and the value stream propagate gradients to the last convolutional layer in the backward pass, we rescale the combined gradient entering the last convolutional layer by 1/ 2.
In addition, we clip the gradients to have their norm less than or equal to 10.
To isolate the contributions of the dueling architecture, we re-train DDQN with a single stream network using exactly the same procedure as described above.
Speciﬁcally, we apply gradient clipping, and use 1024 hidden units for the ﬁrst fully-connected layer of the network so that both architectures (dueling and single) have roughly the same number of parameters.
We refer to this re-trained model as Single Clip, while the original trained model of van Hasselt et al.
As in (van Hasselt et al., 2015), we start the game with up to 30 no-op actions to provide random starting positions for the agent.
To evaluate our approach, we measure improvement in percentage (positive or negative) in score over the better of human and baseline agent scores: We took the maximum over human and baseline agent scores as it prevents insigniﬁcant changes to appear as 2The number of actions ranges between 3-18 actions in the 103104No.
Overall, our agent (Duel Clip) achieves human level performance on 42 out of 57 games.
To obtain a more robust measure, we adopt the methodology of Nair et al.
Speciﬁcally, for each game, we use 100 starting points sampled from a human expert’s trajectory.
We refer to this metric as Human Starts.
In particular, our agent does better than the Single baseline on 70.2% (40 out of 57) games and on games of 18 actions, Duel Clip is 83.3% better (25 out of 30).
So in our ﬁnal experiment, we investigate the integration of the dueling architecture with prioritized experience replay.
We use the prioritized variant of DDQN (Prior.
We also chose not to measure performance in terms of percentage of human performance alone because a tiny difference relative to the baseline on some games can translate into hundreds of percent in human performance difference.
For comparison we also show results for the deep Q-network of Mnih et al.
Again, we seen that the improvements are often very dramatic.
We veriﬁed that this gain was mostly brought in by gradient clipping.
For this reason, we incorporate gradient clipping in all the new approaches.
To better understand the roles of the value and the advantage streams, we compute saliency maps (Simonyan et al., 2013).
More speciﬁcally, to visualize the salient part of the image as seen by the value stream, we compute the absolute value of the Jacobian of (cid:98)V with respect to the input frames: |∇s(cid:98)V (s; θ)|.
Similarly, to visutage stream, we compute |∇s(cid:98)A(s, arg maxa(cid:48) (cid:98)A(s, a(cid:48)); θ)|.
We keep all the parameters of the prioritized replay as described in (Schaul et al., 2016), namely a priority exponent of 0.7, and an annealing schedule on the importance sampling exponent from 0.5 to 1.
We combine this baseline with our dueling architecture (as above), and again use gradient clipping (Prior.
To avoid adverse interactions, we roughly re-tuned the learning rate and the gradient clipping norm on a subset of 9 games.
As a result of rough tuning, we settled on 6.25× 10−5 for the learning rate and 10 for the gradient clipping norm (the same as in the previous section).
When evaluated on all 57 Atari games, our prioritized dueling agent performs signiﬁcantly better than both the prioritized baseline agent and the dueling agent alone.
When initializing the games using up to 30 no-ops action, we observe mean and median scores of 591% and 172% respectively.
Here, we place the gray scale input frames in the green and blue channel and the saliency maps in the red channel.
This more frequent updating of the value stream in our approach allocates more resources to V , and thus allows for better approximation of the state values, which in turn need to be accurate for temporaldifference-based methods like Q-learning to work (Sutton & Barto, 1998).
Conclusions We introduced a new neural network architecture that decouples value and advantage in deep Q-networks, while sharing a common feature learning module