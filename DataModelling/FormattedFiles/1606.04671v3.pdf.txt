We evaluate this architecture extensively on a wide variety of reinforcement learning tasks (Atari and 3D maze games), and show that it outperforms common baselines based on pretraining and ﬁnetuning.
Using a novel sensitivity measure, we demonstrate that transfer occurs at both low-level sensory and high-level control layers of the learned policy.
Unfortunately, the approach has drawbacks which make it unsuitable for transferring across multiple tasks: if we wish to leverage knowledge acquired over a sequence of experiences, which model should we use to initialize subsequent models? This seems to require not only a learning method that can support transfer learning without catastrophic forgetting, but also foreknowledge of task similarity.
As we will show, progressive networks naturally accumulate experiences and are immune to catastrophic forgetting by design, making them an ideal springboard for tackling long-standing problems of continual or lifelong learning.
Second, we extensively evaluate the model in complex reinforcement learning domains.
In the process, we also evaluate alternative approaches to transfer (such as ﬁnetuning) within the RL domain.
In particular, we show that progressive networks provide comparable (if not slightly better) transfer performance to traditional ﬁnetuning, but without the destructive consequences.
Finally, we develop a novel analysis based on Fisher Information and perturbation which allows us to analyse in detail how and where transfer occurs across tasks.
f is an element-wise non-linearity: we use f (x) = max(0, x) for all intermediate layers.
These modelling decisions are informed by our desire to: (1) solve K independent tasks at the end of training; (2) accelerate learning via transfer when possible; and (3) avoid catastrophic forgetting.
In contrast, we make no assumptions about the relationship between tasks, which may in practice be orthogonal or even adversarial.
In practice, we augment the progressive network layer of Equation 2 with non-linear lateral connections which we call adapters.
Deﬁning the vector of anterior features h(<k) i−1 ] of dimensionality n(<k) i−1 , in the case of dense layers, we replace the linear lateral connection with a single hidden layer MLP.
Before feeding the lateral activations into the MLP, we multiply them by a learned scalar, initialized by a random small value.
Omitting bias terms, we get: where V (k:j) reduction is performed via 1 × 1 convolutions [10].
We explored two related methods: an intuitive, but slow method based on a perturbation analysis, and a faster analytical method derived from the Fisher Information [2].
To evaluate the degree to which source columns contribute to the target task, we can inject Gaussian noise at isolated points in the architecture (e.g.
We ﬁnd that this method yields similar results to the faster Fisher-based method presented below.
We thus relegate details and results of the perturbation analysis to the appendix.
We can get a local approximation to the perturbation sensitivity by using the Fisher Information matrix [2].
While the Fisher matrix is typically computed with respect to the model parameters, we compute a modiﬁed diagonal Fisher ˆF of the network policy π with respect to the normalized activations 2 at each layer ˆh(k) .
For convolutional layers, we deﬁne ˆF to implicitly perform a summation over pixel locations.
We deﬁne the diagonal matrix ˆF , having elements ˆF (m, m), and the derived Average Fisher Sensitivity (AFS) of feature m in layer i of column k as: where the expectation is over the joint state-action distribution ρ(s, a) induced by the progressive network trained on the target task.
In this work we present an architecture for deep reinforcement learning that in sequential task regimes that enables learning without forgetting while supporting individual feature transfer from previous learned tasks.
The block-modular architecture of [21] has many similarities to our approach but focuses on a visual discrimination task.
We evaluate progressive networks across three different RL domains.
First, we consider synthetic versions of Pong, altered to have visual or control-level similarities.
Next, we experiment broadly with random sequences of Atari games and perform a feature-level transfer analysis.
Lastly, we demonstrate performance on a set of 3D maze games.
We rely on the Async Advantage Actor-Critic (A3C) framework introduced in [13].
We report results by averaging the top 3 out of 25 jobs, each having different seeds and random hyper-parameter sampling.
We present transfer score curves for selected source-target games, and summarize all such pairs in transfer matrices.
Models and baselines we consider are illustrated in  Figure 3: Illustration of different baselines and architectures.
The ﬁrst evaluation domain is a set of synthetic variants of the Atari game of Pong ("Pong Soup") where the visuals and gameplay have been altered, thus providing a setting where we can be conﬁdent that there are transferable aspects of the tasks.
As expected, we observe high positive transfer with baseline 3 (single column, full ﬁnetuning), a well established paradigm for transfer.
We use the metric derived in Sec.
We see that when switching from Pong to H-Flip, the network reuses the same components of low and mid-level vision (the outputs of the two convolutional layers; Figure 5a).
We next investigate feature transfer between randomly selected Atari games [3].
To this end we start by training single columns on three source games (Pong, River Raid, and Seaquest) 3 and assess if the learned features transfer to a different subset of randomly selected target games (Alien, Asterix, Boxing, Centipede, Gopher, Hero, James Bond, Krull, Robotank, Road Runner, Star Gunner, and Wizard of Wor).
We evaluate progressive networks with 2, 3 and 4 columns, 3Progressive columns having more than one “source” column are trained sequentially on these source games, i.e.
Across all games, we observe from Fig.
This is especially promising as we show in the Appendix that progressive network use a diminishing amount of capacity with each added column, pointing a clear path to online compression or pruning as a means to mitigate the growth in model size.
To differentiate these cases, we consider the Average Fisher Sensitivity for the 3 column case (e.g., see Fig.
At ﬁrst glance, this result appears unintuitive: if a progressive net ﬁnds a valuable feature set from a source task, shouldn’t we expect a high degree of transfer? We offer two hypotheses.
We observe less transfer on the Seek Track levels, which have dense reward items throughout the maze and are easily learned.
We believe that we are the ﬁrst to show positive transfer in deep RL agents within a continual learning framework.
Moreover, we have shown that the progressive approach is able to effectively exploit transfer for compatible source and task domains; that the approach is robust to harmful features learned in incompatible tasks; and that positive transfer increases with the number of columns, thus corroborating the constructive, rather than destructive, nature of the progressive architecture.
We explored two related methods for analysing transfer in progressive networks.
We describe the second method based on perturbation analysis in this appendix, as it proved too slow to use at scale.
Given its intuitive appeal however, we provide details of the method along with results on Pong Variants (see Section 5.2), as a means to corroborate the AFS score.
Our perturbation analysis aims to estimate which components of the source columns materially contribute to the performance of the ﬁnal column on the target tasks.
To this end, we injected Gaussian noise into each of the (post-ReLU) hidden representations, with a new sample on every forward pass, and calculated the average effect of these perturbations on the game score over 10 episodes.
We did this at a coarse scale, by adding noise across all features of a given layer, though a ﬁne scale analysis is also possible per feature (map).
In order to be invariant to any arbitrary scale factors in the network weights, we scale the noise variance proportional to the variance of the activations in each feature map and fully-connected neuron.
In the basic approach we pursue in the main text, the number of hidden units and feature maps grows linearly with the number of columns, and the number of parameters grows quadratically.
Here, we sought to determine the degree to which this full capacity is actually used by the network.
We leveraged the Average Fisher Sensitivity measure to study how increasing the number of columns in the Atari task set changes the need for additional resources.
In Figure 10a, we measure the average fractional use of existing feature maps in a given layer (here, layer 2).
We do this for each network by concatenating the per-feature-map AFS values from all source columns in this layer, sorting the values to produce a spectrum, and then averaging across networks.
We ﬁnd that as the number of columns increases, the average spectrum becomes sparser: the network relies on a smaller proportion of features from the source columns.
Similarly, in Figure 10b, we measure the capacity required in the ﬁnal added column as a function of the total number of columns.
Again, we measure the spectrum of AFS values in an example layer, but here from only the ﬁnal column.
In our grid we sample hyper-parameters from categorical distributions: • Learning rate was sampled from {10−3, 5 · 10−4, 10−4}.
• Strength of the entropy regularization from {10−2, 10−3, 10−4} • Gradient clipping cut-off from {20, 40} • scalar multiplier on the lateral feature is initialized randomly to one from {1, 10−1, 10−2} For the Atari experiments we used a model with 3 convolutional layers followed by a fully connected layer and from which we predict the policy and value function.
We use 16 workers and the same RMSProp algorithm without momentum or centring of the variance.
For this reason through out the paper we actually report results in terms of agent perceived steps rather than environment steps.
That is, the maximal number of agent perceived step that we do for any particular run is 4e7.
We plot learning curves for two column, three column and four column progressive networks alongside Baseline 3 (gray dashed line), a model pretrained on Seaquest and then ﬁnetuned on the particular target game and Baseline 1 (gray dotted line), where a single column is trained on the source game Seaquest.
We can see that overall baseline 3 performs well.
The levels we employed can be characterized as follows: • Seek Track 1: simple corridor with many apples • Seek Track 2: U-shaped corridor with many strawberries • Seek Track 3: Ω-shaped, with 90o turns, with few apples • Seek Track 4: Ω-shaped, with 45o turns, with few apples • Seek Avoid 1: large square room with apples and lemons • Seek Avoid 2: large square room with apples and mushrooms • Seek Maze M : M-shaped maze, with apples at dead-ends • Seek Maze Y : Y-shaped maze, with apples at dead-ends 0.04.0steps1e75001000150020002500scoreTarget game: alien0.04.0steps1e7200040006000800010000scoreTarget game: asterix0.04.0steps1e7020406080100scoreTarget game: boxing0.04.0steps1e720003000400050006000scoreTarget game: centipede0.04.0steps1e7100020003000400050006000scoreTarget game: gopher0.04.0steps1e750001000015000200002500030000scoreTarget game: hero0.04.0steps1e75001000150020002500scoreTarget game: jamesbond0.04.0steps1e7200040006000800010000scoreTarget game: krull0.04.0steps1e70500010000150002000025000300003500040000scoreTarget game: road runner0.04.0steps1e751015202530scoreTarget game: robotank0.04.0steps1e7500010000150002000025000300003500040000scoreTarget game: star gunner0.04.0steps1e71000200030004000500060007000scoreTarget game: wizard of worBaseline 1Baseline 3seaquest Prog