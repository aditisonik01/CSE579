In this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories.
Our framework learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for ﬁne-tuning to adapt to new class types.
We then deﬁne one-shot learning problems on vision (using Omniglot, ImageNet) and language tasks.
Our algorithm improves one-shot accuracy on ImageNet from 87.6% to 93.2% and from 88.0% to 93.8% on Omniglot compared to competing approaches.
We also demonstrate the usefulness of the same model on language modeling by introducing a one-shot task on the Penn Treebank.
a child can generalize the concept of “giraffe” from a single picture in a book – yet our best deep learning systems need hundreds or thousands of examples.
This motivates the setting we are interested in: “one-shot” learning, which consists of learning a class from a single labelled example.
This, in our view, is mostly due to the parametric aspect of the model, in which training examples need to be slowly learnt by the model into its parameters.
Previous work on metric learning in non-parametric setups [18] has been inﬂuential on our model, and we aim to incorporate the best characteristics from both parametric and non-parametric models – namely, rapid acquisition of new examples while providing excellent generalisation from common examples.
The novelty of our work is twofold: at the modeling level, and at the training procedure.
We propose Matching Nets (MN), a neural network which uses recent advances in attention and memory that enable rapid learning.
Secondly, our training procedure is based on a simple machine learning principle: test and train conditions must match.
Thus to train our network to do rapid learning, we Figure 1: Matching Networks architecture train it by showing only a few examples per class, switching the task from minibatch to minibatch, much like how it will be tested when presented with a few examples of a new task.
Besides our contributions in deﬁning a model and training criterion amenable for one-shot learning, we contribute by the deﬁnition of tasks that can be used to benchmark other approaches on both ImageNet and small scale language modeling.
We hope that our results will encourage others to work on this challenging problem.
We organized the paper by ﬁrst deﬁning and explaining our model whilst linking its several components to related work.
Then in the following section we brieﬂy elaborate on some of the related work to the task and our model.
In Section 4 we describe both our general setup and the experiments we performed, demonstrating strong results on one-shot learning on a variety of tasks and setups.
2 Model Our non-parametric approach to solving one-shot learning is based on two components which we describe in the following subsections.
First, our model architecture follows recent advances in neural networks augmented with memory (as discussed in Section 3).
Given a (small) support set S, our model deﬁnes a function cS (or classiﬁer) for each S, i.e.
Second, we employ a training strategy which is tailored for one-shot learning from the support set S.
We draw inspiration from models such as sequence to sequence (seq2seq) with attention [2], memory networks [29] and pointer networks [27].
Our contribution is to cast the problem of one-shot learning within the set-to-set framework [26].
More precisely, we wish to map from a (small) support set of k examples of image-label pairs S = {(xi, yi)}k i=1 to a classiﬁer cS(ˆx) which, given a test example ˆx, deﬁnes a probability distribution over outputs ˆy.
We deﬁne the mapping S → cS(ˆx) to be P (ˆy|ˆx, S) where P is parameterised by a neural network.
Thus, when given a new support set of examples S(cid:48) from which to one-shot learn, we simply use the parametric neural network deﬁned by P to make predictions about the appropriate label ˆy for each test example ˆx: P (ˆy|ˆx, S(cid:48)).
In general, our predicted output class for a given input unseen example ˆx and a support set S becomes arg maxy P (y|ˆx, S).
Our model in its simplest form computes ˆy as follows: where xi, yi are the samples and labels from the support set S = {(xi, yi)}k i=1, and a is an attention mechanism which we discuss below.
Where the attention mechanism is zero for the b furthest xi from ˆx according to some distance metric and an appropriate constant otherwise, then (1) is equivalent to ‘k − b’-nearest neighbours (although this requires an extension to the attention mechanism that we describe in Section 2.1.2).
In this case we can understand this as a particular kind of associative memory where, given an input, we “point” to the corresponding example in the support set, retrieving its label.
In our experiments we shall see examples where f and g are parameterised variously as deep convolutional networks for image tasks (as in VGG[22] or Inception[24]) or a simple form word embedding for language tasks (see Section 4).
We note that, though related to metric learning, the classiﬁer deﬁned by Equation 1 is discriminative.
However, the objective that we are trying to optimize is precisely aligned with multi-way, one-shot classiﬁcation, and thus we expect it to perform better than its counterparts.
The main novelty of our model lies in reinterpreting a well studied framework (neural networks with external memories) to do one-shot learning.
Despite the fact that the classiﬁcation strategy is fully conditioned on the whole support set through P (.|ˆx, S), the embeddings on which we apply the cosine similarity to “attend”, “point” or simply compute the nearest neighbor are myopic in the sense that each element xi gets embedded by g(xi) independently of other elements in the support set S.
Furthermore, S should be able to modify how we embed the test image ˆx through f.
We propose embedding the elements of the set through a function which takes as input the full set S in addition to xi, i.e.
This could be useful when some element xj is very close to xi, in which case it may be beneﬁcial to change the function with which we embed xi – some evidence of this is discussed in Section 4.
We use a bidirectional Long-Short Term Memory (LSTM) [8] to encode xi in the context of the support set S, considered as a sequence (see appendix for a more precise deﬁnition).
K is the ﬁxed number of unrolling steps of the LSTM, and g(S) is the set over which we attend, embedded with g.
f (ˆx, S) = attLSTM(f(cid:48)(ˆx), g(S), K) In the previous subsection we described Matching Networks which map a support set to a classiﬁcation function, S → c(ˆx).
We achieve this via a modiﬁcation of the set-to-set paradigm augmented with attention, with the resulting mapping being of the form Pθ(.|ˆx, S), noting that θ are the parameters of the model (i.e.
Our model has to perform well with support sets S(cid:48) which contain classes never seen during training.
Typically we consider T to uniformly weight all data sets of up to a few unique classes (e.g., 5), with a few examples per class (e.g., up to 5).
To form an “episode” to compute gradients and update our model, we ﬁrst sample L from T (e.g., L could be the label set {cats, dogs}).
We then use L to sample the support set S and a batch B (i.e., both S and B are labelled examples of cats and dogs).
Crucially, our model does not need any ﬁne tuning on the classes it has never seen due to its non-parametric nature.
Obviously, as T (cid:48) diverges far from the T from which we sampled to learn θ, the model will not work – we belabor this point further in Section 4.1.2.
Our work takes the metalearning paradigm of [21], where an LSTM learnt to learn quickly from data presented sequentially, but we treat the data as a set.
The one-shot learning task we deﬁned on the Penn Treebank [15] relates to evaluation techniques and models presented in [6], and we discuss this in Section 4.
The loss is very similar to ours, except we use the whole support set S instead of pair-wise comparisons which is more amenable to one-shot learning.
However, there is not much one-shot literature on ImageNet, which we hope to amend via our benchmark and task deﬁnitions in the following section.
In this section we describe the results of many experiments, comparing our Matching Networks model against strong baselines.
All of our experiments revolve around the same basic task: an N-way k-shot learning task.
We compared a number of alternative models, as baselines, to Matching Networks.
L(cid:48) denotes the held-out subset of labels which we only use for one-shot.
We ran one-shot experiments on three data sets: two image classiﬁcation sets (Omniglot [14] and ImageNet [19, ILSVRC-2012]) and one language modeling (Penn Treebank).
For vision problems, we considered four kinds of baselines: matching on raw pixels, matching on discriminative features from a state-of-the-art classiﬁer (Baseline Classiﬁer), MANN [21], and our reimplementation of the Convolutional Siamese Net [11].
We then took this network and used the features from the last layer (before the softmax) for nearest neighbour matching, a strategy commonly used in computer vision [3] which has achieved excellent results across many tasks.
We also tried further ﬁne tuning the features using only the support set S(cid:48) sampled from L(cid:48).
This yields massive overﬁtting, but given that our networks are highly regularized, can yield extra gains.
Our model makes far less mistakes than the Inception baseline.
Following [21], we augmented the data set with random rotations by multiples of 90 degrees and used 1200 characters for training, and the remaining character classes for evaluation.
We used a simple yet powerful CNN as the embedding function – consisting of a stack of modules, each of which is a 3 × 3 convolution with 64 ﬁlters followed by batch normalization [10], a Relu non-linearity and 2 × 2 max-pooling.
We resized all the images to 28 × 28 so that, when we stack 4 modules, the resulting feature map is 1 × 1 × 64, resulting in our embedding function f (x).
Results comparing the baselines to our model on Omniglot are shown in Table 1.
For both 1-shot and 5-shot, 5-way and 20-way, our model outperforms the baselines.
We note that the Baseline Classiﬁer improves a bit when ﬁne tuning on S(cid:48), and using cosine distance versus training a small softmax from the small training set (thus requiring ﬁne tuning) also performs well.
Siamese nets fare well versus our Matching Nets when using 5 examples per class, but their performance degrades rapidly in one-shot.
Like the authors in [11], we also test our method trained on Omniglot on a completely disjoint task – one-shot, 10 way MNIST classiﬁcation.
Our model achieves 72%.
Our experiments followed the same setup as Omniglot for testing, but we considered a rand and a dogs (harder) setup.
In the rand setup, we removed 118 labels at random from the training set, then tested only on these 118 classes (which we denote as Lrand).
For the dogs setup, we removed all classes in ImageNet descended from dogs (totalling 118) and trained on all non-dog classes, then tested on dog classes (Ldogs).
Thus, as well as using the full ImageNet data set, we devised a new data set – miniImageNet – consisting of 60, 000 colour images of size 84 × 84 with 100 classes, each having 600 examples.
We used 80 classes for training and tested on the remaining 20 classes.
In total, thus, we have randImageNet, dogsImageNet, and miniImageNet.
As we an see, FCE improves the performance of Matching Networks, with and without ﬁne tuning, typically improving performance by around two percentage points.
PIXELS INCEPTION CLASSIFIER MATCHING NETS (OURS) INCEPTION ORACLE Cosine Cosine Cosine (FCE) Softmax (Full) Y (Full) (cid:54)=Ldogs Lrand (cid:54)=Lrand Ldogs 41.4% 43.0% 42.0% 42.8% 59.8% 90.0% 87.6% 92.6% 93.2% 97.0% 58.8% 96.4% ≈ 99% ≈ 99% ≈ 99% ≈ 99% Next we turned to experiments based upon full size, full scale ImageNet.
Our baseline classiﬁer for this data set was Inception [25] trained to classify on all classes except those in the test set of classes (for randImageNet) or those concerning dogs (for dogsImageNet).
We also compared to features from an Inception Oracle classiﬁer trained on all classes in ImageNet, as an upper bound.
Our Baseline Classiﬁer is one of the strongest published ImageNet models at 79% top-1 accuracy on the standard ImageNet validation set.
Instead of training Matching Networks from scratch on these large tasks, we initialised their feature extractors f and g with the parameters from the Inception classiﬁer (pretrained on the appropriate subset of the data) and then further trained the resulting network on random 5-way 1-shot tasks from the training data set, incorporating Full Context Embeddings and our Matching Networks and training strategy.
However, on the much more challenging Ldogs subset, our model degrades by 1%.
We hypothesize this to the fact that the sampled set during training, S, comes from a random distribution of labels (from (cid:54)=Ldogs), whereas the testing support set S(cid:48) from Ldogs contains similar classes, more akin to ﬁne grained classiﬁcation.
Thus, we believe that if we adapted our training strategy to samples S from ﬁne grained sets of labels instead of sampling uniformly from the leafs of the ImageNet class tree, improvements could be attained.
We leave this as future work.
We also introduce a new one-shot language task which is analogous to those examined for images.
Here we show a single example, though note that the words on the right are not provided and the labels for the set are given as 1-hot-of-5 vectors.
we had a lot of people who threw in the <blank_token> today said <unk> ellis a partner in benjamin jacobson & sons a specialist in trading ual stock on the big board.
On each trial, we make sure that the set and batch are populated with sentences that are non-overlapping.
This means that we do not use words with very low frequency counts; e.g.
if there is only a single sentence for a given word we do not use this data since the sentence would need to be in both the set and the batch.
We used a batch size of 20 throughout the sentence matching task (SMT) and varied the set size across k=1,2,3.
We ensured that the same number of sentences were available for each class in the set.
We split the words into a randomly sampled 9000 for training and 1000 for testing.
As well, we trained and tested using sentences taken from the standard PTB training set and tested with sentences from the standard test set.
We compared our one-shot matching model to an oracle LSTM language model (LSTM-LM) [30] trained on all the words.
To do so, we examined a similar setup wherein a sentence was presented to the model with a single word ﬁlled in with 5 different possible words (including the correct answer).
In our sentence matching task the sentences provided in the set are randomly drawn from the PTB corpus and are related to the sentences in the query batch only by the fact that they share a word.
In this paper we introduced Matching Networks, a new neural architecture that, by way of its corresponding training regime, is capable of state-of-the-art performance on a variety of one-shot classiﬁcation tasks.
Further, we have deﬁned new one-shot tasks on ImageNet, a reduced version of ImageNet (for rapid experimentation), and a language modeling task.
An obvious drawback of our model is the fact that, as the support set S grows in size, the computation for each gradient update becomes more expensive.
Although there are sparse and sampling-based methods to alleviate this, much of our future efforts will concentrate around this limitation.
Further, as exempliﬁed in the ImageNet dogs subtask, when the label distribution has obvious biases (such as being ﬁne grained), our model suffers.
We feel this is an area with exciting challenges which we hope to keep improving in future work.
We would like to thank Nal Kalchbrenner for brainstorming around the design of the function g, and Sander Dieleman and Sergio Guadarrama for their help setting up ImageNet.
We would also like thank Simon Osindero for useful discussions around the tasks discussed in this paper, and Theophane Weber and Remi Munos for following some early developments.
Thanks also to Geoff Hinton and Alex Toshev for discussions about our results.
arXiv preprint In this section we fully specify the models which condition the embedding functions f and g on the whole support set S.
Much previous work has fully described similar mechanisms, which is why we left the precise details for this appendix.
We deﬁne K to be the number of “processing” steps following work from [26] from their “Process” block.
Since we do K steps of “reads”, attLSTM(f(cid:48)(ˆx), g(S), K) = hK where hk is as described in eq.
A.2 The Fully Conditional Embedding g In section 2.1.2 we described the encoding function for the elements in the support set S, g(xi, S), as a bidirectional LSTM.
Then we deﬁne g(xi, S) = (cid:126)hi + (cid:126)hi + g(cid:48)(xi) with: (cid:126)hi, (cid:126)ci = LSTM(g(cid:48)(xi), (cid:126)hi−1, (cid:126)ci−1) (cid:126)hi, (cid:126)ci = LSTM(g(cid:48)(xi), (cid:126)hi+1, (cid:126)ci+1) where, as in above, LSTM(x, h, c) follows the same LSTM implementation deﬁned in [23] with x the input, h the output (i.e., cell after the output gate), and c the cell.
3, we add a skip connection between input and outputs.
Here we deﬁne the two class splits used in our full ImageNet experiments – these classes were excluded for training during our one-shot experiments described in section 4.1.2.
n01498041, n01537544, n01580077, n01592084, n01632777, n01644373, n01665541, n01675722, n01688243, n01729977, n01775062, n01818515, n01843383, n01883070, n01950731, n02002724, n02013706, n02092339, n02093256, n02095314, n02097130, n02097298, n02098413, n02101388, n02106382, n02108089, n02110063, n02111129, n02111500, n02112350, n02115913, n02117135, n02120505, n02123045, n02125311, n02134084, n02167151, n02190166, n02206856, n02231487, n02256656, n02398521, n02480855, n02481823, n02490219, n02607072, n02666196, n02672831, n02704792, n02708093, n02814533, n02817516, n02840245, n02843684, n02870880, n02877765, n02966193, n03016953, n03017168, n03026506, n03047690, n03095699, n03134739, n03179701, n03255030, n03388183, n03394916, n03424325, n03467068, n03476684, n03483316, n03627232, n03658185, n03710193, n03721384, n03733131, n03785016, n03786901, n03792972, n03794056, n03832673, n03843555, n03877472, n03899768, n03930313, n03935335, n03954731, n03995372, n04004767, n04037443, n04065272, n04069434, n04090263, n04118538, n04120489, n04141975, n04152593, n04154565, n04204347, n04208210, n04209133, n04258138, n04311004, n04326547, n04367480, n04447861, n04483307, n04522168, n04548280, n04554684, n04597913, n04612504, n07695742, n07697313, n07697537, n07716906, n12998815, n13133613 n02085620, n02085782, n02085936, n02086079, n02086240, n02086646, n02086910, n02087046, n02087394, n02088094, n02088238, n02088364, n02088466, n02088632, n02089078, n02089867, n02089973, n02090379, n02090622, n02090721, n02091032, n02091134, n02091244, n02091467, n02091635, n02091831, n02092002, n02092339, n02093256, n02093428, n02093647, n02093754, n02093859, n02093991, n02094114, n02094258, n02094433, n02095314, n02095570, n02095889, n02096051, n02096177, n02096294, n02096437, n02096585, n02097047, n02097130, n02097209, n02097298, n02097474, n02097658, n02098105, n02098286, n02098413, n02099267, n02099429, n02099601, n02099712, n02099849, n02100236, n02100583, n02100735, n02100877, n02101006, n02101388, n02101556, n02102040, n02102177, n02102318, n02102480, n02102973, n02104029, n02104365, n02105056, n02105162, n02105251, n02105412, n02105505, n02105641, n02105855, n02106030, n02106166, n02106382, n02106550, n02106662, n02107142, n02107312, n02107574, n02107683, n02107908, n02108000, n02108089, n02108422, n02108551, n02108915, n02109047, n02109525, n02109961, n02110063, n02110185, n02110341, n02110627, n02110806, n02110958, n02111129, n02111277, n02111500, n02111889, n02112018, n02112137, n02112350, n02112706, n02113023, n02113186, n02113624, n02113712, n02113799, n02113978 Here we deﬁne the two class splits used in our PTB experiments – these classes were excluded for training during our one-shot language experiments described in section 4.1.3