We propose using progressive networks to bridge the reality gap and transfer learned policies from simulation to the real world.
We present an early demonstration of this approach with a number of experiments in the domain of robot manipulation that focus on bridging the reality gap.
Unlike other proposed approaches, our real-world experiments demonstrate successful task learning from raw visual input on a fully actuated robot manipulator.
While recent advances in simulation-driven deep RL are impressive [8, 19, 6, 12, 20, 15, 5], demonstrating learning capabilities on real robots remains the bar by which we must measure the practical applicability of these methods.
In this paper, we use progressive networks, a deep learning architecture that has recently been proposed for transfer learning, to demonstrate such an approach, thus providing a proof-of-concept pathway by which deep RL can be used to effect fast policy learning on a real robot.
From the LSTM state, using a linear layer, we compute a set of discrete action outputs that control the different degrees of freedom of the simulated robot as well as the value function.
Our initial ﬁndings show that the inductive bias imparted by the features and encoded policy of the simulation net is enough to give a dramatic learning speed-up on the real robot.
2 Transfer Learning from Simulation to Real Our approach relies on the progressive nets architecture, which enables transfer learning through lateral connections which connect each layer of previously learnt network columns to each new column, thus supporting rich compositionality of features.
We ﬁrst summarize progressive nets and then discuss its application for transfer in robot domains.
f is an element-wise non-linearity: we use f (x) = max(0, x) for all intermediate layers.
In contrast, we make no assumptions about the relationship between tasks, which may in practice be orthogonal or even adversarial.
For training in simulation, we use the Async Advantage Actor-Critic (A3C) framework introduced in [15].
We evaluate both feedforward and recurrent neural networks.
A standard-sized network is used for the simulation-trained column and a reduced-capacity network is used for the robot-trained columns, chosen because we found empirically that more capacity does not accelerate learning (see Section4.2), presumably because of the features reused from the previous column.
The MuJoCo physics simulator [22] is used to train the ﬁrst column for our experiments, with a rendered camera view to provide observations.
We veriﬁed this intuition by comparing wide and narrow network architectures, and found that the narrow network had slower learning and worse performance (see Figure 4).
We also see that the LSTM model out-performs the feedforward model by an average Figure 4: Learning curves are shown for wide and narrow versions of the feedforward (left) and recurrent (right) models, which are trained with the MuJoCo simulator.
h4(1)h2(1)h1(1)(cid:7528)1(cid:7528)2(cid:7528)9v...h3(1)h4(2)h2(2)h1(2)xt(cid:7528)1(cid:7528)2(cid:7528)9v...h3(2)LSTMconv2conv1fc01234Steps1e70510152025303540RewardsSimulation-trained first columnWide networkNarrow network01234Steps1e70510152025303540RewardsSimulation-trained first column (LSTM)Narrow networkWide networkFigure 5: Real robot training: We compare progressive, ﬁnetuning, and ‘from scratch’ learning curves.
We compare wide and narrow columns for both the progressive experiments and the randomly initialized baseline.
We calculate that learning this task, which trains to convergence in 24 hours using a CPU compute cluster, would take 53 days on the real robot even with continuous training for 24 hours a day.
In simulation, we explore learning rates and entropy costs, which are sampled uniformly at random on a log scale.
We train a baseline from scratch, a ﬁnetuned ﬁrst column, and a progressive second column.
We also try a randomly initialized wide network.
To assess this empirically, we start with a simulator-trained ﬁrst column, as described above, and then either ﬁnetune that column or add a narrow progressive column and 0100002000030000400005000060000Steps55152535RewardsReal-robot-trained progressive nets vs.
progressive approaches, we add color or perspective changes to the environment in simulation and then train 300 networks with different random seeds, learning rates, and entropy costs.
For each of these environment perturbations, we train 300 times with different seeds, learning rates, and entropy costs, which are the most sensitive hyperparameters.
As shown in Figure 6, we ﬁnd that progressive networks are more stable and reach higher ﬁnal performance than ﬁnetuning.
To demonstrate this, we train a second column on the reacher task but add proprioceptive features as an additional input, alongside the RGB images.
To evaluate this architecture, we train on a dynamic target task.
If the second column is instead trained on the static reacher task, and the third column is then trained on the conveyor task, we observe immediate transfer, and full performance is reached almost immediately (Figure 7, right).
We took full advantage of the ﬂexibility and computational scaling afforded by simulation and compared many hyperparameters and architectures for a random start, random target control task with visual input, then successfully transferred the skill to an agent training on the real robot.
We have described an initial set of experiments that prove that progressive nets can be used to achieve reliable, fast transfer for pixel-to-action RL policies.
Welling, C.
Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 1071–1079