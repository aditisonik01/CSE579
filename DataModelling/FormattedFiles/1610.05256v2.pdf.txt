In this paper, we measure the human error rate on the widely used NIST 2000 test set, and ﬁnd that our latest automated system has reached human parity.
In both cases, our automated system establishes a new state of the art, and edges past the human benchmark, achieving error rates of 5.8% and 11.0%, respectively.
The key to our system’s performance is the use of various convolutional and LSTM acoustic model architectures, combined with a novel spatial smoothing method and lattice-free MMI acoustic training, multiple recurrent neural network language modeling approaches, and a systematic use of system combination.
To better understand human performance, we have used professional transcribers to transcribe the actual test sets that we are working with, speciﬁcally the CallHome and Switchboard portions of the NIST eval 2000 test set.
We ﬁnd that the human error rates on these two parts are different almost by a factor of two, so a single number is inappropriate to cite.
We improve on our recently reported conversational speech recognition system [20] by about 0.4%, and now exceed human performance by a small margin.
Our progress is a result of the careful engineering and optimization of convolutional and recurrent neural networks.
The description of a novel spatial regularization method which signiﬁcantly boosts our LSTM acoustic model performance 3.
Expanded coverage of the Microsoft Cognitive Toolkit (CNTK) used to build our models 6.
The remainder of this paper describes our system in detail.
Section 2 describes our measurement of human performance.
Section 4 describes our implementation of i-vector adaptation.
Language model rescoring is a signiﬁcant part of our system, and described in Section 6.
We describe the CNTK toolkit that forms the basis of our neural network models in Section 7.
To measure human performance, we leveraged an existing pipeline in which Microsoft data is transcribed on a weekly basis.
One week, we added the NIST 2000 CTS evaluation data to the work-list, without further comment.
Aside from the standard twopass checking in place, we did not do a complex multi-party transcription and adjudication process.
Thus, it is the same condition as reported for our automated systems.
Error rates of 4.1 to 4.5% are reported for extremely careful multiple transcriptions, and 9.6% for “quick transcriptions.” While this is a different test set, the numbers are in line with our ﬁndings.
We note that the bulk of the Fisher training data, and the bulk of the data overall, was transcribed with the “quick transcription” guidelines.
Notably, the performance of our artiﬁcial system aligns almost exactly with the performance of people on both sets.
CONVOLUTIONAL AND LSTM NEURAL We use three CNN variants.
The only difference is that we apply Batch Normalization before computing ReLU activations.
We have found empirically that a suitable scale for this energy is 0.1 with respect to the existing cross entropy objective function.
Table 2 shows the beneﬁt in error rate for some of our early systems.
We observed error reductions of between 5 and 10% relative from spatial smoothing.
We also trained a fused model by combining a ResNet model and a VGG model at the senone posterior level.
The fused system is our best single system.
While our best performing models are convolutional, the use of long short-term memory networks (LSTMs) is a close second.
We use a bidirectional architecture [48] without frameskipping [29].
We found that using networks with more than six layers did not improve the word error rate on the development set, and chose 512 hidden units, per direction, per layer, as that provided a reasonable trade-off between training time and ﬁnal model accuracy.
Inspired by the human auditory cortex, where neighboring neurons tend to simultaneously activate, we employ a spatial smoothing technique to improve the accuracy of our LSTM models.
The smoothing is implemented as a regularization term on the activations between layers of the acoustic Speaker adaptive modeling in our system is based on conditioning the network on an i-vector [49] characterization of each speaker [50, 51].
For convolutional networks, this approach is inappropriate because we do not expect to see spatially contiguous patterns in the input.
Instead, for the CNNs, we add a learnable weight matrix W l to each layer, and add W lvs to the activation of the layer before the nonlinearity.
Note that the i-vectors are estimated using MFCC features; by using them subsequently in systems based on log-ﬁlterbank features, we may beneﬁt from a form of feature combination.
After standard cross-entropy training, we optimize the model parameters using the maximum mutual information (MMI) objective function.
We construct the denominator graph from this language model, and HMM transition probabilities as determined by transition-counting in the senone sequences found in the training data.
Our approach not only largely reduces the complexity of building up the language model but also provides very reliable training performance.
We have found it convenient to do the full computation, without pruning, in a series of matrix-vector operations on the GPU.
The underlying acceptor is represented with a sparse matrix, and we maintain a dense likelihood vector for each time frame.
As in [54], we use cross-entropy regularization.
In all the lattice-free MMI (LFMMI) experiments mentioned below we use a trigram language model.
In our implementation, we use a mixed-history acoustic unit language model.
We found this model to perform better than either purely word-based or phone-based models.
Based on a set of initial experiments, we developed the following procedure: An initial decoding is done with a WFST decoder, using the architecture described in [55].
We use an N-gram language model trained and pruned with the SRILM toolkit [56].
We have experimented with both RNN LMs and LSTM LMs, and describe the details in the following two sections.
Our RNN-LMs are trained and evaluated using the CUEDRNNLM toolkit [58].
Our RNN-LM conﬁguration has several distinctive features, as described below.
We trained both standard, forward-predicting RNNLMs and backward RNN-LMs that predict words in Table 3.
In addition, we trained a second RNN-LM for each direction, obtained by starting with different random initial weights.
In order to make use of LM training data that is not fully matched to the target conversational speech domain, we start RNN-LM training with the union of in-domain (here, CTS) and out-of-domain (e.g., Web) data.
We found best results with an RNN-LM conﬁguration that had a second, non-recurrent hidden layer.
While the RNN-LM estimates a probability for unknown words, we take a different approach in rescoring: The number of out-of-set words is recorded for each hypothesis and a penalty for them is estimated for them when optimizing the relative weights for all model scores (acoustic, LM, pronunciation), using the SRILM nbest-optimize tool.
After obtaining good results with RNN-LMs we also explored the LSTM recurrent network architecture for language modeling, inspired by recent work showing gains over RNN-LMs for conversational speech recognition [37].
In addition to applying the lessons learned from our RNN-LM experiments, we explored additional alternatives, as described below.
There are two types of input vectors our LSTM LMs take, word based one-hot vector input and letter trigram vector [60] input.
Including both forward and backward models, we trained four different LSTM LMs in total.
For the word based input, we leveraged the approach from [61] to tie the input embedding and output embedding together.
Here we also used a two-phase training schedule to train the LSTM LMs.
First we train the model on the combination of in-domain and out-domain data for four data passes without any learning rate adjustment.
We then start from the resulting model and train on in-domain data until convergence.
We tried applying dropout on both types of language models but didn’t see an improvement.
Based on this, we proceeded with three hidden layers, with 1000 hidden units each.
The perplexities of each LSTM-LM we used in the ﬁnal combination (before interpolating with the N-gram model) can be found in Table 5.
For the ﬁnal system, we interpolated two LSTM-LMs with an N-gram LM for the forward-direction LM, and similarly for the backward-direction LM.
To this we added 62M words of UW Web data as out-of-domain data, for use in the two-phase training procedure described above.
Instead, we perform a greedy search, starting with the single best system, and successively adding additional systems, to ﬁnd a small set of systems that are maximally complementary.
We experimented with two search algorithms to ﬁnd good subsets of systems.
We always start with the system giving the best individual accuracy on the development set.
If no improvement is found with any of the unused systems, we try adding each with successively lower relative weights of 0.5, 0.2, and 0.1, and stop if none of these give an improvement.
To avoid overﬁtting, the weights for an N-way combination are smoothed hilattices and post-processing them, we consider LFMMI to be a signiﬁcant advance in technology.
The ﬁnal system incorporated a variety of BLSTM models with roughly similar performance, but differing in various metaparameters (number of senones, use of spatial smoothing, and choice of pronunciation dictionaries).2 To further limit the number of free parameters to be estimated in system combination, we performed system selection in two stages.
First, we selected the four best BLSTM systems.
We then combined these with equal weights and treated them as a single subsystem in searching for a larger combination including other acoustic models.
This yielded our best overall combined system, as reported in Section 8.3.
The resulting fast experimental turnaround using the full 2000hour corpus was critical for our work.
CNTK made training times feasible by parallelizing the SGD training with our 1-bit SGD parallelization technique [66].
In [66], we showed that gradient values can be quantized to just a single bit, if one carries over the quantization error from one minibatch to the next.
Table 7 compares the runtimes, as multiples of speech duration, of various processing steps associated with the different acoustic model architectures (ﬁgures for DNNs are given only as a reference point, since they are not used in our system).
We observe that the GPU gives a 10 to 100-fold speedup for AM evaluation over the CPU implementation.
We train with the commonly used English CTS (Switchboard and Fisher) corpora.
The bulk of our models use three state left-to-right triphone models with 9000 tied states.
Additionally, we have trained several models with 27k tied states.
We use a 30k-vocabulary derived from the most common words in the Switchboard and Fisher corpora.
Table 3 shows the result of i-vector adaptation and LFMMI training on several of our early systems.
We achieve a 5–8% relative improvement from i-vectors, including on CNN systems.
We see a consistent 7–10% further relative reduction in error rate for all models.
Overall Results and Discussion The performance of all our component models is shown in Table 8, along with the BLSTM combination and full system combination results.
(Recall that the four best BLSTM systems are combined with equal weights ﬁrst, as described in Section 6.5.) Key benchmarks from the literature, our own best results, and the measured human error rates are compared in Table 9.4 All models listed in Table 8 are selected for the combined systems for one or more of the three rescoring LMs.
While this yields our single best acoustic model, only the individual VGG and ResNet models are used in the overall system combination.
We also observe that the four model variants chosen for the combined BLSTM subsystem differ incrementally by one hyperparameter (smooththe word “hmm.” The scoring guidelines treat “hmm” as a word distinct from backchannels and hesitations, so this is not a scoring mistake.
For both people and our automated system, the insertion and deletion patterns are similar: short function words are by far the most frequent errors.
In particular, the single most common error made by the transcribers was to omit the word “I.” While we believe further improvement in function and content words is possible, the signiﬁcance of the remaining backchannel/hesitation confusions is unclear.
We see that the human transcribers have a somewhat lower substitution rate, and a higher deletion rate.
We also assessed the lower bound of performance for our lattice/Nbest rescoring paradigm.
With oracle error rates less than half the currently achieved actual error rates, we conclude that search errors are not a major limiting factor to even better accuracy.
In this section, we compare the errors made by our artiﬁcial recognizer with those made by human transcribers.
We ﬁnd that the machine errors are substantially the same as human ones, with one large exception: confusions between backchannel words and hesitations.
Focusing on the substitutions, we see that by far the most common error in the ASR system is the confusion of a hesitation in the reference for a backchannel in the hypothesis.
We speculate that this is due to the nature of the Fisher training corpus, where the “quick transcription” guidelines were predominately used [41].
We ﬁnd that there is inconsistent treatment of backchannel and hesitation in the resulting data; the relatively poor performance of the automatic system here might simply be due to confusions in the training data annotations.
The most frequent substitution for people on the Switchboard corpus was mistaking a hesitation in the reference for 5The NIST scoring protocol treats hesitation words as optional, and we therefore delete them from our recognizer output prior to scoring.
This explains why we see many substitutions of backchannels for hesitations, but not vice-versa.
RELATION TO PRIOR WORK Compared to earlier applications of CNNs to speech recognition [67, 68], our networks are much deeper, and use linear bypass connections across convolutional layers.
We improve on these architectures with the LACE model [46], which iteratively expands the effective window size, layer-by-layer, and adds an attention mask to differentially weight distant context.
Our spatial regularization technique is similar in spirit to stimulated deep neural networks [69].
Whereas stimulated networks use a supervision signal to encourage locality of activations in the model, our technique is automatic.
Our use of lattice-free MMI is distinctive, and extends previous work [12, 54] by proposing the use of a mixed triphone/phoneme history in the language model.
On the language modeling side, we achieve a performance boost by combining multiple LSTM-LMs in both forward and backward directions, and by using a two-phase training regimen to get best results from out-of-domain data.
For our best CNN system, LSTM-LM rescoring yields a relative word error reduction of 23%, and a 20% relative gain for the combined recognition system, considerably larger than previously reported for conversational speech recognition [37].
We have measured the human error rate on NIST’s 2000 conversational telephone speech recognition task.
We ﬁnd that there is a great deal of variability between the Switchboard and CallHome subsets, with 5.8% and 11.0% error rates respectively.
For the ﬁrst time, we report automatic recognition performance on par with human performance on this task.
ASR 44: i 33: it 29: a 29: and 25: is 19: he 18: are 17: oh 17: that 17: the CH Human 73: i 59: and 48: it 47: is 45: the 41: %bcack 37: a 33: you 31: oh 30: that ASR 31: it 26: i 19: a 17: that 15: you 13: and 12: have 12: oh 11: are 11: is Human 34: i 30: and 29: it 22: a 22: that 22: you 17: the 17: to 15: oh 15: yeah ASR 15: a 15: is 11: i 11: the 11: you 9: it 7: oh 6: and 6: in 6: know 4: they Human 10: i 9: and 8: a 8: that 8: the 7: have 5: you 4: are 4: is ASR 19: i 9: and 7: of 6: do 6: is 5: but 5: yeah 4: air 4: in 4: you Human 12: i 11: and 9: you 8: is 6: they 5: do 5: have 5: it 5: yeah 4: a Our system’s performance can be attributed to the systematic use of LSTMs for both acoustic and language modeling, as well as CNNs in the acoustic model, and extensive combination of complementary system for both acoustic and language modeling.
We thank Arul Menezes for access to the Microsoft transcription pipeline; Chris Basoglu, Amit Agarwal and Marko Radmilac for their invaluable assistance with CNTK; Jinyu Li and Partha Parthasarathy for many helpful conversations.
We also thank X.
Weng, D