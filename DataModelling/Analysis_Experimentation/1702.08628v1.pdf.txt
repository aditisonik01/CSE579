7
1
0
2

 

b
e
F
8
2

 

 
 
]

G
L
.
s
c
[
 
 

1
v
8
2
6
8
0

.

2
0
7
1
:
v
i
X
r
a

SUBMITTEDTOTHEIEEETRANSACTIONSONCOMPUTATIONALINTELLIGENCEANDARTIFICIALINTELLIGENCEINGAMES1AnalysisofAgentExpertiseinMs.Pac-ManusingValue-of-Information-basedPoliciesIsaacJ.Sledge,StudentMember,IEEEandJoséC.Príncipe,LifeFellow,IEEEAbstract—ConventionalreinforcementlearningmethodsforMarkovdecisionprocessesrelyonweakly-guided,stochasticsearchestodrivethelearningprocess.Itcanthereforebediﬃculttopredictwhatagentbehaviorsmightemerge.Inthispaper,weconsideraninformation-theoreticapproachforper-formingconstrainedstochasticsearchesthatpromotetheformationofrisk-aversetorisk-favoringbehaviors.Ourapproachisbasedonthevalueofinformation,acriterionthatprovidesanoptimaltrade-oﬀbetweentheexpectedreturnofapolicyandthepolicy’scomplexity.Asthepolicycomplexityisreduced,thereisahighchancethattheagentswilleschewriskyactionsthatincreasethelong-termrewards.Theagentsin-steadfocusonsimplycompletingtheirmainobjectiveinanexpeditiousfashion.Asthepolicycomplexityincreases,theagentswilltakeactions,regardlessoftherisk,thatseektodecreasethelong-termcosts.Aminimal-costpolicyissoughtineithercase;theobtainablecostdependsonasingle,tunableparameterthatregulatesthedegreeofpolicycomplexity.Weevaluatetheperformanceofvalue-of-information-basedpoliciesonastochasticversionofMs.Pac-Man.Amajorcomponentofthispaperisdemonstratingthatrangesofpolicycomplexityvaluesyielddiﬀerentgame-playstylesandanalyzingwhythisoccurs.Weshowthatlow-complexitypoliciesaimtoonlycleartheenvironmentofpelletswhileavoidinginvulnerableghosts.Higher-complexitypoliciesimplementmulti-modalstrategiesthatcompeltheagenttoseekpower-upsandchaseaftervulnerableghosts,bothofwhichreducethelong-termcosts.IndexTerms—Valueofinformation,constrainedsearch,reinforcementlearning,informationtheoryThisworkhasbeensubmittedtotheIEEEforpossiblepublication.Copyrightmaybetransferredwithoutnotice,afterwhichthisversionmaynolongerbeaccessible.IsaacJ.SledgeiswiththeDepartmentofElectricalandComputerEngineering,UniversityofFlorida,Gainesville,FL32611,USA(email:isledge@cnel.uﬂ.edu).HeisalsowiththeComputationalNeuroEngineeringLaboratory(CNEL)attheUniversityofFlorida.JoséC.PríncipeistheEckisChairwithboththeDepartmentofElectricalandComputerEngineeringandtheDepartmentofBiomedicalEngineering,UniversityofFlorida,Gainesville,FL32611,USA(email:principe@cnel.uﬂ.edu).HeisthedirectoroftheComputationalNeuroEngineeringLaboratory(CNEL)attheUniversityofFlorida.TheworkoftheauthorswasfundedviagrantN00014-15-1-2103fromtheUSOﬃceofNavalResearch.TheﬁrstauthorwasadditionallyfundedbyaUniversityofFloridaResearchFellowship,aRobertC.PittmanResearchFellowship,andaNavalResearchEnterpriseFellowship.SUBMITTEDTOTHEIEEETRANSACTIONSONCOMPUTATIONALINTELLIGENCEANDARTIFICIALINTELLIGENCEINGAMES21IntroductionLearningtosuccessfullyplayagameistheprocessofdiscoveringthecorrectactionstoachievebothprimaryandsecondarygoals.Playerstypicallydothisinatrial-and-error-basedmanner:bytakingrisks,intheformoftryingnovelactionsinparticulargamestates,theybegintoconnectwhentotakecertainactions.Eventually,givenadiverserangeofscenarios,playerscanrelatesequencesofactionsandgamestateswiththeabilitytocompletevarioustasksorobtainaminimumscore.Theycanalsoreﬁnetheactionsequencesoncethisoccurs.Thereareavarietyofmachine-learningparadigmsthatcanmimicthisprocessofuncoveringandreﬁningstrate-giestocompleteobjectives.Oneofthemoreprominentexamplesisreinforcementlearning[1].Reinforcementlearn-ingcanbeappliedtosequentialdecision-makingproblems,suchasgames,inwhichtheoutcomeofanactionmightnotbeimmediatelyapparent.Reinforcementlearningmethodsdothisbyassigningcredittoactionsleadingtoanoutcome,whichprovidesfeedbackintheformofexpectedcoststoperformthoseactions.Theexpectedcostscanthenbemanipulatedtoinferamappingbetweenenvironmentalstatesandactions.Agreatamountofreinforcementlearningresearchhasbeenconductedsincetheﬁeld’sinception[2].Despitetheseeﬀorts,therearesomeshortcomingswithmanyconventionalapproaches.Forinstance,itcanbediﬃculttopredicthowagentsmaycompletetheirobjectives.Therearefewdirectwaystoguidetheselectionofrisk-prone,-neutral,or-aversebehaviors,asthelearningprocessistypicallydrivenbystochasticsearch.Engagingincertaintypesofriskybehaviors,atacertainfrequency,maybenecessaryforagentstouncoverhigh-performingpolicies.Aswell,itcanbetroublesometoconstructpoliciesthatswitchbetweenmulti-objectivestrategiesduringanepisode.Thisiscanoccurbecauseinstanceswherediﬀerentstrategiesareviablemaybebriefandrarelyhappen.Iftheagentavoidstakingseeminglynon-optimalactionsinsuchinstances,thenitmayseldomlydeviatefromitscurrentpolicytodetermineifanewstrategywouldbeappropriateunderthegivencircumstances.BothoftheseissuescanbeillustratedwiththearcadegameMs.Pac-Man.Ms.Pac-Manisapredator-preysce-nariosetinamaze-likeenvironment.Thegamehascompetingobjectives.Throughoutamajorityofthegame,theagent,Ms.Pac-Man,isbeingchasedbyinvulnerableentities,whichareghosts.Theobjectiveoftheagentistocleartheenvironmentofpelletswhilenavigatingaroundtheghosts.However,afteractivatingcertainpower-ups,theghostsbecomevulnerableforabriefperiodoftime.Theswitchingamedynamicsnecessitatesachangeintheplaystrat-egy:multipledistinctmodesofbehaviorarerequiredunderdiﬀerentconditions.Despitetheneedformulti-modalbehavior,mostlearningapproacheshavefocusedonlearningmonolithicpoliciesthatguidetheagentregardlessofthevulnerabilityoftheghosts.Althoughitispossibletorepresentmulti-modalbehaviorwithsuchpolicies,itcanbediﬃculttolearnsuchbehavior.Thisis,inpart,duetorisk.Forinstance,throughoutthelearningprocess,anagentmayhavelearnedtoavoidcollidingwiththeghosts.Withoutstrayingfromthisbehavior,theagentwillnotlearnthatthereareinstanceswhereitcansafelychasetheghosts.Inthispaper,weconsideraninformation-theoreticlearningapproachforperformingconstrainedstochasticsearchesthatcanpromoteacontinuumofrisk-aversetorisk-favoringagentbehaviorsduringreinforcementlearning.OurapproachisbaseduponStratonovich’svalueofinformationcriterion[3–5],whichprovidesanoptimalconver-sionbetweencostsandinformation.Thedetailsandtheoreticalpropertiesoftheapplicationofthemethodologywillbeexplainedinacompanionpaper.Here,thegoalistoanalyzetheevolutionoftheagentexpertise,forthearcadegameMs.Pac-Man,whenahyperparameterofthemethodisvariedduringlearning,aswillbeexplainednext.Ourimplementationofthevalueofinformationtodrivethereinforcementlearningprocessisgovernedbyasin-glehyperparameterthatspeciﬁesthepolicycomplexity.Thishyperparameterarisesautomaticallyfromoptimizationofthevalueofinformation.Apolicy’sdegreesoffreedomdirectlyinﬂuencestheexplorationgranularityofthepolicysearchspace.This,inturn,inﬂuencestheagentdynamicsthatareuncovered.Ifthecomplexityislow,thenthepolicyspacewillbesearchedcoarsely.Theconstructedpolicieswillimplementcautiousbehaviors.Thatis,theagentwillavoidactionsthatmightreducetheoverallcostsyetdonotassistincompletingitsobjectives.Thisstrategyisalsowitnessedduringlearning,whichcanleadtohightomoderatelong-termcosts.Theamountofcomputationaleﬀortneededtoﬁndsuchpoliciesislow.Whenthepolicycomplexityishigh,thepolicyspaceisﬁnelysearched.Theagentispronetotakingriskyactionsinthiscase.Indoingso,thepoliciestypicallyproduceactionsequencesthatnotonlyﬁnishthedesiredtasks,butalsoleadtolowlong-termcosts.Suchhigh-complexitypoliciesarealsolikelytopromotetheformationofmulti-modalbehaviors.Ineithercase,optimal-costpoliciesarefound,wheretheactualbestcostsarepre-determinedbythepolicycomplexity.Thiscomesattheexpenseofcomputation,asmanylearningepisodesmaybeneededforthistooccur.Weshowthatdistinctpolicytypes,forMs.Pac-Man,materializeforthreerangesofparametervalues.Theagentmerelyclearsthelevelofpelletsandavoidstheghostsinthelowest-complexitycase.Itdoesnotimplementmulti-modalbehaviors.Asthecomplexityisincreased,theagentexhibitsimprovedgame-playstyles,whichcomeaboutfromengaginginriskybehaviorsduringtraining.Formoderatelylowtomoderatelyhighcomplexityvalues,thepoliciescompeltheagenttoclearthepelletsinanexpeditiousmanner.Italsohastheagentschaseafterfruitpower-ups.Inthelattercase,theagentswitchesbetweenavoidingandsometimeschasingtheghostswhenevertheyareSUBMITTEDTOTHEIEEETRANSACTIONSONCOMPUTATIONALINTELLIGENCEANDARTIFICIALINTELLIGENCEINGAMES3invulnerableandvulnerable,respectively.High-complexitypoliciesattempttoproducethebestcosts.Agentswillcontinuouslyseekoutpowerpellets,soastoremaininvulnerablewhileclearingthelevelandpursuingnearbyghosts.Theagentswillalsolureoneormoreghoststopowerpellets.Theremainderofthispaperisorganizedasfollows.Webegin,insection2,withasurveyoftheresearchthathasbeenperformedusingPac-ManandMs.Pac-Man.Section3outlinesourmethodology.WeﬁrstprovideanoverviewofreinforcementlearningusingMarkovdecisionprocesses.WethenintroducethevalueofinformationfromaMarkov-decision-processesperspective.Risk-basedinterpretationsofthiscriterionareprovided.Wealsodiscussimplementationalaspects.OurreinforcementlearningsimulationsoftheMs.Pac-Mangamearegiveninsec-tion4.Webeginthissectionbycoveringourexperimentalprotocols.Wethentransitiontooursimulationresultsandcorrespondinganalyses.Theseanalysesincludethequalitativeandquantitativeimprovementintheagent’sbehaviorsduringtrainingprocess.Lastly,weconcludeinsection5andlistpossibleavenuesoffutureresearch.2LiteratureReviewPac-MananditssequelMs.Pac-Manareamongthemostpopularvideogames.Theyfeaturegameplaythatissimpleyetrequirescomplicatedstrategiesforsuccess.OfparticularinteresttoresearchershasbeenthegameMs.Pac-Man.Inthisgame,dynamicobstacles,whicharetheghosts,moveinastochasticfashion,whichemphasizestheformationofgameplaystrategiesthatworkwellinmanysituations.Pac-Manreliesondeterministicghostmove-ments,whichgreatlysimpliﬁesthedevelopmentofagentgameplaybehaviors.ManyapproachesdevelopedtoplayeitherPac-ManorMs.Pac-Manrelyoncomputationalintelligenceprinci-ples[6–13].TheworkofLucas[6]isoneoftheearliestexamples.HeusedevolutionaryheuristicstotrainneuralnetworkstoplayMs.Pac-Man.Hisapproachreliedonthecalculationoffeaturesrelevantforgameplay,suchasthedistancestonotableitemsandghosts.Wittkampetal.[7]alsoevolvedneuralnetworks,whichwereappliedtolearnghostmovementstrategiesforPac-Man.Theirapproachwasbasedontheneuro-evolutionofaugmentingtopologiesparadigm.Usingahand-codedagenttocontrolPac-Man,theyuncoveredghoststrategiesthatcanoutperformthosefromtheoriginalgame.Itwasshownthatthemetricsusedforﬁtnessevaluationcansigniﬁcantlyaﬀecttheﬁnalghostbehaviours.Diﬀerentevaluationfunctionscanproduceghoststrategiesthatoutperformthoseoftheoriginalgameintermsofeithertheﬁrst-levelscoreorthenumberofliveslostduringtheﬁrstlevel.Evolutionary-inspiredoptimizationtacticshavealsoappearedinotherworks.GallagherandRyan[8]developedaﬁnite-statemachineapproachtoencodemovementstrategiesforaPac-Managents.Themovementstrategieswereevolvedaccordingtoapopulation-basedincrementallearningalgorithm.TheyconsideredasimpliﬁedversionofPac-Man,whichcontainedonlyasingleghostandnopowerpills;thissigniﬁcantlyreducedthecomplexityofthegame.In[12],GallagherandLedwichdescribedanapproachtodevelopingPac-Managentsthatlearnthegamefromminimalon-screeninformation.Theagentswerecreatedonevolvingneuralnetworkcontrollersusingstraightforwardevolutionaryalgorithms.Theirresultsshowedthatneuro-evolutionisabletoproduceagentsthatexhibitnovice-levelplayingability,despitenothavingknowledgeofthegamerules.Noagentscoreswerementionedineitherpaper,though,makingcomparisonstorecentworkschallenging.AlhejaliandLucas[10]usedgeneticprogrammingtoconstructavarietyofPac-Managents.Theirresultsdemonstratedthattheagentbehaviorsfunctionedwellforthetrainingenvironmentandcouldgeneralizedwelltonovelenvironments.Theaverageandmaximumscoresoftheiragentswereapproximately10000and20000points,respectively.BrandstetterandAhmadi[11]alsoreliedongeneticprogramming,whichwasusedasareactivecontrolmechanismforaMs.Pac-Managent.Theyreportedameanscoreofover19000pointsandabestscoreofabout33000points.Therehavebeenavarietyofnon-computational-intelligence-,heuristic-basedapproachesthathavebeendevel-oped[14–16].Forinstance,SzitaandLorinczproposedsimplerule-basedpoliciesforcontrollingaMs.Pac-Managent.Theirruleswereorganizedintodistinctactionmodulesandthedecisionaboutwhichdirectiontomoveisbasedontheprioritiesassociatedwitheachactionmodule.Policiesforchoosingthebestactionmoduleswereuncoveredusingcross-entropyoptimization.Theyreportedanaveragescoreof8100points,whichiscomparabletohumansubjectswhoplayedthesameversionofthegame.Morerecently,WirthandGallagher[15]outlinedhowtoutilizeinﬂuencemapmodelswhenplayingMs.Pac-Man.Theirmodelshighlightdesirableandundesirablelocationsfortheagentstovisitinthegame.Averageandmaximumscoresof6800and19000points,respectively,werereported.Thisperformanceisroughlyonparwithnoviceplayers,whooftenreachbetween15000and20000points.ArtiﬁcialintelligencetechniqueshavealsobeenutilizedtocreatePac-Managents.Reinforcementlearningisonesuchexample[17–22].In[17],BurrowandLucascomparedtheperformanceoftemporal-diﬀerencelearningandevolutionaryalgorithms.Twosimplefeatureswereusedtoestimatethevalueofmovingtoacandidatenodeintheenvironment.Theseincludedthedistancefromacandidatenodetothenearestescapenodeandthedistancefromacandidatenodetothenearestpillalongtheshortestpath.Theirexperimentsdemonstratedthattheevolutionaryapproachoutperformedreinforcementlearninginthislimitedfeature-setcase.In[19],Griﬃthsetal.addressedtheproblemofintegratinghumanfeedbackwithreinforcementlearningusingBayesianmodels.Theytestedtheirap-proachonasmall,non-traditionalPac-Mandomain.Vezhnevetsetal.[20]applieddeepneuralnetworksforlearningSUBMITTEDTOTHEIEEETRANSACTIONSONCOMPUTATIONALINTELLIGENCEANDARTIFICIALINTELLIGENCEINGAMES4multi-stepactionstrategiesinareinforcementlearningsetting.Theirapproachachievedanaveragescoreofapproxi-mately6600pointsafteronlyafewtrainingepisodes.Monte-Carlotreesearchhavealsogrowninpopularityoverthepastfewyears[23–26].RoblesandLucas[23]appliedasimpletreesearchheuristiconaMs.Pac-Managenttoevaluatethedangerofanyparticularcourseofaction.Theiragentswereabletoscoreabout15000pointswhenprocessingscreencapturesofthegame.WhenappliedtoasimulatedversionofMs.Pac-Man,inwhichfeaturescouldbemorereliablyandexpedientlyestimated,theiragentsscoredover41000points.IkehataandIto[24]appliedupperconﬁdenceboundstoMonteCarlosearchtrees,whichconsiderspotentialmovesandformulatesanaveragerewardofactionactionsequence.LiketheworkofRoblesandLucas,thatofIkehataandItoevaluatesdangerousroutesfortheagenttotake,baseduponitslocationintheenvironmentandanynearbyghosts.Theiragentsscoredover58000points.Morerecently,Samothrakisetal.[25]andPepelsetal.[26]appliedMonteCarlotreesearchtocreatehigh-performingagents,whichobtainedaveragescoresofaround81000and87000points,respectively.Foderaroetal.[27,28]reliedontreesearchesaswell.Theyﬁrstdecomposetheenvironmentintoaseriesofconvexcells,whichoutlinedlocationswheretheagentcouldtravel.Thesetofdecomposedcellsisusedtocreateadecisiontreeforselectinganoptimalpaththatminimizestheriskofcollidingwiththeghosts.Averageandbestscores,respectively,of23000and44600pointswerereported.OurobjectiveinthispaperisnottosolelydevelopanintelligentagentforMs.Pac-Man.Rather,weseektodeﬁneaprincipledapproachofselectingactionsduringreinforcementlearning.Wewantthisactionselectiontobedrivenbyacontrollablenotionofrisk.Suchanapproachcanbeutilizedforavarietyofproblems.WhenappliedtoMs.Pac-Man,however,weobtainpromisingresultsafterjustafewthousandtrainingepisodes.Agentsthatobeylow-,moderate-andhigh-riskaction-selectionstrategiesareabletoachieve20700,37700,and55900points,respectively,onaverage,whichcorrespondtothescoringcapabilitiesofsemi-experiencedplayers.3Methodology3.1MarkovDecisionProcessPoliciesTheMarkovdecisionprocessframeworkisusedheavilyinthetheoryofstochasticdecision-makingfordiscrete-spaceanddiscrete-timesettings[29].InaMarkovdecisionprocess,thesystembeingconsideredisassumedtobedrivenbyunderlyingMarkovchains.Thatis,thesystemjumpsrandomlyfromonestatest∈Stothenextst+1∈Sfordiscretetimestepst.Theprobabilityoftransitioningfromonestatetoanotherdependsonlyonthecurrentstateandnotonthehistoryofstates.Moreover,inaMarkovdecisionprocess,theagentisrequiredtochooseanactionat∈Afromasetofavailableofactions.Animmediaterewardgt+1∈R,whichcanbeeither(non-)positiveor(non-)negative,isearnedduringthetransitionbetweenstatesstandst+1wheneveranactionatistaken.Acorrespondingpolicydictatestheactionthatisselectedateachstate.Apolicyisamappingfromthesetofstatestothesetofpossibleactionsπs,a:S→A.Policiescanhaveaprobabilisticinterpretation,whichimpliesthattheycharacterizetheprobabilitydistributionoveractionsgiventhestate.Eachpolicyhasanassociatedaction-valuefunctionq(s,πs,a):S×A→R.Thegoalofareinforcementlearningagentistoﬁndapolicythatoptimizesthevaluefunctionforallstate-actionpairs:π∗s,a=arginfπs,a:S→AE[q(s,πs,a)].Here,thevaluefunctionisdeﬁnedintermsofthediscountedfuturecostsassociatedwithaparticularsequenceofactionsq(st,πst,at)=Eπs,a ∞Xt=0γtgt+1(st,at)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)st=s,at=a!,(3.1)forsomediscountfactorγintheunitinterval.Thediscountingfactorpenalizesrewardsreceivedinthefuture,withanincreasingpenaltyforthosetimesfurtherintothefuture.Theproblemofﬁndingthebestpolicycanbewrittenintermsofthecostfunctionasinfπst,at:S→A Xst∈SXat∈Apr(st)pr(at|st)q(st,πst,at(st))!=Xst∈Spr(st)infat∈A Xst∈Spr(st|at)q(st,at)!.(3.2)SuchapolicyisnothingmorethantheBayesriskπ∗st,at=E[infat∈AE[g(st,at)|st]].Thispolicymaybestochasticordeterministic.Ifthepolicyisstochastic,thenitisamappingfromthestatestoadistributionovertheactions.Ineithercase,theMarkovdecisionprocessisconsideredtobesolvedwheneverthepolicyminimizesthefutureexpectedcostsfromanystartingstate.Markovdecisionprocessesareknowntohaveanoptimalpolicy.ItisoftendiﬃculttodirectlycalculatevaluefunctionsofMarkovdecisionprocessesdirectlybecausetheexpecta-tioniswithrespecttotheobservationsandactionsatallfuturetimes.Dynamicprogrammingcanbeappliedtodealwiththisissue,providedthatastatetransitionmodelfortheenvironmentisknown[29].Formanyproblems,though,evenjustestimatingareasonablygoodmodelisasarduousasuncoveringanoptimalpolicy.Itcanbeadvantageoustodefertomodel-free,approximatedynamicprogrammingapproacheswhendealingwithenvironmentsthatpossessunknown,potentiallycomplicatedstatetransitiondynamics.Examplesofsuchmodel-freereinforcementlearningSUBMITTEDTOTHEIEEETRANSACTIONSONCOMPUTATIONALINTELLIGENCEANDARTIFICIALINTELLIGENCEINGAMES5schemesincludetemporal-diﬀerencelearningproposedbySutton[30],Q-learningpioneeredbyWatkins[31],andSARSAdevelopedbyWieringandSchmidhuber[32].Model-freereinforcementlearningapproachesoperatebyconstructingstochasticapproximationsofthevaluefunctioninanattempttocircumventtheintractablenatureofdirectestimation.Amechanismbehindthisapproxi-mationisaselectionofactionsateachstageofthelearningprocess.Actionscanbeselectedbaseduponthecurrentpolicy,whichintroduceslittletonoadditionalinformationabouthowtheoptimalvaluefunction.Novelactions,whichdeviatefromthecurrentpolicy,mayalsobechosen.Thislatteroptionleadstoanexplorationofthepolicyspaceand,oftentimes,theformationofaccuratevalue-functionapproximations[1].3.2ValueofInformationPoliciesWeseekameanstodeterminewhenitisappropriate,withrespecttotheexpectedcoststhattheagentwillaccrue,tochooseactionsthatdeviatefromthepolicyandwhenitisnot.Wealsoseekawaytospecifyhowcost-sensitivetheaction-selectionprocessshouldbe.This,inturn,dictateshowriskytheagentbehaviorswillbe.Riskyagentsshouldtakeactionsthatattempttominimizecost,attheexpenseofnotcompletingitsobjectivesinagivenepisode.Cautiousagents,incontrast,shouldemphasizeﬁnishingimportingobjectivesbeforeattemptingtoreducecosts.Bothofthesedesirescanberealized,inpracticalform,byleveraginginformationthatthestatescarryabouttheactions.Thisideaofutilizinginformationindecision-makingwasdevelopedintoarigoroustheorybyStratonovich,whichtooktheformofthevalueofinformationcriterion.Thiscriterioncanbedescribedasfollows.ConsideracompositesystemdeﬁnedbyameasurablestatespaceSandameasurableactionspaceA;moregenerally,thesetwospacescanbearbitrary.Weassumethatthestatest∈Sisarandomvariabledescribedbysomeprobabilitydistribution.Forthissystem,weassumethatanobservationismade.Forourpurposes,theobservationwillbethecurrentstate.Aftertheobservationofthestate,onechoosesanoptimalestimatorat∈Athatminimizestheconditionalexpectedpenaltiesinfat∈AE[q(st,at)|st]forsomemeasurablequalityfunctionq.Ifwedonothaveanyinformationaboutthevalueoftherandomvariablest∈S,thenwehaveonlyonewaytochoosetheoptimalestimatorat∈A.Inthiscase,weseektominimizetheaveragepenalties,whichgivesthefollowingleveloflossesinfat∈AE[q(st,at)].NotethatE[q(st,at)]=E[q(st,at)|st],whichimpliesthatwedonotaverageoverst.Naturally,thebeneﬁtyieldedbythereceivedinformationisrelatedtothediﬀerenceinlosses.Ontheotherhand,ifthereiscompleteinformation,thenE[infat∈AE[q(st,at)|st]]canbeminimizedbychoosingat∈Aforeachst∈S.Whentheinformationcontentisinbetweenthesetwoextremes,Stratonovich[3–5]proposedchoos-ingtheoptimalestimatorbymaximizingE[infat∈Aq(st,at)]−infαE[infat∈AE[q(st,at)|st]],wherethesecondtermissubjecttoaninformationconstraintparameterizedbyα∈R.Thiscanbewrittenforthepurposesofreinforce-mentlearningasinfat∈A Xst∈Spr(st)q(st,at)!−infpr(at|st),α Xst∈SXat∈Apr(st)pr(at|st)q(st,π∗at(st))!.(3.3)Thetwotermsin(3.3)havethefollowinginterpretation.Theﬁrsttermcapturesthepossiblereturnforapolicyinwhichnoinformationaboutthestatescanbeinferredfromtheaction.Thisisusedtoestablishthebaselineagentperformance.Iftheactionsarenotinformative,thentheoptimaldecisionisbasedsolelyonthestaterandomvariabledistribution.If,however,theactionsareinformative,thenthereturnsforthesimplestpolicywillbeoﬀsetbyasecondterm,Thissecondtermencodesthereturnsassociatedwithpolicieswhosemaximumaction-statecomplexityisspeciﬁedapriori.Itisbasedontheexpectedreturnusingamodiﬁedaction-valuefunction.Solving(3.3)willquantifytheprobabilityofchoosingaparticularactioninagivenstate.Stratonovichdeﬁnedmultipleversionsofthevalueofinformationcriterion[5].TheversionthatweutilizeistheShannontype,whichhasShannoninformationasaconstraintforthesecondtermin(3.3),pr(at|st)suchthat: Xst∈SXat∈Apr(st)pr(at|st)log(pr(at|st))−Xst∈SXat∈Apr(st,at)log(pr(at))!≤α,α>0.(3.4)Thisinformationconstraintspeciﬁesthecomplexityofthepolicy.Themagnitudeoftheconstraintparameterαdictatesthedegreesoffreedomavailablefortheresultingpolicy:theloweritsvalue,thefewerthedegreesoffreedomforapolicy.Therefore,(3.3)togetherwith(3.4)canbeviewedasdescribingthelargestpossiblereductionofcoststhatcanbeobtainedforapolicyofaprescribedcomplexity.Phrasedinadiﬀerentmanner,wecanviewpoliciesasaofthejointstate-actionspace,sincepoliciesboundthepossibleactionchoicesthatanagentcanmake.Theobjectivefunctionsmeasurethetrade-oﬀbetweentheincurredcostandtheamountofpolicycompressionforthegloballyoptimalpolicy.Theaboveexpressionsdeﬁnetheexpectedcost,E[infat∈Aq(st,at)]−infαE[infat∈AE[q(st,at)|st]],whichisalowerfrontieronutility.Wealsodeﬁneanupperfrontier,E[infat∈Aq(st,at)]−supαE[infat∈AE[q(st,at)|st]],whichcorrespondstotheworst-casescenarioofmaximalcost.Ineithercase,theoptimizationsaresubjecttoaninformationSUBMITTEDTOTHEIEEETRANSACTIONSONCOMPUTATIONALINTELLIGENCEANDARTIFICIALINTELLIGENCEINGAMES6bound.Thesetwofrontiersarereferredtoastheabnormalandnormalbranchesofthevalueofinformationandrepresentthemaximalreductionandmaximalgainincost,respectively.Propertiesofthesetwobranchescangiveinsightsintoaction-selectionbehaviors.Towardthisend,wenotethataction-statespaceswithnon-zeroentropyhaveanon-zeroinformationpotential.Thismeansthataftertakinganaction,theShannoninformationmayincreaseordecreasebythesomeamount,whichcorrespondstochangingtheinformationconstraintparameterαbyavalueα0.Iftheconstraintisdecreased,thenthevalueofinformationwillshifttowardthenormalbranch.Thenormalbranchimpliesrisk-aversioninactionchoices[33].ThisisbecausethepotentialincreaseinfαE[infat∈AE[q(st,at)|st]]−infα+α0E[infat∈AE[q(st,at)|st]]associatedwithchangesα0inShannoninformationislessthanthepotentialde-creaseinfαE[infat∈AE[q(st,at)|st]]−infα−α0E[infat∈AE[q(st,at)|st]].Empiricalevidencealsoimpliesthatrisk-aversioncoincideswithacompressionofthestate-actionspace.Thatis,astheinformationconstraintparameterisdecreased,manystateswillbegroupedtogether.Visitingaparticularstategroupwillelicitthesameactionresponse.Sub-groupsofstatesmaybeuncoupledandassignedadiﬀerentactionresponseduringthelearningprocess,butthisrarelyoccurs.Consequently,thereislittleimpetusforanagenttoundertakeriskyactions,asonceapolicyhasbeenfoundthatfacilitatesthecompletionofanobjectivereasonablywell,itisusuallynotmodiﬁedmuch.Iftheinformationconstraintparametervalueisincreased,thenthevalueofinformationwillshiftawayfromthenormalbranchandtowardtheabnormalbranch.Theconvexityoftheabnormalbranchpromotesrisk-takingbehaviorswhenchoosingactions[33].ThisisbecausethepotentialincreasesupαE[infat∈AE[q(st,at)|st]]−supα+α0E[infat∈AE[q(st,at)|st]]associatedwithchangesα0ininformationisgreaterthanthepotentialdecreasesupαE[infat∈AE[q(st,at)|st]]−supα−α0E[infat∈AE[q(st,at)|st]].Ourexperiencesindicatethatthereislittletonocompressionoftheaction-statespacewhenthisoccurs.Everystatethereforehasthepotentialtobepairedwithauniqueaction.Groupsofstatesthatareformedearlyinthelearningprocessmaybeuncoupledinthelaterstages.Takingactionsthatdeviatefromthecurrentpolicyisnaturallyencouraged,astheywillalterthestate-actionmappingandleadtopotentiallylower-costpolicies.3.2.1FindingPolicieswiththeValueofInformationTheprecedingformofthevalueofinformationcriterionhassomepracticaldiﬃculties.Inparticular,investi-gatorsmusthaveknowledgeoftheminimumexpectedcostassociatedwiththegloballyoptimalpolicy.Evenjustestimatingthiscostcanbetroublesomeformanyenvironments.Wedemonstrated,in[34],thatthecriterioncouldbereformulated,inanequivalentmanner,tosidestepthisissue.Algorithm3.1:Value-of-Information-Basedq-Learning1Chooseanon-negativevalueforthestep-sizeparameterβ.2Chooseanon-negativevaluefortheagentrisk-takingparameterγ.3Initializetheconditionalprobabilitiespr(a|s),s∈S,a∈A.4Initializetheaction-valuefunctionq(a,s),s∈S,a∈A.5fort=0,1,2,...do6Initializetheconditionalprobabilitiespr(0)(at|st),st∈S,at∈A.7Updatethestatevisitationprobabilitiespr(st),pr(st)←Xs0t∈SXat∈Apr(st|at,s0t)pr(at|s0t)pr(s0t),st∈S.8fork=0,1,2,...do9Updatepr(k)(at)←Xst∈Spr(k)t(at|st)prt(st),at∈A.10Updatepr(k)(st)←Xs0t∈SXat∈Apr(st|at,s0t)pr(k)(at|s0t)pr(s0t),st∈S.11Updatepr(k+1)(at|st)←pr(k)(at)eq(st,at)/γ,Xat∈Apr(k)(at)eq(st,at)/γ,st∈S,at∈A.12Updatethepolicyπst,at←pr(k+1)(at|st).13Chooseanactionπs,·(st)→at∈Aandperformastatetransitionst→st+1∈S.Obtainacostgt+1∈R.14Updatetheaction-valuefunctionestimatesq(at,st)←q(st,at)+β gt+1(st,at)+γsupaq(st+1,a)−q(st,at)!.Asabyproductofthisrefor-mulation,weobtainaconstrainedconvexcriterion.Wecanconvertthisconstrainedcriterionintoanunconstrainedone,whichcanbegloballyoptimizedaccordingtoanexpectation-maximization-typeup-date.TheseupdateequationscanbecombinedwithMarkov-decision-process-basedq-learningtoperformreinforcementlearning.Theresultingprocedureforthetabularcaseisgiveninalgorithm3.1.Thecorrespondingoptimizationstepsforthevalueofinformationaregiveninalgorithm3.1,steps8and10.Theupdateinstep10leadstoaprocessforweightingactionchoicesineachstateaccordingtoaBoltz-manndistributionparameterizedbytheexpectedcost.Thisresemblessoft-max-basedactionselection.Thediﬀerenceisthatanextraterm,whoseformisgiveninstep8,hasbeenin-cludedtoaccountforthepromotionorsuppressionofriskyactions.Thisterm,alongwithanassociatedparameter,weightstheexpectedcostsaccordingtothelevelofriskthataninvestigatorSUBMITTEDTOTHEIEEETRANSACTIONSONCOMPUTATIONALINTELLIGENCEANDARTIFICIALINTELLIGENCEINGAMES7wishesanagenttopossess.Itisstraightforwardtoshowthatthisactionweightingisgreedyinthelimit,undersomerelativelymildconditions.Therefore,itwillproduceoptimal-costpolicies[35].Theparameterγ∈R+thatarisesuponminimizationoftheunconstrainedvalueofinformationdictatestherateofchangein(3.3)withrespecttotheconstraintvaluein(3.4).Asthevalueoftheparametergoestoinﬁnity,thepolicycomplexityconstraintsareignoredandtheprobabilityofchoosinganactionbasedonanobservationisequivalenttotheperfectlyrationalpolicyforeachpossibleobservation,independentofanyotherpolicies.Theactionprobabilitiesbecomeamixtureofthesesolutions.Notethatifthereisasubsetofperfectlyrationalsolutionsthatissharedamongtasks,thenonlythissubsetwillbeassignedprobabilitymass,sinceitreducestherelativeentropy.Importantly,highvaluesoftherelativeentropytermin(3.4)willnotleadtoapenalization,whichmeansthatactionscanbeinformativeabouttheobservations.Thebehaviorofanagentwithinﬁnitecomputationalresourceswillthusbeveryobservation-speciﬁc.Asthevalueoftheparametergoestozero,therelativeentropybetweentheactionsandobservationsisminimized.Thisimpliesthattheprobabilityofchoosinganactionbaseduponthecurrentstateisindependentofthecurrentstate.Thatis,allstateselicitthesameresponse,sincethepolicyissosimplethattheagentcannotchangeitsbehavior.Despitethislimitation,though,theagentwillstillselectactionsthatattempttooptimizetheexpectedpolicyreturns.Forvaluesoftherationalityparameterbetweenthesetwoextremes,thereisatrade-oﬀthatoccurs.Astheparametervalueincreases,theagentmayfavorstate-speciﬁcactionsthatyieldabetterexpectedreturnforparticularstates.Thisleadstoahighrelativeentropybetweenthestatesandactions,implyingthatthepolicycomplexitywillbehigh.Alternatively,astheparametervaluedecreases,theagentmayfavoractionsthatyieldagoodexpectedutilityformanyobservations,whichproducesalowerrelativeentropy.Theresultingpolicycomplexitywillhencebelower.Choosinggoodparametervaluescanbechallenging,astheirinﬂuenceoveragentriskisapplicationdependent.Thereareafewstrategiesthatcanbeusedforthispurpose,however.Oneexamplewouldinvolveusingexpertknowl-edge.Investigatorsoftenhaveexpectationsaboutthescorestheywishforthepoliciestoobtain.Areasonableupperandlowerboundforthesescorescouldthenbeusedtoderiveacorrespondingparametervalue.Theparametervalueswouldthenbeﬁxedduringthelearningprocess,whichleadstoanunchangingamountofrisk.Albeitasimplestrat-egy,theamountofriskmaynotbeappropriateeitherearlyorlateintothelearningprocess.Thepolicyreturnsmaybeadverselyimpactedasaconsequence.Amoreprincipledstrategywouldbetoemploydeterministicannealingtoadjusttheamountofagentrisk.Ini-tially,ahighvaluecouldbeselectedfortheparameter,whichpromotesdeviatingfromthepolicywhenthereispo-tentialforcostimprovement.Steps4-13inalgorithm3.1wouldthenbeexecuteduntilconvergenceisachieved.Oncethishappens,thissolutionwouldbetakenastheinitialiterateofanewsearchprocess.Theparametervaluewouldsimultaneouslybereducedaccordingtoanannealingrateparameter.Thisforcestheagenttotakelessrisksasthelearningprogresses.Thisprocessrepeatsuntiltheparameterreachessomeminimalvalue.Bythistime,assumingthattheannealingrateisnottoofast,theagentshouldchooseitsactionsinadeterministicmanner.Althoughthisprocedureismorecomputationallydemanding,itensuresthattheagent’sactionswillbechoseninawaythatmax-imizestheoptimizationcriterion.Ifaninvestigatoriswillingtosacriﬁceguaranteedoptimalityinfavorofreducedcomputation,asimplerheuristicofloweringtheparametervalueacrossepisodescouldbeutilized.4SimulationsandDiscussionsWenowassessourreinforcementlearningstrategyforaMs.Pac-Managent,whichisasuﬃcientlychallengingproblem.Foremost,itisadynamicenvironmentinwhichtherearemovingobstacles,whicharetheghosts.Theghosts’movementsarerandom,whichmakestheentireproblemstochastic.Aswell,thegamerequiresswitchingbetweendiﬀerentplaystyles,uponconsumingpowerpellets,soastominimizethetotalcosts.Thisshiftinplaystylesispredominantlyriskdriven.Withoutanabilitytopromoteriskyagentbehaviors,thedevelopmentofmulti-modalstrategiesmaynotoccur.Theaimsofourreinforcementlearningsimulationsaretwo-fold.First,wewouldliketoquantifytheinﬂuenceoftheinformationconstraintparameterontheagentperformance.Secondly,wewanttodeterminehowmuchriskisneededformulti-modalbehaviorstoemerge.4.1SimulationPreliminaries4.1.1AgentRewardsThescoringsystemforourimplementationofMs.Pac-Manisasfollows.IfamoveleadstoastatewhereMs.Pac-Manconsumesapellet,asmallnegativecostisincurred(−10).Nocostisgivenforeatingapowerpellet.Eat-ingarandomlyspawningandmovingfruitleadstoaone-timereductionincostsperlevel(−150).AnegativecostisincurredwhenMs.Pac-Maneatsapowerpelletandthenproceedstoeataghostwithinthepower-pellettimecon-straints(−200).Alargenegativecostisobtainedwhenevernopelletsremainonagivenlevel(−500).AconstantpositivecostisassignedwheneverMs.Pac-Manmovesinoneoffourcardinaldirectionstoanewlocation(+1).SUBMITTEDTOTHEIEEETRANSACTIONSONCOMPUTATIONALINTELLIGENCEANDARTIFICIALINTELLIGENCEINGAMES8Remainingwithinthesamelocationacrossastatetransitionisalsopenalized(+1).Collidingwithaghost,with-outhavingrecentlyeatenapowerpellet,resultsinahighpenalty(+300).Winningthisgamethereforeamountstoselectingthebestactioninanygivenstateinordertoreducethefutureexpectedcosts.4.1.2StateSpacesThereareavarietyoffeaturesthatcanbeusedtocodifythedecision-processstatespace.Here,wecomputeaseriesofholisticfeatures.Suchfeaturesincludethedistancetothenearestpellet,powerpellet,fruit,nearestghost,andsecondnearestghost,allofwhichareinrelationtotheagent.Thesedistancesaremeasuredbythenumberofstepstheagentwouldneedtoreachthoselocationsinthemostdirectfashion.Asbefore,wequantizethedistancesintotwocategories.Wealsoincludethecardinaldirectioninwhichtheagentwouldneedtotraveltoreachthoselocations,assumingnoobstaclesarepresentinthelevel.Directionaltiesarebrokenrandomly.Wealsohavebinaryfeaturesthatdetermineifthetwonearestghostsareinthesametunnelastheagent,ifthefruithasspawned,ifapowerpelletisactive,ifthenearestandsecondnearestghostsarevulnerable,andiftheagentisinatunnelorajunction.Lastly,wehaveafeaturethatquantiﬁesthenumberofstepssincethelastpowerpellethasbeeneaten.Intotal,thereareeighteenfeatures.Sincetheholisticfeaturesarenotdependentonthelevelconﬁguration,policiesobtainedfromthemshouldgeneralizewell.4.2SimulationResultsFortrainingpurposes,weconsidera26×29gridlevelconﬁgurationwithaﬁxednumberofstaticobstaclesandinter-connectingwarppipes.Weconsiderasmall,non-standardgridsizetoexpeditethetrainingofMs.Pac-Managents.Thissmallenvironmentalsoallowstheinvestigationofpolicygeneralizationtolargerenvironments.Onthisgrid,wehaveallocated184pelletsalongvariouscorridors.Fourpowerpelletsareplacedinthecornersofthegrid.Fromthis,theminimumattainablecostforsimplyclearingthelevelwiththeminimalnumberofmovesis−2105.Thebestattainablecostis−5455,whichassumesthatallfourghostsareconsumedforeachactivepowerpellet,thatthefruitisconsumed,andthatthereisnounnecessarybacktrackingthroughthelevel.Inpractice,theempiricalcostswillbeabovetheseamounts,astheagentmayneedtomovetonon-pellet-ﬁlledregionstoevadetheghosts.Theagentmayalsoleavebehindsmallgroupsofpelletsasitclearsthelevel.Wehavethreefreeparametersthatneedtobesetduringthelearningprocess.Theﬁrstparameterdeterminestheamountofpolicy-spacesearchgranularity.Areasonablevalueforthisis3760bits.Thisvalueisthehalfwaypointbetweenaminimalpolicycomplexity,1880bits,andthemaximalpolicycomplexitythatweconsider,5640bits.Theminimalpolicycomplexitydescribestheleastnumberofcontrolbitsneededtoclearthegivenlevel,assumingnounnecessarybacktrackingisperformed.Themaximalpolicycomplexitywasobtainedassumingthattheagentmayhavetobacktrackthroughtheentirelevelthreetimes.Aparametervalueof3760bitsshouldthereforeallowtheagenttoevadeghostsandstillcompletethelevel.Itshouldalsostrikeagoodriskbalance.Exceptwhereotherwisenoted,weusethisparametervaluethroughoutoursimulationsandwhendiscussingthesimulationresults.Theremainingtwoparametersarethelearningrateandthediscountfactor,whichdictatetheaction-valueupdatebehaviorwhenusingQ-learning.Wesetthelearningrateto0.3sothatmoreweightisgiventopreviouslyacquiredaction-valuemagnitudesthanthosethatweremorerecentlyacquired.Wetemporarilyincreasethelearningrateto0.7forasingleepisodeifthatepisodehasthelowestcostcomparedtoallpreviousepisodes.Thisensuresthatthepolicyquicklyimplementsnewagentbehaviorstobetterplaythegame.Adiscountfactorto0.9wasusedsothattheagentwouldseekactionsequenceswithlowlong-termcosts.EachofthesevalueswaschosenbaseduponpreviousapplicationsofQ-learningtostochasticgameenvironments.Exceptwhereotherwisenoted,weusetheseparametervaluesthroughoutoursimulations.4.2.1Early-TermAgentPerformanceandBehaviorsOurﬁrstsetofsimulationsareusedtoestablishtheaverageexpectedperformanceforcomparativepurposes.Wehaveplottedresultsofthesesimulationsinﬁgure4.1.Theplotsshowninﬁgures4.1(a)–4.1(d)arosefromtheuseofholisticfeatures.Theplotinﬁgure4.1(e)arosefromtheuseofhybridfeatures.Theresultspresentedinthissectionandthenextpertaintotheuseofholisticfeatures,exceptwhereweindicateotherwise.Figures4.1(a)–4.1(c)giveavisualoverviewoftheagent’searlybehaviorasthenumberoftrainingepisodesincreasesfrom1000to3000.Atrainingepisodeendswhenevertheagenteitherclearsthelevelorcollideswithaninvulnerableghost;thelevelresetsineithercase.Asshowninﬁgure4.1(a),theagent’sinitialperformanceispoor.Theagentofteneitherremainsstationaryorexecutesarandom,unnecessaryaction.Thelatterchoicemayleadtobacktrackingthroughalreadyvisitedareasofthelevel,leadingtocostincreases.Additionally,theagenthasnotyetlearnedthatcollidingwithghostsresultsinalarge,positivecost.Theagentthereforeclearsonlysmallsectionsofthelevel.Suchbehaviorsarisesincethepolicyspacehasonlybeensparselyexploredatthispoint.Thatis,severalstatesSUBMITTEDTOTHEIEEETRANSACTIONSONCOMPUTATIONALINTELLIGENCEANDARTIFICIALINTELLIGENCEINGAMES9(a)LevelConfiguration,Step:16TrainingEpisode:1000,Cost:-94LevelConfiguration,Step:24TrainingEpisode:1000,Cost:-136LevelConfiguration,Step:32TrainingEpisode:1000,Cost:-171LevelConfiguration,Step:36TrainingEpisode:1000,Cost:-187FeatureWeights0.050.060.070.010.010.030.040.080.130.010.140.010.010.010.010.030.050.03(b)LevelConfiguration,Step:16TrainingEpisode:2000,Cost:-144LevelConfiguration,Step:32TrainingEpisode:2000,Cost:-288LevelConfiguration,Step:48TrainingEpisode:2000,Cost:-432LevelConfiguration,Step:80TrainingEpisode:2000,Cost:-630FeatureWeights0.070.120.130.030.020.040.060.110.160.030.170.030.040.030.010.050.060.06(c)LevelConfiguration,Step:25TrainingEpisode:3000,Cost:-225LevelConfiguration,Step:50TrainingEpisode:3000,Cost:-370LevelConfiguration,Step:75TrainingEpisode:3000,Cost:-705LevelConfiguration,Step:100TrainingEpisode:3000,Cost:-920FeatureWeights0.160.160.170.050.050.070.090.200.210.040.220.040.040.040.010.100.110.150100020003000-750-500-2500250TrainingEpisodeCostAgentPerformanceHolisticFeaturesComplexity:5640bitsComplexity:3760bitsComplexity:1880bits(d)Figure4.1:Anoverviewoftheagent’sghostavoidancebehaviorimprovementsovermultipletrainingepisodes.Theimagesinrows(a)through(c)ontheleft-handsideoftheﬁgurehighlighttheenvironmentstateovermultipleactionsforagiventrainingepisode.Ineachoftheseimages,thecoloredlinesindicatethepathsthattheagent(yellowline),invulnerableghosts(blue,red,orange,andmagentalines),andvulnerableghosts(limegreen,blue,green,andcyanlines)tookthroughtheenvironmentacrossanumberofactionchoices.Attheendofeachrow,weprovideatableofthenormalizedfeatureweightsforeachoftheeighteenholisticfeatures.Darkercolorsindicategreaterfeatureimportance.Thefeaturesarepresentedinthesameorderastheywereintroducedfromthediscussions,wherethenumberingisfromtoptobottomthenlefttoright.Apolicycomplexityof3760bitswasusedfortheresultsgiveninrows(a)through(c).Onlyholisticfeatureswereusedtogeneratetheseresults.Plot(d)ontheright-handsideoftheﬁguregivetheaverageagentcostacrossthetrainingepisodes.Forthisplot,wehighlighttheaveragecostswhenconsideringapolicycomplexityof5640bits(blue),3760bits(green),and1880bits(red).Thesecomplexityvaluescorrespondtohigh,moderate,andlowlevelsofpolicy-spaceexploration,respectively.haveeithernotbeenvisitedorvisitedfrequently,andmanyrelevantvalue-functionentriesdonotyethavemeaningfulvalues.Astheagentaccruesmoreexperiences,theagentcanmoreeﬀectivelyclearthelevel.Thisishighlightedinﬁgures4.1(b)–4.1(c).Theresultsinﬁgure4.1(b)demonstratethattheagentmayspendsigniﬁcantlylesstimeuselesslybacktrackingthroughalreadyclearedsectionsofthelevel.Theagentalsoactivelyavoidsghosts,therebyincreasingitschancesofconsumingallofthepellets.However,theagenttendstonotkeepaconservativedistancebetweenitselfandnearbyghosts;itremains6.87±2.24gridcellsaway.Thiscanleadtoitbeingtrappedwhentheghostslaunchatwo-orthree-prongedpincerattack,asisthecaseinﬁgure4.1(b).Laterinthelearningprocess,theagentattemptstoincreasethedistancebetweenitandtheghosts,staying9.25±3.28cellsaway.Manyﬂankingstrategiesthattheghostsemployarenolongereﬀective.Asshownattheendofﬁgure4.1(c),theagenttypicallyleavesenoughofagapthroughwhichitcanescape.Inﬁgures4.1(a)–4.1(c),wehaveincludedplotsofthenormalizedfeatureweights[36].Theweightsindicatetheimportanceoftheeighteenholisticfeaturesfordecision-making,withhighervaluesimplyinggreaterimportance.Inorderfromtoptobottom,lefttoright,theeighteenfeaturesare:thedistance/directiontothenearestpellet,powerpellet,fruit,nearestghost,andsecondnearestghost,ifthenearestandsecondnearestghostsareinthesametunnel,ifthefruithasspawned,ifapowerpelletisactive,thevulnerabilityofthenearestandsecondnearestghosts,ifanagentisinatunnel,andthenumberofstepssinceapowerpellethasbeeneaten.Asshownininﬁgure4.1(a),theweightsaremostlyhomogeneous,suggestingthatthereisnopreferenceforusingvariousfeaturestotakecertainactionsearlyinthelearningprocess.Theonlyexceptionsarethetwofeaturesrelatedtothenearestpellets,astheyareearlyfactorsofthetotalscore.Anotherexceptionistheattributethatdescribesifanagentisinatunnelorajunction.Thisfeatureisusedtodetermineiftheagentcanpickanewdirectioninwhichtotravel.Inﬁgure4.1(b),thereisanincreasedemphasisonfeaturesrelatedtothedistanceandpositionofthenearestpelletandnearestghosts,iftheagentisinatunnel,andiftheghostsareinthesametunnelastheagent.ThischangeSUBMITTEDTOTHEIEEETRANSACTIONSONCOMPUTATIONALINTELLIGENCEANDARTIFICIALINTELLIGENCEINGAMES10correspondswiththeabilityfortheagenttoclearlargerportionsofthelevel.Inﬁgure4.1(c),therelativeimportanceoffeaturesrelatedtothetwonearestghostsbecomealmostequal,suggestingthatbothghostswillinﬂuencetheagent’sactions.Incomparison,forﬁgure4.1(b),onlythenearestghostfeaturesareprominent.Theagentcanthusbesusceptibletocoordinatedattacksfromtwoormoreghosts.Evenaftermanyepisodes,therearefeaturesthatarerelativelyunimportantfordecisionmaking.Mostoftheseattributesarerelatedtotheutilizationofpowerpellets.Aswewilldiscussshortly,certainconditionsneedtoarisefortheagenttotakeadvantageofpowerpellets.Aswell,theagentdoesnotinitiallyknowthatconsumingfruitsleadstoacostreduction.Certainsituationsneedtooccurforthesefruit-basedfeaturestobeofuse.Figure4.1(d)highlightsthepolicyperformanceacrosstheﬁrst3000episodesfor25MonteCarlosimulations.Thisplotshowsasharpdecreaseincost,duetoaneﬀectiveexplorationofthesearchspaceandexploitationofhigh-performingactions.Fortheseearlyepisodes,theagenthasnotyetlearnedtochaseafterspawnedfruit.Italsounnec-essarilyevadesvulnerableghosts.Nevertheless,thevalue-of-information-basedpoliciesareperformingwellfortheholisticfeaturecase.Inﬁgure4.1(d),wealsocapturetheeﬀectsofdiﬀerentpolicycomplexityvaluesontheﬁnalcostfortheholisticfeaturecase.Whenincreasingthecomplexityfrom3760bitsto5640bits,theearlyagentperformanceisworse:−249.4±177.9forthelattercaseand−661.0±101.2fortheformercase.Thisistobeexpected,asthepolicysolutionspaceisbeingﬁnelysearched.Theagentisthereforeoftenattemptingactionsthatdeviatefromtheprescribedpolicy.Withasuﬃcientnumberofepisodes,though,thisrisk-intensivesearchwilleventuallygiverisetopolicieswithlowercosts.Policiesfromthemorerisk-intensivesearchcasewilltendtorivalthepoliciesfromtherisk-balancedsearchcasetheafter6526.5±267.2episodesandovertakethemafter7980.3±323.4episodes.Duringtheﬁrst3000episodes,suchhigh-complexitypoliciestendtoleave2.80±1.92moreislandsofpelletsthanthemoderate-complexitycase,with7.75±4.83pelletsineachisland.Theagentalsoroutinelyoutpacestheghosts,staying5.36±1.79gridcellsaway.However,thisisamarkeddecrease,astheagentwouldonlystay9.25±3.28cellsawayfromenemiesinthebalancedsearchcase.Additionally,theagentattemptstomaneuvertoadjacenttunnelswithin9.07±2.62stepsifitdetectsthatoneormoreghostsarepresentinthecurrenttunnel.Inthemoderate-complexitycase,itmaneuversintoanemptytunnelwithin6.32±1.97steps.Inshort,high-complexitypoliciesyieldbehaviorsthatlowertheagent’schancesofsuccessfullycompletingthelevelearlyinthetrainingprocess.Givenenoughtime,high-complexitypolicieswillperformjustaswellifnotbetterthanthoseoflowercomplexity.Whendecreasingthepolicycomplexityfrom3760bitsto1880bits,theperformanceofthepolicyimproves.Theaverageexpectedcostsdropsfrom−661.0±101.2to−775.15±72.31intheﬁrst3000episodes.Inthiscase,thepolicysolutionspaceisbeingcoarselysearched.Theagentwillonlypossessthemostbasicbehaviorsforwinningthegame,whichincludenavigatingtothenearestpelletsandﬂeeingnearbyghosts.Rarelywilltheseabilitiesbehonedsothattheagentconsistentlyclearsthelevelsastrainingprogresses.Fortheﬁrst3000episodes,however,low-complexitypoliciesoutperformbothmoderate-andhigh-complexitypolicies.Forinstance,suchpoli-ciestypicallyremain10.12±2.47gridcellsawayfromghosts.Italsotakes,onaverage,about6.18±1.57stepsfortheagenttomovetoanemptytunnel.Boththesestatisticsindicatethattheagentwillnotbeaseasilycaughtincoordinatedattacksfromtheghosts.Lastly,itleaves1.62±0.76fewerislandsofpelletsthanthemoderate-complexitycase,with6.35±1.83pelletseach.Backtrackingthroughthelevelisthuslessnecessary,whichminimizestheagent’sriskofdying.Theaboveresultswereobtainedforthesituationwherewehadarelativelylowlearningrateduringamajorityofthetrainingprocess.Thiscorrespondstothesituationwheretheactionvalueisbuildingupanaverageofalltheagent’sexperiences.Raisingthelearningrateuptoacertainthresholdhadamarginalimpactontheagentperfor-mance.From0.3to0.45,wefoundaslightdecreaseinthemeancostintheﬁrst3000episodes:−77.51±12.83.Between0.45and0.65,theimpactbecamelesspronouncedintheﬁrst3000episodes:−31.28±10.28.Beyondalearningrateof0.65,theagentperformancedropped.Forlearningratesof0.7,0.8,and0.9,theperformancechangeswere9.65±2.62,43.27±11.44,and97.25±24.31,respectively,overtheﬁrst3000episodes.Eachofthesecasescorrespondstotherunningaveragebeingmoreeasilyperturbedbytheagent’sexperiences.Earlyinthetrainingprocess,thepolicieswouldbegreatlyaﬀectedwhenevertheagentcollidedwithavulnerableghost.Thiseventwouldsometimesimpedetheformationofhigh-performingbehaviorsformanysubsequentepisodes.Whentheagentstum-bleduponagoodstrategy,thecorrespondingaction-statepairswouldbeemphasizedsoastopromotethesametypesofmaneuversinthefuture.Giventhattheenvironmentisstochastic,however,suchgoodstrategieswouldbeundone.Wehaveassumedthatthelearningratewasﬁxedforeachstate-actionpairinoursimulations.WatkinsandDayan[31]suggested,fortabularQ-learning,thateachstate-actionpairshouldhaveitsownlearningrate,whichisdictatedbythenumberoftimesthatstatehasbeenvisitedandthatactionhasbeenchosen.Theyprovedthatsuchadynamicweightupdatewillcausetheaction-valuemagnitudestoconvergetouniquevalueswhichdeﬁneastationary,deter-ministicallyoptimalpolicy.Thisresultisindependentoftheaction-statetableinitialization.Inourfutureinvestiga-tions,wewilldetermineifthisresultsholdsforpoliciesfoundusingvalue-of-information-basedQ-learning.Inthemeantime,ourempiricalﬁndingshavedemonstratedthatthereisamodestbeneﬁttousingdiﬀerentlearningratesSUBMITTEDTOTHEIEEETRANSACTIONSONCOMPUTATIONALINTELLIGENCEANDARTIFICIALINTELLIGENCEINGAMES11(a)LevelConfiguration,Step:40TrainingEpisode:3000,Cost:-360LevelConfiguration,Step:50TrainingEpisode:3000,Cost:-440LevelConfiguration,Step:60TrainingEpisode:3000,Cost:-520LevelConfiguration,Step:70TrainingEpisode:3000,Cost:-600FeatureWeights0.190.190.200.070.090.090.110.250.260.050.250.060.050.060.010.150.160.19(b)LevelConfiguration,Step:40TrainingEpisode:4000,Cost:-360LevelConfiguration,Step:50TrainingEpisode:4000,Cost:-450LevelConfiguration,Step:60TrainingEpisode:4000,Cost:-510LevelConfiguration,Step:70TrainingEpisode:4000,Cost:-680FeatureWeights0.210.210.220.170.160.110.120.290.290.050.250.060.050.060.010.150.160.21(c)LevelConfiguration,Step:40TrainingEpisode:5000,Cost:-310LevelConfiguration,Step:50TrainingEpisode:5000,Cost:-390LevelConfiguration,Step:60TrainingEpisode:5000,Cost:-480LevelConfiguration,Step:70TrainingEpisode:5000,Cost:-720FeatureWeights0.260.270.270.260.250.160.150.310.310.070.290.080.090.100.010.230.240.263000400050006000-1250-1000-750-500-250TrainingEpisodeCostAgentPerformanceHolisticFeaturesComplexity:5640bitsComplexity:3760bitsComplexity:1880bits(d)Figure4.2:Anoverviewoftheagent’sfruitchasingimprovementsovermultipletrainingepisodes.Theimagesinrows(a)through(c)ontheleft-handsideoftheﬁgurehighlighttheenvironmentstateovermultipleactionsforagiventrainingepisode.Ineachoftheseimages,thecoloredlinesindicatethepathsthattheagent(yellowline),invulnerableghosts(blue,red,orange,andmagentalines),andvulnerableghosts(limegreen,blue,green,andcyanlines)tookthroughtheenvironmentacrossanumberofactionchoices.Attheendofeachrow,weprovideatableofthenormalizedfeatureweightsforeachoftheeighteenholisticfeatures.Darkercolorsindicategreaterfeatureimportance.Thefeaturesarepresentedinthesameorderastheywereintroducedfromthediscussions,wherethenumberingisfromtoptobottomthenlefttoright.Apolicycomplexityof3760bitswasusedfortheresultsgiveninrows(a)through(c).Onlyholisticfeatureswereusedtogeneratetheseresults.Plot(d)ontheright-handsideoftheﬁguregivetheaverageagentcostacrossthetrainingepisodes.Forthisplot,wehighlighttheaveragecostswhenconsideringapolicycomplexityof5640bits(blue),3760bits(green),and1880bits(red).Thesecomplexityvaluescorrespondtohigh,moderate,andlowlevelsofpolicy-spaceexploration,respectively.foreachstate-actionpair.Whenusingaﬁxedlearningrate,theagentperformanceis−661.0±101.2aftertheﬁrst3000episodes.Changingthepolicycomplexityto1880bitsand5640bitsresultsincostsof−775.15±72.31and−249.4±177.9,respectively.Whenrelyingonanadaptivelearningrate,alongthelinesofWatkinsandDayan[31],theagentperformanceimprovesto−712.89±55.43aftertheﬁrst3000episodes.Changingthepolicycomplexityto1880bitsand5640bitsresultsincostsof−838.88±68.10and−307.41±114.52,respectively.Foroursimulations,wechoseadiscountfactorthatwouldfavorfuturecostsoverthoseinthepresent.Thiswastoensuretheagentwouldstriveforlowlong-termcosts.Decreasingthediscountfactorcanhaveadetrimentaleﬀecttheagent’slong-termbehaviors.Asanexample,supposethatapolicycomplexityof5640bitsisused.Whenchangingthediscountfactorfrom0.7to0.5andto0.3,theagentperformanceis49.72±76.18and89.27±121.47,respectively,aftertheﬁrst1500episodes.After3000episodes,thesecostsbecome−175.81±114.33and−129.42±133.58,respectively,whichrepresentsareductionintotalcostcomparedtothecastswherepolicycomplexitiesof3760and1880bitswereusedwithadiscountfactorof0.7.Thisinitialimprovementoccursbecausethereisadeclineintheamountofpolicy-spaceexploration.Oncetheagenthasuncoveredareasonablygoodpolicy,itfocusesonmakingsmall,risk-adverseadjustmentstothatstrategy.However,thecumulativeeﬀectofthisbehavior,overmanyepisodes,isthatthecostsbegintolagbehindthoseobtainedwithsmallerpolicycomplexities.Forinstance,after5000episodes,assumingadiscountfactorof0.5,thereisacostincreaseof209.66±91.85and290.07±78.72whenpolicycomplexitiesof3760and1880bits,respectively,areusedwithadiscountfactorof0.7.Foradiscountfactorof0.3,thecostincreasesbecome296.24±101.12and326.59±125.37,respectively.Inessence,theagentisunabletotakeriskyactionsthatmightimproveitsfutureperformance.4.2.2Late-TermAgentPerformanceandBehaviorsMuchoftheinitiallearningprocessisdevotedtotwofundamentalbehaviorsthatareneedtosuccessfullycom-pleteeachepisode:consumingpelletsandavoidinginvulnerableghosts.Aftertheagenthassuccessfullyacquiredbothoftheseabilities,othersemerge,providedthatthereissuﬃcientriskintheaction-selectionprocess.SUBMITTEDTOTHEIEEETRANSACTIONSONCOMPUTATIONALINTELLIGENCEANDARTIFICIALINTELLIGENCEINGAMES12Onesuchbehaviorinvolveschasingspawningfruit.Duringtheﬁrst2000episodes,theagentoftenignoresanypresentfruit,asthepolicyhasnotyetassociatedeatingfruitwithlowercosts.Infact,theagentwillnotpursuefruit,evenifitisadjacenttotheagent,asindicatedinﬁgure4.2(a).Eventually,bycoincidence,theagentwillbeinthesamecorridorsasfruitsandconsumethem.Byaboutepisodes2500to4500,thiswillhavehappenedfrequentlyenoughfortheagenttoactivelychasefruitsfromuptosevencellsaway,assumingthattherearenoghostsalongthatpath.Thiscaseisshowninﬁgure4.2(b).Afteranywherefrom6000to12000episodes,theagentwillseekoutfruitsuptoaroundtwelvecellsaway.Itwillevendosoifthechosenpathhasfewpelletsalongit,sincethecostreductionsoﬀsetthemovementpenalties.Beyondﬁfteencells,theagenttendstooverlooksuchbonusitems,especiallyifitiswithinthesamecorridorasaghost.Thefeatureweightsinﬁgures4.2(a)and4.2(b)signalthatthishappensbecauseghostavoidanceisamoreprominentbehavior.Eventually,knowingthelocationofbothfruitandghostsbecomeequallyimportantforthepurposesofminimizingcost,asshowninﬁgure4.2(c).Theagenthasthreefeaturesitcanusetodeterminewhenafruithasspawned.Theﬁrsttwoarerelatedtothedistanceanddirectionofthefruitintheenvironment,whichhasspecialvaluesfornon-existentfruit.Theotherisabinary-valuedfeaturethatdeterminesifafruithasspawnedornot.Asshowninﬁgures4.1and4.2,thislatterfeatureconsistentlyhasalowweight.Thisistrueevenaftermanyepisodes,whichishighlightedinﬁgures4.3and4.4.Itisthereforenotimportantfordecisionmaking.Wefoundthatthisfeaturecouldbereplacedwithtwoattributesthatwouldservetofurtherdecreasecosts.Theﬁrstisafeaturethatdeterminesifafruitisabouttospawn.Thesecondisquantizeddirectionalinformationaboutwherethiswilloccur.Afterapproximately5500episodes,thepoliciesusebothfeaturestoguidetheagentbacktoitsstartinglocation.Itisroutinelyabletoeatthefruitshortlyafteritappears:onaverageonly8.60±2.64actionsaretakenbeforethisoccurs.Insomecases,however,theghostsblockthemostdirectpaths.Anotherbehaviorthatmaterializesisthatofeatingvulnerableghosts.Akintothepreviousexample,theagenthasnoinitialknowledgeofifvulnerableghostscanbeeatenwhileundertheeﬀectsofapowerpellet.Itthereforedefaultstoevadingtheenemieswheneverpossible,whichisillustratedinﬁgure4.3(a).Wetypicallywitnessedthisstrategyduringtheﬁrst2000episodes.Itisnotuntilthosesituationswheretheagentbecomestrappedbetweentwoormorevulnerableghosts,suchasinﬁgure4.3(b),thatthepolicyshiftstoactivelyeatingghosts.Beingsurroundedbyghostsisasomewhatrareoccurrence,though,astheagentquicklylearnstooutpacethem.Wefoundthatshort-termincreasesinthelearningratehelpedtoincreasetheadoptionrateofsuchcost-reducingactionsequences.Otherwise,uptotwentytimesmoreepisodeswereneededtoachievethesamepolicyeﬀects,onaverage.Byaroundepisode8500,theconstructedpoliciesnowcompeltheagenttoseekoutedibleghostswheneverapowerpelletisactivated.Thisisdemonstratedinﬁgure4.3(c),wheretheagentgoesaftertwoghostsinrapidsuccession.Sincethereisnomemoryofhowmanypowerpelletshavebeenconsumed,theagentisusuallycautiousinitsactivepursuitofghosts.Theagentwilltypicallyseekghoststhatarewithinﬁfteencells,uponactivatingapowerpellet.Ifweincorporatedaholisticfeaturethatmonitorsthenumberofavailablepowerpellets,thenwefoundthattheagentwouldbemuchlesstimid.Itwouldchaseghostsforuptotwenty-sevenstepsfortheﬁrsttwopowerpelletsandnineteenstepsfortheremainingpowerpellets.Thatis,thepolicywouldaccountforthetimingdiﬀerencesbetweenthetwosetsofpower-ups.Thesecapabilitiesaroseduetoincreasedweightingofcertainfeatures.Whencomparingthefeatureweightsinﬁgures4.1and4.2tothoseinﬁgure4.3,itcanbeseenthatthedistanceanddirectiontothenearestghostsandpowerpelletsbecomemoreprominent.Theagentisnotonlytryingtodetermineifitshouldheadtothenearestpowerpellet,butalsoifthereareanynearbyghostsforittoconsumeonceitdoes.Featureassociatedwithghostvulnerabilityalsohadamuchgreaterinﬂuenceovertheactionchoices.Curiously,onlyamoderateamountofweightwasgiventothenumberofstepstakensinceactivatingapowerpellet.Wesurmisethatthisisbecausetheagentconsumestheﬁrstandsecondghostswithin12.07±4.73stepsofactivatingapowerpellet.Rarelywillitbeinapositiontogoafterathirdandfourthghostbeforethepowerpelleteﬀectsfade.Onceapproximately50000episodeshaveelapsed,theagentsometimesdevelopedaningeniousstrategy.Itwouldbeginthelevelbymaneuveringtothenearestpowerpelletinthebottomleftorrightcorners.Itwouldthenproceedtoeitherchaseafterthenearestghostorloiternearthemiddleofthestageforanotherghosttoappear.Oncetheeﬀectsofthatinitialpowerpelletworeoﬀ,itwouldimmediatelyheadtothenextnearestpowerpellet.Thisprocesswasrepeateduntiltherewerenomorepowerpellets,uponwhichtheagentwouldattempttocleartheremainingpellets.Whatisinterestingaboutthisstrategyisthattheagentcouldremoveovertwo-thirdsofthepelletsfromthelevelwhileundertheeﬀectsofapowerpelletandwhenheadingeithertoorawayfromapowerpellet.Inessence,theagenthadlearnedthatbeingnear-invulnerablewouldenableittocompletethelevelmorefrequently.Ifweincreasedallofthepower-pelletdurationstothirty-ﬁvesteps,theagentwouldonlychaseghostsonlyintermittently,asthoseeatenghostswouldbeinvulnerablewhentheyre-spawned.Theagentinsteadfocusedonclearingthelevelasquicklyaspossible,onlydeviatingtoactivateanotherpowerpelletoncethecurrentonesubsided.Inﬁgure4.4,weprovideresultsforthebest-performingpoliciesofdiﬀerentcomplexitiesafter25000episodes.Itisimportanttonoticethatthelevelconﬁgurationinﬁgure4.4isslightlydiﬀerentthaninﬁgures4.1–4.3.TheSUBMITTEDTOTHEIEEETRANSACTIONSONCOMPUTATIONALINTELLIGENCEANDARTIFICIALINTELLIGENCEINGAMES13(a)LevelConfiguration,Step:35TrainingEpisode:4000,Cost:-275LevelConfiguration,Step:40TrainingEpisode:4000,Cost:-290LevelConfiguration,Step:45TrainingEpisode:4000,Cost:-285LevelConfiguration,Step:50TrainingEpisode:4000,Cost:-280FeatureWeights0.290.300.300.270.270.180.180.340.350.090.300.100.120.150.010.230.250.29(b)LevelConfiguration,Step:26TrainingEpisode:6000,Cost:-234LevelConfiguration,Step:34TrainingEpisode:6000,Cost:-296LevelConfiguration,Step:42TrainingEpisode:6000,Cost:-308LevelConfiguration,Step:50TrainingEpisode:6000,Cost:-530FeatureWeights0.300.320.320.270.270.180.180.360.360.130.310.140.150.170.010.240.260.29(c)LevelConfiguration,Step:40TrainingEpisode:8000,Cost:-350LevelConfiguration,Step:50TrainingEpisode:8000,Cost:-580LevelConfiguration,Step:60TrainingEpisode:8000,Cost:-780LevelConfiguration,Step:70TrainingEpisode:8000,Cost:-800FeatureWeights0.300.320.320.270.270.180.190.360.360.190.310.230.260.250.020.240.260.296000700080009000-1400-1200-1000TrainingEpisodeCostAgentPerformanceHolisticFeaturesComplexity:5640bitsComplexity:3760bitsComplexity:1880bits(d)Figure4.3:Anoverviewoftheagent’sghostchasingimprovementsovermultipletrainingepisodes.Theimagesinrows(a)through(c)ontheleft-handsideoftheﬁgurehighlighttheenvironmentstateovermultipleactionsforagiventrainingepisode.Ineachoftheseimages,thecoloredlinesindicatethepathsthattheagent(yellowline),invulnerableghosts(blue,red,orange,andmagentalines),andvulnerableghosts(limegreen,blue,green,andcyanlines)tookthroughtheenvironmentacrossanumberofactionchoices.Attheendofeachrow,weprovideatableofthenormalizedfeatureweightsforeachoftheeighteenholisticfeatures.Darkercolorsindicategreaterfeatureimportance.Thefeaturesarepresentedinthesameorderastheywereintroducedfromthediscussions,wherethenumberingisfromtoptobottomthenlefttoright.Apolicycomplexityof3760bitswasusedfortheresultsgiveninrows(a)through(c).Onlyholisticfeatureswereusedtogeneratetheseresults.Plot(d)ontheright-handsideoftheﬁguregivetheaverageagentcostacrossthetrainingepisodes.Forthisplot,wehighlighttheaveragecostswhenconsideringapolicycomplexityof5640bits(blue),3760bits(green),and1880bits(red).Thesecomplexityvaluescorrespondtohigh,moderate,andlowlevelsofpolicy-spaceexploration,respectively.agenthasfeweravenuesinwhichitcanescape,implyingthatithastobemoreconscientiousabouttheactionsthatitchooses.Whentheresultsfromﬁgure4.4arecombinedwiththoseﬁgures4.1–4.3,acleartrendofagentbehaviormateri-alizes.Foralowpolicycomplexityof1880bits,theperformanceinitiallyoutpacesthatofotherpolicycomplexities.Thisisbecausemanystatesaregroupedtogetherandassignedthesameaction,whichhelpsinearlydecisionmakingwhentheaction-valuefunctiontableisbeingpopulatedandinterpolated.Muchoftheearlytermperformanceissim-plyduetoclearingthepelletsinthelevel.Astheepisodecountincreasesandthepolicyspaceiscoarselyexplored,someofthesestatesgroupsmaybefragmentedandassigneddiﬀerentactions.However,sincethenumberoffragmen-tationsthatoccurwillbelow,thepolicyrarelyprogressesbeyondimplementingthislevel-clearingfunctionality.Italsoattemptstotaketheminimalnumberofactionswithoutmuchbacktracking.Bothofthesebehaviorscanhaveadetrimentalimpactonperformance.Theagentmayneitherlearntoproperlyevadeghostsnorlearntochaseafterfruit.Anexampleofthisisgiveninﬁgure4.4(a).Theagent’spoorghost-avoidanceandfruit-chasingabilitiesarecorroboratedbythelowholisticfeatureweightsinﬁgure4.4(a).Forahighpolicycomplexityof5680bits,theperformancetrailsbehindthatforotherpolicycomplexitiesinthebeginningoftraining.Thisisbecauseonlyafewstatesaregroupedtogether,eachofwhichmaynothavereasonableactionchoices.Forinstance,theagentmaymoveintoatunnelthatleadsittobeingtrappedbyghostsoneitherside.Itmayalsoheadstraightintoenemies.However,thesetypesofmistakesallowtheagenttocircumventsimilarissuesinthefuture:moreoftheaction-statespaceisexploredtodeterminewhichactionsareinappropriateforcertainfeaturevalues.Theincreasedemphasisontakingrandomactionspermitstheagenttoimplementbehaviorsthatmayhaveahighcostoverafewstepsbutalowcostaftermanysteps.Anexampleofthisisgivenin3.4(c).Theagentbacktracksthroughapreviouslyclearedsectionoftheleveltoconsumeasecondpowerpelletandchaseafterghosts.Theagentmayalsobacktrackthroughemptyportionsoftheleveltoevadeghostsandchaseafterfruits,bothofwhichhelpreducethecost.Allofthesebehaviorsareindicatedbythehighfeatureweightsinﬁgure4.4(c).Consequently,high-SUBMITTEDTOTHEIEEETRANSACTIONSONCOMPUTATIONALINTELLIGENCEANDARTIFICIALINTELLIGENCEINGAMES14(a)LevelConfiguration,Step:40TrainingEpisode:25000,Cost:-360LevelConfiguration,Step:60TrainingEpisode:25000,Cost:-500LevelConfiguration,Step:80TrainingEpisode:25000,Cost:-670LevelConfiguration,Step:85TrainingEpisode:25000,Cost:-675FeatureWeights0.140.150.150.120.120.190.220.290.300.100.280.150.150.130.020.160.160.14(b)LevelConfiguration,Step:40TrainingEpisode:25000,Cost:-320LevelConfiguration,Step:60TrainingEpisode:25000,Cost:-600LevelConfiguration,Step:80TrainingEpisode:25000,Cost:-780LevelConfiguration,Step:100TrainingEpisode:25000,Cost:-950FeatureWeights0.340.360.370.320.340.290.290.430.450.260.400.330.340.310.020.280.310.34(c)LevelConfiguration,Step:40TrainingEpisode:25000,Cost:-350LevelConfiguration,Step:60TrainingEpisode:25000,Cost:-1090LevelConfiguration,Step:80TrainingEpisode:25000,Cost:-1210LevelConfiguration,Step:100TrainingEpisode:25000,Cost:-1550FeatureWeights0.460.490.500.450.450.470.490.500.500.290.500.460.460.450.020.460.460.47Figure4.4:Anoverviewoftheagent’sperformancefordiﬀerentpolicycomplexitiesandagiventrainingepisode.Theimagesinrows(a)through(c)ontheleft-handsideoftheﬁgurehighlighttheenvironmentstateovermultipleactionsforagiventrainingepisode.Ineachoftheseimages,thecoloredlinesindicatethepathsthattheagent(yellowline),invulnerableghosts(blue,red,orange,andmagentalines),andvulnerableghosts(limegreen,blue,green,andcyanlines)tookthroughtheenvironmentacrossanumberofactionchoices.Attheendofeachrow,weprovideatableofthenormalizedfeatureweightsforeachoftheeighteenholisticfeatures.Darkergrayscalecolorsindicategreaterfeatureimportance.Policycomplexitiesof1880bits,3760bits,and5640bitswereusedfortheresultsgiveninrows(a)through(c),respectively.Thesecomplexityvaluescorrespondtolow,moderate,andhighlevelsofpolicy-spaceexploration,respectively.complexitypoliciesoftenperformsbetterthanlower-complexityonesafterasuﬃcientlylargenumberoflearningepisodes.Whenthepolicycomplexityissetbetweenthesetwoextremes,at3760bits,severalagentbehaviorsemergequickly.Theagentlearnstheconnectionbetweenpowerpelletsandghostvulnerability,forinstance,earlierthanforhigher-complexitypolicies.Onaverage,ittakes7247.5±761.72and8018.3±863.30episodesfortheformerandlattercases,respectively.Likewise,fruitchasingoccursafter3106.3±605.86and3932.1±822.66episodesfortheformerandlattercases,respectively.Thisisbecausethesearchprocesshasatendencytotakelargerjumpsinthepolicyspace,whichoftenallowittouncoverreasonablygoodpolicies.Thesepoliciescanbeexploitedearlyoninthelearningprocess.Moderatepolicycomplexitiesalsopermitsmallerjumps,comparedtothelow-complexitycase.Thishelpstoreﬁneexistingpoliciesandimprovetheirperformance.However,thecostimprovementforthemoderate-complexitycaserarelyapproachesthatofthehigh-complexitycaseasthenumberofepisodesgrows:notenoughofthesearchspacewillbethoroughlyexplored.Amoderate-complexitypolicymaythereforecausetheagenttoeschewvulnerableghostsmoreoftenthanforthelattercase.Itmayallowfruitstoescapedownwarppipes.Itmayalsoleavemoreislandsofpellets.Inﬁgure4.4,thenormalizedfeatureweightsequalizeasthepolicycomplexitygrows.Thisindicatesthatalmostallofthefeaturesbecomeimportantforthepurposesofreducingthecost.Thefeaturesthatoverseethepursuitofpelletsandavoidanceofghosts,however,aretheslightlymoreprominent,astheyleadtonecessarybehaviorstocompletethegame.4.2.3MethodologicalComparisonsForcomparativepurposes,weutilizetwoofthemoreprominentreinforcementlearningsearchstrategies:epsilon-greedyexplorationandsoft-max-basedexploration.Epsilon-greedyexplorationisbasedonthenotionthatrandomactionsshouldbetakenatrandomtimes.Thechoiceofwhentotakearandomactionisgovernedbysingleparameter.Itiswellknownthatthistypeofapproachcanconvergetooptimalvalues,incertainsituations,asthenumberofepisodesgrows[37].Forourapplication,thisdidnotoccur.After3000,6000,and9000episodes,theaveragecostswere−162.68±175.52,−494.38±158.98,andSUBMITTEDTOTHEIEEETRANSACTIONSONCOMPUTATIONALINTELLIGENCEANDARTIFICIALINTELLIGENCEINGAMES15−786.57±167.79,respectively,whenusingaparametervalueof0.5.Thiscorrespondstothesituationofselectingarandomactionaftereverytwosteps.Increasingtheparametervalueto0.7,whichencouragesahighdegreeofexploration,ledtoworseresults:−84.83±192.63,−367.45±165.20,and−605.49±128.81,respectively.Decreasingtheparametervalueto0.3,whichpromotesexploitation,yieldedonlyamarginalimprovement:−193.57±168.15,−562.08±173.62,and−901.59±146.29,respectively.Learningovermanytimesmoreepisodesdidlittletoenhancetheagent’sperformance.Adjustingthelearningrateanddiscountfactorhadonlyamarginalimpactaswell.Partofthereasonwhyepsilon-greedyexplorationperformspoorlyisthatthereisnoweightingoftheactionsbeingconsidered.Theinﬂuenceofthestate-actionpairsontheexpectedreturnsshould,attheveryleast,guidetheselectionofappropriateactions,asinsoft-max-basedexploration.Inourapplication,implementingsoft-max-basedexplorationyieldedmodestbeneﬁts.Foraparametervalueof0.5,theaveragecostswere−250.69±163.48,−627.41±131.20,and−957.75±118.66,respectively,after3000,6000,and9000episodes.Thisparametervaluecorrespondstoanevenmixtureofexplorationandexploitation.Biasingtowardthebest-performingaction,withaparametervalueof0.3,ledtocostsof−264.32±148.97,−707.24±126.18,and−1167.2±121.55,respectively.Promotingnear-equi-probableactionselection,withaparametervalueof0.7,ledtocostsof−195.83±151.69,−536.02±163.40,and−807.71±142.58,respectively.Whilethesepoliciesoutperformthosefromepsilon-greedyexploration,theybehavedworsethanthoseproducedbythevalueofinformation.Itisinterestingtonotethatneitherepsilon-greedy-norsoft-max-basedpoliciesimplementedmulti-modalagentbehaviorswell.Theagentswouldoftenavoidvulnerableandinvulnerableghostsalike.Throughoutoursimulations,wehavereliedonanon-standardenvironmentconﬁguration,whichwasdonetoexpeditetraining.ThismakescomparisonswithexistingﬁndingsonMs.Pac-Mandiﬃcult.Toresolvethisissue,weappliedourbest-performingpolicies,fromthisnon-standardconﬁguration,tothefourstandardMs.Pac-Manenvironmentsconﬁgurations.Wealsousedthesamepointsystemastheoriginalgame.Wefoundthatepsilon-greedy-andsoft-max-basedpoliciescouldreach12214.6±531.12and15387.49±478.25points,respectively,over25MonteCarloexperiments.Thiscorrespondstothecapabilitiesofanoviceplayerwhoisabletoclearanywherefromﬁvetosevenlevels.Whenrelyingonholisticfeatures,value-of-information-basedpoliciescouldobtainscoresof20740.87±519.82,37784.38±462.7,and55988.94±605.05forthethreecomplexityvaluesthatweconsideredhere.Forthehybridfeaturecase,wewitnessedscoresof12956.34±678.92,18625.11±625.48,and23024.01±675.53forthethreecomplexityvalues.Asbefore,theseaverageswereobtainedfrom25MonteCarloexperiments.Theseresultscorrespondtoasemi-experiencedplayerwhocancompleteuptoﬁfteenlevels.Suchresultsindicatethatthepoliciesgeneralizewellandarenotspatiallysensitive.TheyalsocomparefavorablytothescoresobtainedbyAlhejaliandLucas[10],BrandstetterandAhmadi[11],WirthandGallagher[15],andFoderaroetal.[27,28].5ConclusionsInthispaper,wehaveconsideredaninformation-theoreticapproachforeitherpromotingoreschewingriskyagentbehaviorsduringreinforcementlearning.Ourapproachisbasedonavalueofinformationcriterion.WhenappliedtoMarkovdecisionprocesses,thiscriterionquantiﬁestheexpectedreductionincoststhatcanbeachievedforpolicieswithapre-speciﬁedcomplexity.Equivalently,itquantiﬁesthesimplestpolicycomplexitythatproducescostsbelowapre-speciﬁedthreshold.Inshort,thecriterionprovidestheoptimalconversionbetweeninformationandcosts.Regardlessofwhichformofthecriterionisutilized,thereisafreeparameterwhosevaluemustbeselected.Thisparameterdictatestheamountofpolicycompression.Fortheﬁrstformofthiscriterion,smallparametervaluescorrespondwithlow-complexitypolicies.Italsocorrespondswithrisk-avoidingagentbehaviors.Higherparam-etervaluesleadtolowerpolicycompressionlevels.Agentsaremorelikelytofrequentlytakerisksinthesecases,whichcanpromotetheformationofmulti-modalstrategies.Whatconstituteslargeandsmallvaluesdependsontheproblemdomain.Forthesecondformofthevalueofinformationcriterion,wehavetheconversesituation.Thatis,low-magnitudeparametervaluescorrespondwithhighamountsofrisk-takingbyagents,asmoreemphasisisplacedonﬁndinghigh-performingpolicies.Thereisalsothepotentialforeachstatetohaveauniqueactionresponse.Whenusinghigh-magnitudeparametervaluesinthesecondformofthecriterion,groupsofstateswillhavethesameactionresponse.Amoreconservativeaction-selectionproceduredominatesinthissituation.Optimizationofeitherformofthevalueofinformationcriteriongivesrisetosoft-max-like,weighted-randomaction-selectionprocess.Theweightsaredeterminedbyboththeexpectedcostoftakinganactioninaparticularstateandeitherapolicycomplexityconstraintoracostthresholdconstraint.Inoursimulations,theseconstraintshadaprofoundinﬂuenceonthepolicyreturnquality.Highlevelsofriskyieldedpolicieswhoseexpectedcostsap-proachedthosefortheoptimalsolution.Theyalsofacilitatedtheswitchingofgame-playstrategies.Agreatmanylearningepisodeswereneededtoconstructsuchpolicies,though.Lowerlevelsofriskallowedforthequickgenera-tionofpoliciesthatperformedreasonablywell.Asthenumberofepisodesincreased,though,thepolicies’qualitieslaggedbehindthosefoundwithhigherlevelsofrisk.Comparatively,purelysoft-max-basedandepsilon-greedyac-tionselectionproducedworsepolicies.ThisisbecausethesetypesofselectionproceduresassumethatthepoliciesSUBMITTEDTOTHEIEEETRANSACTIONSONCOMPUTATIONALINTELLIGENCEANDARTIFICIALINTELLIGENCEINGAMES16canbeunboundedlycomplex.Formanyproblems,policieswithﬁnitecomplexitycanapproachthesamelevelsofperformanceinfewerepisodes.Ouremphasisinthispaperhasbeenondemonstratingthatthevalueofinformationprovidesaprincipledtrade-oﬀbetweenexplorationandexploitation.Ourfutureendeavorswillinvolveextensionsofthiscriteriontoeﬀectivelyaddressmanyclassesofproblems.Oneextensionwillbetheautomatedselectionofthecriterion’sfreeparameter.Inourcurrentwork,wehavemanuallyspeciﬁedthisvaluebaseduponexpertknowledgeoftheproblemdomain.Forcertaindomains,however,trial-and-error-basedtestingmaybeneeded,whichcanbetimeconsuming.Tosidestepthisissue,wewillinvestigateinformation-theoreticapproachesforadaptingtheparametervalueduringlearning.Ausefulheuristicmaybetochoosevaluesthatmonotonicallydecreasetheaction-stateentropy.Byminimizingentropy,weareforcingthepoliciestoeventuallybecomedeterministic.Nofurtherriskswillneedtobetakenbytheagent,ascompleteknowledgeofthatenvironmentwillhavebeenobtained.References[1]R.S.SuttonandA.G.Barto,ReinforcementLearning:AnIntroduction.Cambridge,MA,USA:MITPress,1998.[2]L.P.Kaelbling,M.L.Littman,andA.W.Moore,“Reinforcementlearning:Asurvey,”JournalofArtiﬁcialIntelligenceResearch,vol.4,no.1,pp.237–285,1996.Available:http://dx.doi.org/10.1613/jair.301[3]R.L.Stratonovich,“Onvalueofinformation,”IzvestiyaofUSSRAcademyofSciences,TechnicalCybernetics,vol.5,no.1,pp.3–12,1965.[4]R.L.StratonovichandB.A.Grishanin,“Valueofinformationwhenanestimatedrandomvariableishidden,”IzvestiyaofUSSRAcademyofSciences,TechnicalCybernetics,vol.6,no.1,pp.3–15,1966.[5]R.L.Stratonovich,InformationTheory.Moscow,SovietUnion:SovetskoeRadio,1975.[6]S.M.Lucas,“EvolvinganeuralnetworklocationevaluatortoplayMs.Pac-Man,”inProceedingsoftheIEEESymposiumonComputationalIntelligenceandGames(CIG),Colchester,UK,April4-62005,pp.203–210.[7]M.Wittikamp,L.Barone,andP.Hingston,“UsingNEATforcontinuousadaptationandteamworkformationinPac-Man,”inProceedingsoftheIEEESymposiumonComputationalIntelligenceandGames(CIG),Perth,Australia,December15-182008,pp.234–242.Available:http://dx.doi.org/10.1109/CIG.2008.5035645[8]M.GallagherandM.Ledwich,“EvolvingPac-Manplayers:Canwelearnfromrawinput?”inProceedingsoftheIEEESymposiumonComputationalIntelligenceandGames(CIG),Honolulu,HI,USA,April1-52007,pp.282–287.Available:http://dx.doi.org/10.1109/CIG.2007.368110[9]R.ThawonmasandT.Ashida,“EvolutionstrategyforoptimizingparametersinMs.Pac-MancontrollerICEPambush3,”inProceedingsoftheIEEESymposiumonComputationalIntelligenceandGames(CIG),Copenhagen,Denmark,August18-212010,pp.235–240.Available:http://dx.doi.org/10.1109/ITW.2010.5593350[10]A.M.AlhejaliandS.M.Lucas,“UsingatrainingcampwithgeneticprogrammingtoMs.Pac-Managents,”inProceedingsoftheIEEESymposiumonComputationalIntelligenceandGames(CIG),Seoul,SouthKorea,August31-September32011,pp.118–125.Available:http://dx.doi.org/10.1109/CIG.2011.6031997[11]M.F.BrandstetterandS.Ahmadi,“ReactivecontrolofMs.Pac-Manusinginformationretrievalbasedongeneticprogramming,”inProceedingsoftheIEEESymposiumonComputationalIntelligenceandGames(CIG),Granada,Spain,September11-142012,pp.250–256.Available:http://dx.doi.org/10.1109/CIG.2012.6374163[12]M.GallagherandA.Ryan,“LearningtoplayPac-Man:Anevolutionary,rule-basedapproach,”inProceedingsoftheIEEECongressonEvolutionaryComputation(CEC),Canberra,Australia,December8-122003,pp.2462–2469.Available:http://dx.doi.org/10.1109/CEC.2003.1299397[13]P.RohlfshagenandS.M.Lucas,“Ms.Pac-ManversusGhostTeamCEC2011competition,”inProceedingsoftheIEEECongressonEvolutionaryComputation(CEC),NewOrleans,LA,USA,June5-82011,pp.70–77.Available:http://dx.doi.org/10.1109/CEC.2011.5949599[14]A.FitzgeraldandC.B.Congdon,“RAMP:Arule-basedagentforMs.Pac-Man,”inProceedingsoftheIEEECongressonEvolutionaryComputation(CEC),Trondheim,Norway,May18-212009,pp.2646–2653.Available:http://dx.doi.org/10.1109/CEC.2009.4983274[15]N.WirthandM.Gallagher,“AninﬂuencemapmodelforplayingMs.Pac-Man,”inProceedingsoftheIEEESymposiumonComputationalIntelligenceandGames(CIG),Perth,Australia,December15-182008,pp.228–233.Available:http://dx.doi.org/10.1109/CIG.2008.5035644[16]D.J.GagneandC.B.Congdon,“FRIGHT:Aﬂexiblerule-basedintelligentghostteamforMs.Pac-Man,”inProceedingsoftheIEEESymposiumonComputationalIntelligenceandGames(CIG),Granada,Spain,September11-142012,pp.273–280.Available:http://dx.doi.org/10.1109/CIG.2012.6374166[17]P.BurrowandS.M.Lucas,“EvolutionversustemporaldiﬀerencelearningforlearningtoplayMs.Pac-Man,”inProceedingsoftheIEEESymposiumonComputationalIntelligenceandGames(CIG),Milan,Italy,September7-102009,pp.53–60.Available:http://dx.doi.org/10.1109/CIG.2009.5286495[18]L.L.DeLoozeandW.R.Viner,“FuzzyQ-learninginanondeterministicenvironment:DevelopinganintelligentMs.Pac-Managent,”inProceedingsoftheIEEESymposiumonComputationalIntelligenceandGames(CIG),Milan,Italy,September7-102009,pp.162–169.Available:http://dx.doi.org/10.1109/CIG.2009.5286478[19]S.Griﬃth,K.Subramanian,J.Scholz,C.Isbell,andA.L.Thomaz,“Policyshaping:Integratinghumanfeedbackwithreinforcementlearning,”inAdvancesinNeuralInformationProcessingSystems(NIPS),C.J.C.Burges,L.Bottou,M.Welling,Z.Ghahramani,andK.Q.Weinberger,Eds.Cambridge,MA,USA:MITPress,2013,pp.2625–2633.[20]A.Vezhnevets,V.Mnih,S.Osindero,A.Graves,O.Vinyals,J.Agapiou,andK.Kavukcuoglu,“Strategicattentivewriterforlearningmacro-actions,”inAdvancesinNeuralInformationProcessingSystems(NIPS),D.D.Lee,SUBMITTEDTOTHEIEEETRANSACTIONSONCOMPUTATIONALINTELLIGENCEANDARTIFICIALINTELLIGENCEINGAMES17M.Sugiyama,U.V.Luxburg,I.Guyon,andR.Garnett,Eds.Cambridge,MA,USA:MITPress,2016,pp.3486–3494.[21]J.Oh,X.Guo,H.Lee,R.L.Lewis,andS.Singh,“Action-conditionalvideopredictionusingdeepnetworksinAtarigames,”inAdvancesinNeuralInformationProcessingSystems(NIPS),C.Cortes,N.D.Lawrence,D.D.Lee,M.Sugiyama,andR.Garnett,Eds.Cambridge,MA,USA:MITPress,2015,pp.2863–2871.[22]H.P.vanHasselt,A.Guez,M.Hessel,V.Mnih,andD.Silver,“Learningvaluesacrossmanyordersofmagnitude,”inAdvancesinNeuralInformationProcessingSystems(NIPS),D.D.Lee,M.Sugiyama,U.V.Luxburg,I.Guyon,andR.Garnett,Eds.Cambridge,MA,USA:MITPress,2016,pp.4287–4295.[23]D.RoblesandS.M.Lucas,“AsimpletreesearchmethodforplayingMs.Pac-Man,”inProceedingsoftheIEEESymposiumonComputationalIntelligenceandGames(CIG),Milan,Italy,September7-102009,pp.249–255.Available:http://dx.doi.org/10.1109/CIG.2009.5286469[24]N.IkehataandT.Ito,“MonteCarlotreesearchinMs.Pac-Man,”inProceedingsoftheIEEESymposiumonComputationalIntelligenceandGames(CIG),Seoul,SouthKorea,August31-September32011,pp.39–46.Available:http://dx.doi.org/10.1109/CIG.2011.6031987[25]S.Samothrakis,D.Robles,andS.M.Lucas,“FastapproximateMax-nMonteCarlotreesearchforMs.Pac-Man,”IEEETransactionsonComputationalIntelligenceandAIinGames,vol.3,no.2,pp.142–154,2011.Available:http://dx.doi.org/10.1109/TCIAIG.2011.2144597[26]T.Pepels,M.H.M.Winands,andM.Lanctot,“Real-timeMonteCarotreesearchinMs.Pac-Man,”IEEETransactionsonComputationalIntelligenceandAIinGames,vol.6,no.3,pp.245–257,2014.Available:http://dx.doi.org/10.1109/TCIAIG.2013.2291577[27]G.Foderaro,A.Swingler,andS.Ferrari,“Amodel-basedcelldecompositionapproachtoon-linepursuit-evasionpathplanningandthevideogameMs.Pac-Man,”inProceedingsoftheIEEESymposiumonComputationalIntelligenceandGames(CIG),Granada,Spain,September11-142012,pp.281–287.Available:http://dx.doi.org/10.1109/CIG.2012.6374167[28]——,“Amodel-basedapproachtooptimizingMs.Pac-Mangamestrategiesinrealtime,”IEEETransactionsonComputationalIntelligenceandAIinGames,2016,(accepted,inpress).Available:http://dx.doi.org/10.1109/TCIAIG.2016.2523508[29]R.E.Bellman,DynamicProgramming.Mineola,NY,USA:DoverPublications,2003.[30]R.S.Sutton,“Learningtopredictbythemethodsoftemporaldiﬀerences,”MachineLearning,vol.3,no.1,pp.9–44,1988.Available:http://dx.doi.org/10.1023/A:1022633531479[31]C.J.C.H.WatkinsandP.Dayan,“Q-learning,”MachineLearning,vol.8,no.3,pp.279–292,1992.Available:http://dx.doi.org/0.1023/A:1022676722315[32]M.WieringandJ.Schmidhuber,“FastonlineQ(λ),”MachineLearning,vol.33,no.1,pp.105–115,1998.Available:http://dx.doi.org/10.1023/A:1007562800292[33]R.V.Belavkin,“Asymmetryofriskandvalueofinformation,”inDynamicsofInformationSystems,C.Vogiatzis,J.Walteros,andP.Pardalos,Eds.NewYork,NY,USA:Springer-Verlag,2014,pp.1–20.[34]I.J.SledgeandJ.C.Príncipe,“Balancingexplorationandexploitationinreinforcementlearningusingavalueofinformationcriterion,”inProceedingsoftheIEEEInternationalConferenceonAcoustics,Speech,andSignalProcessing(ICASSP),NewOrleans,LA,USA,March5-92017,(accepted,inpress).[35]S.P.Singh,T.Jaakkola,M.L.Littman,andC.Szepesvári,“Convergenceresultsforsingle-stepon-policyreinforcement-learningalgorithms,”MachineLearning,vol.38,no.3,pp.287–308,2000.Available:http://dx.doi.org/10.1023/A:1007678930559[36]M.S.Emigh,E.G.Kriminger,A.J.Brockmeier,J.C.Príncipe,andP.M.Pardalos,“Reinforcementlearninginvideogamesusingnearestneighborinterpolationandmetriclearning,”IEEETransactionsonComputationalIntelligenceandAIinGames,vol.8,no.1,pp.56–66,2008.Available:http://dx.doi.org/10.1109/TCIAIG.2014.2369345[37]J.N.Tsitsiklis,“AsynchronousstochasticapproximationandQ-learning,”MachineLearning,vol.16,no.3,pp.185–202,1994.Available:http://dx.doi.org/10.1023/A:1022689125041